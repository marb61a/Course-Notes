                    Artificial Intelligence A-Z : Learn How To Build An AI
                    Course Notes


                    Section 1 - Welcome to the course!
A brief welcome to the course from the instructors
Why AI
  - Moores Law has meant that computer power has reached the point where AI is possible
    - https://en.wikipedia.org/wiki/Moore's_law
  - Some of the priciples that AI use in games can be used in business
    - AI was able to reduce Google's data center electricity bill by 40%
There are also materials available for download
This course will use PyTorch
  - Some of the resources use Tensorflow but are well worth looking at



                    Section 2  - Fundamentals Of Reinforcement Learning
1 - Q-Learning Intuition
Reinforcement Learning is a process 
  - The example of training a dog is used eg giving a treat as a reward
    - These rewards are + or - minus 1 etc in AI
  - The course uses the example of a maze
    - An environment does not have to be a maze, it can be anything in life
    - Making breakfast is a routine that has actions and state changes
  - Each time the agent takes an action there is a state change and sometimes a reward
    - Actions should be taken at the appropriate time
  - Preprogrammed functions are the opposite of reinforcement learning
    - In preprogrammed robot dogs walking automatically are already in memory
    - This is different in robot dogs not preprogrammed, moving forward etc will get +1 and falling -1 for example
    - This will eventually be learned by the robot dogs and they may be better than reinforcement versions

The Bellman Equation
  - This can be complex and will be introduced gradually
  - Named after Richard Ernest Bellman
    - https://en.wikipedia.org/wiki/Richard_E._Bellman
  - https://en.wikipedia.org/wiki/Bellman_equation
There are some concepts being used
  - s -> State that the agent is in or any other it can be in
  - a -> An action that an agent can take
  - R -> This is for reward that the agent gets for entering a certain state
  - γ -> (Gamma) This is the discount factor
  - The equation itself looks something like this
    V(s) = max(R(s,a) + yV(s'))
            a
  - s' -> s prime is the state following this state
  - Max is use because there are many states that an action can take 
  The plan
    - Using the maze example the plan is to make a map showing route to travel
    - Numerical values can be replaced by arrows showing the direction to take
    - Policies are similar to plans but differ slightly

Markov Equation
  - Markov Decision Process (MDP)
  - Deterministic Search
    - If an agent decides on an action eg up in the example maze then that is what will happen
  - Non-Deterministic Search
    - If an agent decides on an action there are options
      - In the example going up could be 80% and left and right 10% each
      - This is a much more realistic example
      - This is a Markov process
  - MDP technical definition      
    - https://en.wikipedia.org/wiki/Markov_chain
    - https://en.wikipedia.org/wiki/Markov_decision_process
    - A stochastic process has the Markov property if the conditional probability distribution of the future states
    of the process (conditional on both past and present states) depends only upon the present state not on the sequence
    of events that preceded it. A process with this property is called a Markov Process
    - It basically means that you will decided on an action based on where you are not how you got there
    - MDP's provide a mathematical framework for modeling decision making in situations where outcomes are partly random
    and partly under the control of the decision maker
    - It is an extension to the Bellman equation
      - This is for random events that cannot be controlled
      - This will be referred to as the Bellman equation from this point
      V(s) = max (R(s,a) + yΣp(s, a, s')V(s'))
              a             s'
              
Plan v Policy
  - A plan is for when you know what steps to take next
  - A plan is not feasible when randomness is introduced
  - Discount factors are more apparent when using non-deteministic actions

Adding a living penalty
  - This will  be focused on the reward section of the equation
  - This will be a negative reward
    - It is a fraction of the main negative and positive rewards
  - It does not have to be applied to every state

Q-Learning Intuition
  - The big question to be answered is whers is the Q?
  - Q is used because of quality (not definitive)
  - Q is used for the quality of the action
    - Previously we were looking at the quality of the state
  - Q is the metric for assessing an action
  - Because actions lead to states there has to be a link
    - Both approaches should lead to the same result
  - The formula is
    Q(s, a) = R(s,a) + yΣ(p(s, a, s')V(s'))
                        s'

Temporal Difference
  - Temporal Difference is the heart and soul of the Q-Learning algorithm
  - 


                    Section 3 - Self-Driving Car (Deep Q-Learning) 
1 - Deep Q-Learning Intuition

2 - Installation for Part 1

3 - Creating the environment

4 - Building an AI

5 - Playing with the AI



                    Section 3  - Doom (Deep Convolutional Q-Learning)
1 - Deep Convolutional Q-Learning Intuition

2 - Installation for Part 2

3 - Building an AI

4 - Playing with the AI



                    Section 4 - Breakout (A3C) 
1 - A3C Intuition

2 - Installation for Part 3

3 - Building an AI

4 - Annex 1: Artificial Neural Networks

5 - Annex 2: Convolutional Neural Networks

