<b><p align=center>                    
  Blue Array Academy Technical SEO </br>
  Course Notes  
https://www.bluearrayacademy.com/


<p align=center>An Introduction to Technical SEO <br/>

What is Technical SEO?
  -
  - SEO has 3 main pillars - Content, Links and Technical
  - SEO is about improving a site visibility for search engines
  - Most are oriented towards Google search as it is the market leader
  - Content is ranked by algorithms when search is used
  - Prior to content ranking it is important that search engines can find the website
  - Technical SEO comes into play at this point
  - Technical SEO ensures that search engine bots can access and understand a website as quickly as easy as possible
  - There are several topics in Technical SEO 
    - Information Architecture
    - Crawling, Indexing, Rendering and Ranking
    - Various tools
    - Page Experience
    - Schema, On-Page
    - Security
    - Images
    - Accessibility

Morals & Ethics
  -
  - There are some things to consider when investigating a site from a technical perspective
  - Provide Solutions over Problems
    - When conducting a site audit it is very easy to provide problems
    - These problems observed in isolation rarely give any context to cause or solutions
    - Problems should be quoted specifically, the scale and the solutions to the problem
    - Root causes are essential to issues being fully understood
  - Work with team members
    - Analysis of sites and discovery of solutions and problems needs to be communicated to all parties effectively
    - Providing problems and solutions helps with this and can be provided in different ways to different audiences
    - Team members may not know technical jargon or reasoning behind decisions
  - Crawling Ethics
    - Ensure that permission is granted before crawling a site, check before hand
    - There will be times when anti-crawling software will be in place
  - General Ethics
    - Do not break the law
    - Always follow best practice
    - Do the best thing for a client
    - Do not provide clients with misleading information
  - Environmental Considerations
    - Try do the right thing for the environment
    - Send on the data that needs to be sent and reduce C02 emissions
    - Have webpages as light as possible

The Basics - Hosting
  -
  - There are 2 types of host that need to be known
    - Web Hosts which is where content is stored
    - Domain Hosts which is where information on domain names is stored
  - Web Hosting
    - In order for content to be published on the internet there needs to be a place to publish it, in this case a website
    - This site has to be hosted somewhere
    - When a user requests a web address that is on your site, the server that hosts the website talks with the request and if able then fulfills it
    - The web site hosting can either be with a company or hosted yourself.
  - Hosted vs Self-Hosted
    - Hosted sites are typically built with site builders such as Wix, WordPress or Squarespace
    - Self-Hosted sites are usually easier to customise but this needs to be carefully done
    - When self hosting a lot of work that external hosts do needs to be done such as running backups
  - Shared vs Dedicated Hosting
    - Once a hosted solution has been decided on then it is time to decide on either shared or dedicated hosting
    - Shared is much cheaper and is better for those with limited technical skills as there will not be as many options
    - Dedicated Hosting is good for technical minded people and is like owning your own home
  - What should a good host do
    - Provide excellent technical support when needed
    - There should be the option to have either a dedicated or shared server
    - There should be no problem scaling as demand increases
    - There should be a focus on security
    - Additional functionality should be available such as the ability to whitelist IP Addresses
  - Content Delivery Networks (CDN's) 
    - Although not strictly necessary for a website there are advantages to using them
    - They are a group of servers which are distributed around the globe allowing for faster worldwide content delivery
    - They can be used in addition to web hosting as they allow for content caching on their servers
    - They inprove both performance and security and they reduce dependence on a single server thus improving uptime

Networking
  -
  - Protocols
    - There are multiple protocols used to ensure that servers talk to one another and users get their data
       - IP which is used for network routing
       - TCP which ensures data is delivered reliably
       - HTTP which transfers data between devices
       - HTTPS which is an encrypted version of HTTP and is used more
       - TLS/SSL which is the encryption that HTTTPS uses 
  - Header Requests
    - After a web address has been typed into a browser a request is then sent to a server
    - It contains various information that allows the server to tailor it's response
     - Requested URI, Host, The type of data accepted by the browser, a User Agent, Source IP
  - Header Responses
    - Depending on the information that is sent to the server in the header request, the server will then respond with its own headers 
    - After this then if the browser is allowed to view content, it will begin downloading HTML from the server using details from the response

Standards
  -
  - What is W3C
    - This is an organisation which develops standards for the Web
    - Due to the advent of website builders these standards are not as rigouously adhered to as they once were
    - Most website builders are used by people who may not know HTML
    - W3C offers a HTML validator which allows people to test & check on a per site basis
      - https://validator.w3.org/ 
  - Why should you care?
    - If invalid HTML is used there is no guarantee it will render correctly in all browsers
    - Validation may not be important for the Googlebot as long as the site is rendered and structured data can be extracted 
    - Valid HTML is still important as it can help pages render and load faster
    - Broken HTML within the head section of a page can cause problems for the Google crawler which can cause other elements not to be seen
    - There maybe rendering issues for users if there is invalid HTML
  - What are RFC's
    - Request For Comments are used to develop a network protocol that is seen as standard
    - Almost all netowrk protocols made for the internet are built on RFC's
    - IP, TCP, SMTP, FTP, HTTP etc are built on RFC's
    - There are a lot of Non-SEO focused RFC's being published
  - Robot Exclusion Protocol
    - It was never turned into a standard when created in 1994
    - This has led to many different interpretations down the years
 
  <br/><br/>

<p align=center>A Technical View of how search works <br/>
  
Crawling
  -
  - Search engines return a list of webpages based on a user's search
  - These are known as SERP's or Search Engine Results Pages
  - It is not possible to search the entire internet for every single query in real time
  - An index is a list of words, phrases etc that can be used to return predetermined results
  - Index requires crawling every site prior to searching
  - Google will be the search engine focused on as it is by far the market leader
  - Crawling in it's simplist terms is discovery, it is what the search engine do to find pages
  - The Crawling process is as follows
    - A webpage is discovered and its URL is noted as well as the HTTP response code
    - The webpage DOM (Document Object Model) is then gone through to find and note each link found
    - These will then be crawled as well
    - Sometimes a URL is not stored, this is done by using a HTML directive telling the search engine not to follow a link
    - This is called a nofollow directive
    - Site hierarchies are also stored for indexing purposes
  - Other pieces of software that crawl around the web can be thought of as crawlers, bots, etc but here the search engine crawlers are of interest
  - When crawling search engines do not restrict themselves solely to webpages, other items gathered include
    - Images, Videos, Documents, Book scans, some databases, user generated content such as map locations 
    - This helps monetise searchs
  - Only in an ideal world would a search engine be aware of all sites and updated their change immediately
  - Resources however large are still limited and crawling everything is not feasible
  - Crawl Budget is the amount of effort that a search engine puts into crawling a site
    - Crawl Budget is mostly a concern for big site as only a certain amount of pages will be crawled before moving on
    - This is because search engines want to crawl a large variety of pages on a large variety of websites 
    - Sites may change regularly but may only be recrawled monthly or weekly
    - On large sites it is important that the most important pages are crawled rather than least important pages 
    - There are 2 factors which determin a site's crawl budget
      - Crawl Limit which is how much crawling a site can handle
      - Crawl Demand which is the perceived demand for a site's contents 
    - Crawl Budget is not just for HTML pages but for all document type resource eg JS files
    - It is not solely based on a single site, page load speeds can affect the amount of pages being crawled
    - Search engine capacity can also affect site crawl budget
    - Other factors include content freshness, page popularity and whether the page is optimised for Desktop, Mobile etc
  - Search engines will not follow paid for links so this is not a way to get crawled
  - To appear in SERP's it is essential that a site can be found and crawled
  - Search engines can be supplied with a list of pages to be crawled on a site so that they don't miss any
  - There are a few methods available so that search engines find all the pages that the should see
    - Google Search Console aka GSC
    - Robots using the robots.txt file
    - Sitemaps using the sitemap.xml file
    - Internal links within the website
    - Passwords protected files and directories   
  - Some of the above methods can be generated automatically or developed manually
  - Ensure that there is a logical hierarhy within a site to ensure that crawlers can find pages
  - Robots.txt can hold search engine directives, these can be disregarded and are more suggestion than order
  - Crawlers do not use passwords and usernames so if pages are hidden behind these then they will not be seen
  - There are some steps to ensure that pages are found by search engines
    - First verify that the search engine can find the site
    - Then ensure that the page submitted looks correct
    - Submit a single url if there are changes and recrawling needs to be done ASAP
    - Ensure good navigation so that all pages can be found (Especially on the home page)
    - If there is to be only one page submitted then it is best to submit the homepage 
    - If possible then get links from pages that have already been crawled

Indexing
  -
  - Whereas crawling is akin to discovery then indexing is akin to understanding
  - Search engine must understand what a search query has found and provide reliable and quick results
  - Google uses a reverse index which is a list of words/phrases that are stored in a database as well as pointer to related docs
    - A document is this case is a web page, pdf, image etc
  - Tokenisation is used to reduce searches to their core meaning which reduces the resources need to perform the search
  - If the index was not inverted then a list of documents would need to be searched for the words which would be a lot slower
  - An index is more like a catalogue of everything that a search engine has found
    - URL's which point to various things, Web Pages, Images, Videos etc
    - Enteries are also made for different language versions of documents which are considered as separate documents
  - Indexing helps return more intelligent and more useful results
    - Google's index is called Caffine and it was introduced in 2009
    - Recent changes are referred to as medic as misinformation was being dropped down the page results
    - This had an effect on YMYL pages, (Your Money Your Life)
  - As well as indexing documents there are highly compressed versions stored too
    - This will contain only the textual elements and is the latest version of the page that was seen
    - The process helps to speed up search engine indexing
    - Sometimes the search engine will allow for the cached version to be viewed
  - Where there are several similar versions of a page the search engine selects what it considers to be the canonical version
    - This is the authoritative or master version of a site
  - Search engines may get the canonical version wrong so it should be set in a meta tage in the HTML head section of a page
    - If the page is not the canonical version then tags should point to the canonical version
  - Selecting an incorrect canonical version may lead to the wrong page being listed in the search
    - Similar pages may cause confusion for search engines and using tags removes this.
  - There are a few ways to improve indexing of a page
    - Firstly start off by creating page titles that are short and meaningful (30 - 60 characters)
    - Make use of page headings that show the subject of the page
    - Use text that is both useful and meaningful to visitors
  - Images and videos should be annotated with alt text etc as needed
  - There are some methods available when you want to stop a page being indexed
    - There are multiple reasons to avoid indexing a page mostly to avoidwasting crawl budget
    - It should be said that even with methods available to stop indexing the best way to avoid data on the internet is to not put it there
    - HTTP header Meta tags such as content=noindex
    - HTTP response header returns X-Robots-Tag: noindex
    - Robots file which should be in the site root directory (robots.txt)
    - Server configuration using the .htaccess file
    - Some CMS software has plugins to help block indexing
  - It should be remembered tha non indexing are requests and may be ignored especially by malicious crawlers

Rendering
  -
  - Rendering is performed during indexing, it is the process of retrieving a page, executing the code within and assessing results
    - Pre-render and post render is examined to identify the structure and layout of a site
  - By taking the site HTML, CSS, JS and other needed files the search engine can build a DOM
    - The Document Object Model (DOM) takes all the data and markup the browser needs to render a page
  - The search engine creates its own DOM which means it is effectively rendering the page similar to the browser
    - Information derived from the rendering is used for search engine ranking
  - The main reason that search engines render is that many pages replace content using JS eg user interaction
    - Another reason is that the content is time dependent eg news and would need to be replaced to avoid staleness
  - It is important that JS is working properly in browsers from an SEO perspective
    - Website that disable JS should still show content properly
  - At some point webpage were relatively easy to understand programmatically
    - This has changed due to the need for more advanced functionality on webpages
  - The search engines have had to adapt to as they need to see both pre-renders and versions of the post render page
  - Google introduced its AJAX crawler to make those type of pages accessible to the googlebot
    - This necessitated developers to add special hash tags to url to indicate AJAX was being used
  - Webpages have more JavaScript than ever and the AJAX scheme has been phased out
    - In its place Google will render the JavaScript itself
    - It does this by taking a url which is in the crawl queue
    - A request is then made for the URL and all files are retrieved
    - These are then put into the render queue for processing
    - A headless version of the Chrome browser is then used by the render which renders by executing the internal JS
      - Headless refers to the fact that there is no browser window available to view this 
    - Once rendering is complete any links found are stored for future processing by putting them into the crawl queue
    - The page is then examined so that it can be added to the index
  - There can be issues with the above approach
    - What happens if the JS executes correctly in the browser environment but not in Googles headless environment
    - It is vitally important the this not happen as if a search engine cannot render a site, it cannot see it and thus cannot index it
  - There are some factors that can cause a search engine to stop rendering content
    - Important to remember that if there is no pre-render then there is no rendering at all
    - URL's that have been submitted have tags stopping indexing
    - There are blocked URL's in the robots.txt file
    - There was no page returned by a page (5** HTTP Codes)
    - There is a redirect chain happening
      - If there are more than 5 redirects then Google will stop looking
    - What is referred to as a soft 404 coded where a 200 OK status is returned but appears to be a 404 Page Not Found code
  - There are also some reasons that content will be rendered incorrectly
    - There is blocked Javascript, cookies or style files which can cause the below factors
      - Site text is too small to read
      - Clickable elements have been placed too close together
      - Website content is larger than the screen viewing
      - There are images with text that cannot be read
        - There should be alt tags available for all images
      - There are animations that convey meaning

Ranking
  -
  - A search engine will return a ranked list of results that best fits a search query
  - Because different search engines return different results a place in the rankings can change
    - When search engines change what they are looking for (Algorithm updates) this can also affect ranking results
  - In 2006 there were 200 signals that effect rankings, there are likely to be many more today
    - There were 8 main categories such as site, domain, page, brand etc and each have their own factors
    - In page for example there are factors such as keywords in the title, page loading speed, content length etc
  - There are too many categories to try and rank for each and everyone of them
    - It is best to adhere to good practice guidelines and follow standards which will take a site a large part of the way
  - Search engines try to determine intent
    - For example searching for a thai restaurant is likely to mean restaurants need your location rather than restauants in Thailand
  - There are several things that can be done to improve ranking
    - Follow the search engine webmaster guidelines
      - This is advice given by the search engines themselves so should be followed
    - Ensure that the site is quick to load
    - Sites should also be responsive namely mobile device friendly
    - Content should be useful, relevant, fresh and up to data as well as being authoritative
  - The above factors when taken together are known as Technical SEO
  - Google uses a link analysis algorithm known as PageRank to rank pages
  - PageRank is also known as Link Juice because of the way the rank flows from link to link
    - It also passes sometimes to pages it links to which is known as RankFlow
  - The higher the PageRank score, the higher it is likely to appears in SERP's
    - Other tools use similar metrics to Googles page rank
      - AHRefs uses URL rating
      - DeepCrawl uses DeepRank
      - Moz uses Page Authority
    - Links that have a nofollow tag will not count towards ranking
      - Backlinks are links to our site from other sites and to our pages from our other pages
      - These are important as pages gain importance the more that they are cited elsewhere
      - Links that are paid for or advert links may be penalised
  - Intent in the context of search engines is the intention of the searcher
    - Satisfying what the searcher is actually looking for is the primary goal of search engines
    - Content should be aimed at the searcher's intent too
  - Search intent can be classified into 4 types
    - Informational intent is one of the most common as the intent is to simply look for information
    - Commercial intent which is where a searcher is looking for information about something they are considering purchasing
    - Transactional intent which is closely related to commercial as a searcher is looking for a place to buy something they have decided to purchase
    - Navigational intent which is where a searcher is trying to find a specific website
    - The first 3 are related as they follow a searchers journey from searching for something to but to buying it
    - To help with intent, in June 2021 Google announced it's new development, the Multitask Uniformed Model
    - This approach uses artificial intelligence to gain insights into searcher intentions
    - The idea is to find what a searcher actually wants without having to make multiple searches 
    - One example is looking a landmarks in a foreign country, google aims to provide best ways to get them, recommended accomodation etc
    - This provides the opportunity to gain results from other languages as that will not be problem with Google translating
    - These sites are likely to be more authoritative if the searcher destination is located in their country
  - There are a number of ways of displaying content within search results and not just the standard list of results
    - Some other features that Google may choose to use are Featured snippet, Image Pack, Local Pack, Knowledge Panel, Featured Snippet
    - To appear in these formats it may be necessary to use structured data or schema in a page
    - There are multiple different types of schema that can be used, details are available at
      - https://schema.org/
    - The Google search gallery also lists the different types of schema that are currently used
    - There are different schema available for blogs, books, podcasts etc and all can increase website traffic <br/><br/>
 
<p align=center>Tools <br/>

Google Tools 
  -
  - Users should be logged in to their Google account in order to use these tools
  - Google Search Console (GSC)
    - https://search.google.com/search-console/about
    - This tool is used for several different things
      - Reviewing and monitoring site performance within organic and Discover
      - Review indexation of URL's
      - Review the reporting of warnings & errors regarding indexation within the index coverage report
      - It can be used to monitor sitemaps and see URL indexation within them
      - Errors, warnings and valid results from Rich Results can be seen
      - Examine crawl stats from a high level
      - Information on core web vitals as well as page experience can be viewed
      - How mobile compatible pages are can be checked
      - Manual actions and security issues as well as inspecting URL's
  - The index coverage report maybe the most important report when it comes to Technical SEO
    - Testing the live url within an error in the report allows for understanding the current status of a URL
    - Recrawling a page may fix errors due to temporary noindexing
  - Google Page Speed Insights
    - It is used to review not only page speed but core web vitals
    - It can highlight possible opportunities as well as provide detailed diagnostics when looking at performance
    - Can be useful in providing an overall performance audit
  - Google Lighthouse
    - This is what powers page speed insights
    - It is used to provide automation of core web vitals reviews
    - It can provide sitewide analysis for opportunities and diagnostics
  - Mobile Friendly Test
    - It tests the mobile friendliness of a site 
    - It will provide a fairly detailed report
    - There can also be testing for any mobile realted rendering issues
  - Rich Results Test
    - This is used for structured data testing
    - There seems to be better rendering than other tools
    - This tests for rich results inclusion and schema issues that result in rich results being implemented
    - The schema.org validator needs to be used when testing all schemas
    - The rich results test only tests schema types that result in rich results
  - Google Analytics
    - This is a very powerful tool and can be used on for report creation on live and past data
    - It can uncover insights into customer behaviour
    - It can report on performance, understand audience demographics, trend extraction

Bing Tools
  -
  - Using Google Search Console is the only way that verified data from Google searches can be gotten
  - There are benefits however to using Bing tool suite
    - Sanity checking of issues occuring in GSC or Google can be checked to see if it occurs in Bing tools
    - More data from multiple search engines can be beneficial
    - Bing search engine is not as big as Google but will still make a contribution to your search engine strategy
    - There is more information provided in Bing tools than the Google suite
  - There are some very useful tools in the Bing tools suite
    - Search Performance
    - URL Inspection
    - Site explorer
    - Sitemaps
    - SEO -- Backlinks, Keyword Research, SEO Report and Site Scan
    - Crawl controls
    - Robot.txt tester
  - There are a lot of similarities to the Google suite of tools
  - Bing does offer some functionality not offered by google such as viewing previously inspected URL's

3rd Party Tools
  -
  - There are a huge amount of 3rd party tools available to help with SEO
  - These tools combined with SEO tools will help get the best insight into a site as is possible
  - Using different tolls can highlight issues in certain areas and fixes or recommendations that can be made
  - Its important though to understand what a tool is saying as sometimes a tool can label something as critical and it isn't and vice versa
  - Schema Validator
    - https://validator.schema.org/
    - Google Rich Results test only test schemas that results is rich results within search
    - The schema validator will validate any schema that is in place on a site
    - It can be ran against any URL or JSON code
  - GTMetrix
    - https://gtmetrix.com/
    - It is an excellent site speed testing tool that shows a different view to the PageSpeed Insights tool
    - There are multiple features that make this tool worthwhile using such as the waterfall view, overall performance and structure test
    - There are numerous config settings such as location or device type
  - Web Page Test
    - https://www.webpagetest.org/
    - This is a similar tool to GMetrix
    - It offers core web vitals auditing, CDN check, testing from multiple locations
  - SiteBulb
    - https://sitebulb.com/
    - SiteBulb is a crawling tool for auditing websites
    - Simply enter a URL and the tools will crawl the domain a perform a large amount of tests
  - Screaming Frog
    - https://www.screamingfrog.co.uk/seo-spider/
    - This is one of the most well know SEO tools
    - There is a very good log file analyser in addition to the crawler
  - ContentKing
    - https://www.contentkingapp.com/
    - This is a real-time SEO auditing and monitoring platform
    - Users can monitor multiple sites as well as reporting on a number of different areas
    - It integrates with GSC and Google Analytics and can export to Data Studio
    - It can identify issues before they cause problems
  - Python & SQL
    - Python can be used to speed up data analysis quite considerably
      - It shines when finding trends and other information from larg crawl files
    - SQL is also very useful when analysing data
      - Recommendations for using =QUERY which is a SQL based formula which saves a huge amount of time when bringing in large spreadsheet dats
  - Plugins
    - There are several highly recommended Chrome plugins that should be looked at in addition to tools
    - SEO Meta in One Click
      - Allows users to see the high level SEO features of a page
    - AYIMA Redirect Path
      - Allows for seeing page status codes as well as any redirects that were needed 
      - Useful information froma high level especially when testing individual pages
    - User Agent Switcher
      - Users can switch between agents such as Chrome and Googlebot
      - Useful when checking to see if different agents are seeing different versions of a site
    - View Rendered Source
      - Source code can be compared against rendered source code 
      - It allows users to see which elements are rendered using JavaScript
    - Axe Dev Tools
      - Axe is used for accessibility testing
    - Web Developer
      - Offers a huge amount of functionality such as highlighting links, showing alt attributes
  

<p align=center>Crawling Technology <br />
  
Why We Crawl
  -
  - Google constantly crawls the web to understand the relationship between websites and to help them rank content
  - It does this by following links that are in place on a site as well as going through sitemaps and any URL's that have been directly submitted
  - The company has huge server farms which constantly crawl the web which update as they go
  - SEOs perform something similar to Google albeit on a much smaller scale
    - They do this to be able to better understand the technical setup and layout of a site
  - There are multiple software applications that allow for simply entering a URL and will crawl a website based on configurations
    - These will return data, normally this is in the form of reports but graphics will be available on most as well
  - There are different options when looking at crawling software
    - Cloud based which are more expensive but more effective at crawling enterprise websites
      - DeepCrawl - https://www.lumar.io/
      - OnCrawl - https://www.oncrawl.com/
      - Botify - https://www.botify.com/
      - ContentKing - https://www.contentkingapp.com/
    - Local Based
      - ScreamingFrog - https://www.screamingfrog.co.uk/seo-spider/
      - Xenu Link Sleuth - https://home.snafu.de/tilman/xenulink.html
      - SiteBulb - https://sitebulb.com/
  - A huge amount of information can be gotten from crawling a site including
    - Overall site information architecture and how pages link or don't
    - Internally linked redirects and where they are linked from
    - Any internal 404's that maybe occuring and reducing them
    - MetaData that is above or below certain expected levels
    - Any duplicate content that is on a site
      - Hashing will show any pages that are nearly identical 
      - Some tools eg SiteBulb will highlight pages that suffer from a portion of duplicate content
    - Security issues which may affect HTTP
      - Any pages that use protocol relative URL's which is a potential security issue
      - https://en.wikipedia.org/wiki/Wikipedia:Protocol-relative_URL
    - HRef lang set so that internationalisation strategy can be audited
    - Schema auditing for any schema related issues that may be happening

Crawling with SiteBulb
  - Demo using the SiteBulb tool

How to interpret and use data
  - Getting the most from the data generated by SiteBulb reports <br/><br/>


<p align=center>Page Experience <br/>
  
Page Speed
  -
  - Faster download of page material means better user experience (UX), less bandwidth and less server work
  - Google reckons 53% of visits to sites are abandoned after 3 seconds
  - PageSpeed Insights is a tool which can be used to check page load speed
  - There are multiple ways to improve download speed such as:
    - Use a content delivery network
    - Optimise and serve any images correctly
    - Remove any assets that are not needed
    - Use a compression technique 
  - Mobile vs Desktop devices
    - It used to be the case that the web was accessed using computers with big monitors
    - Mobile devices were not available and many sites were designed with this reality in mind
    - Layouts were often fixed to common screen resolutions
    - As monitor size increased so did the issues with some sites looking squashed as they only took a small area
    - When smartphones started to come in developers started to develop sites that responded to different screen sizes
    - Enhancements to HTML and CSS were made to allow for these new screen sizes
    - Things such as tags and attributes allowed for graceful responsiveness to different screens
    - Google responded by implementing mobile first indexing
      - It is very important to note that this is mobile first not mobile only 
    - This means that it is always beneficial to have websites work well with desktop and mobile devices
    - Sites that cater only to desktop devices will still be in the index
      - There will however be ranking penalties as these sites will not rank as high as sites that cater for both mobile and desktop devices
    - Even though mobile sites were available the desktop was still considered the primary version of a site
      - Some mobile versions of sites were stripped down versions of the desktop sites
    - Mobile first as made mobile site versions the primary version of a site
    - A good design will work well for all platforms and can ensure good page load speed
  - Page Load Speed Importance
    - Good load speeds for a page is important for a few reasons
    - It provides a better experience for users
    - There are lower bounce rates and higher conversion rates for sites
    - Search rankings will be higher the quicker pages load
    - There are cost savings as well as being environmentally friendly
    - The techniques for making pages load faster also result in smaller files, less bandwidth and fewer CPU cycles
  - Tools
    - There are numerous tools available for help improve page load times
    - They include GTMetrix, GSC, Lighthouse, PageSpeed Insights, Digital Beacon
    - There are many areas that can be tackled to improve page loading speeds
      - They include using a CDN, caching files, using optimised image sizes 
  - Data Types
    - When using tools to measure page experience be careful to check scores are using lab data or field data
      - Lab Data which is data that has been obtained in a controlled manner using predetermined devices and configurations
        - Tools include PageSpeed Insights, Lighthouse, Dev Tools, GTMetrix
      - Field Data is actual data for real visitor to the site
        - Tools include PageSpeed Insight, GSC, BigQuery project, CrUX report
        - CrUX stands for Chrome User Experience Report
        - https://developer.chrome.com/docs/crux/
        - This is a Google database which has real measurements taken from users of Chrome browser
        - These users must have opted into syncing their browser history
        - CrUX data is used in PageSpeed Insights
    - Sometimes field data is more important than lab data
      - One case is when a site's users are predominantly local 

Core Web Vitals
  -
  - Web vitals are a series of metrics devised by Google
  - Core web vitals are the metrics that are most valuable for good UX
  - It uses field data from the CrUX database
  - There is a focus on 3 areas of the user experience
    - Loading, interactivity and visual stability
    - These are First Contentful Paint (FCP), Largest Contentful Paint (LCP), First Input Delay (FID) and Cumulative Layout Shift (CLS)
    - Underlying metrics include time to interactive, total blocking time, speed index and others
  - It is important with mobile first that any issues with mobile versions of sites are fixed
    - However ensure that any issues with desktop versions of sites are fixed too
  - The phrase Good Page Experience should be kept in mind
  - Google classifies sites into 3 categories
    - Good, needs improvement and poor
    - Google recommends that a site be in the top 75th percentile to be considered as a good page experience
    - These metrics should be exceeded wherever possible
  - FCP First Contentful Paint
    - This measures the time it takes from an initial request of a page until the time some content appears in the viewport
    - Content in this case means text, images, anything visible counts as content
    - FCP is the time taken from request until things appear on screen
    - FCP should be equal to or below 1.8 seconds to qualify as a good user experience
  - LCP or Largest Contentful Paint
    - This is very similar to FCP
    - The difference is that LCP is the time from request to the largest content piece appearing on screen
    - Again this can be an image or a text area
    - Elements that are considered for LCP are image or video elements, background images loaded using the URL function and block elements containing text
    - The size to be used to determine the largest element is based on the size of the image displayed
      - It is not the actual image size but how it appears in the viewport 
      - CSS features such as borders and padding are not used in the calculation
    - LCP should be equal to or below 2.5 seconds to qualify as a good user experience
  - FID or First Input Delay
    - This is not the time the page takes until it is able to accept user input
    - It is the time it takes from when a user starts to interact with the page until the page starts to respond
    - To have a good user experience FID should be under 100ms
  - CLS or Cumulative Layout Shift
    - This is a metric which measures visual stability within a page
    - It is of particular importance to mobile sites
    - Sometimes layout shifts happen when trying to go to a certain part of a site and endup somewhere else, this can be more than inconvienient
    - CLS issues are classed as bad user experiences and Google uses CLS as a ranking factor
    - CLS is measured by looking at unexpected layout shifts and how much the layout shifts by
    - A burst of shifts is when a group of shifts occur with less than a second between them
    - The largest of the shifts is used to calculate the CLS metric
    - Largest is defined as being the burst with the largest cumulative score of all shifts in the burst
    - Google used to use all shifts in calculations but changed in 2021
    - A CLS score of 0.1 or less is classed as a good user experience
    - To calculate the score the input fraction is mutiplied by the distance fraction
      - This is the size of an element that has moved between frames and how far it has moved 
    - The larger the asset that moves and the furthre it moves the worse the CLS score will be
    - Layout shift had been resolved but in order to make pages more responsive CSS was used which reintroduced the problem
    - Image optimisation can again help lower CLS scores
  - Tools for measuring Core Web Vitals (CWV's)
    - PageSpeed Insights is an excellent tool for measuring CWV's, GTMetrix is another tool, automated tools have some benefits
      - They can measure from multiple locations and use different devices
      - These tools can diagnose performance issues
      - They will display page loading timelines
      - They will identify protential areas for improvement, list areas for attention and calculate potential savings.
      - They will also provide a list of site diagnostics
  - It is recommended that site owners sign up for GSC
    - If working with a client then ask for access to their console
  - Page Load Timelines
    - These are visual reports that show a page at various points during it's loading
  - Digital Beacon
    - https://digitalbeacon.co/
    - This is a tool which calculates the envionmental impact and carbon footprint of a site
    - It also shows what improvements can be made to make a site even more environmentally friendly

Mobile Usability
  -
  - Mobile Usability is how easy it is for a visitor to interact with a mobile site
  - It is very important as a large portion of site visits are with a mobile device
  - Over 50% of users access a site through a mobile device so this is why getting a device mobile friendly is more important than the desktop
  - Check for Mobile Usability issues
    - Using GSC this is a easy as selecting Mobile Usability from the left hand side menu
  - Errors Reported by GSC
    - Incompatible plugins such as Flash being used on a page
      - Flash is not support be the majority of mobile browsers
    - The viewport is not set which stops the browser from being able to adjust dimensions
    - The viewport is set to a fixed width rather than a device width 
    - Content wider than the screen usually caused by CSS using fixed width blocks which then causes content to go off screen
    - Text too small to read which can happen if it is reduced too much from when a large monitor is used
    - Clickable elements on the screen are too close together
  - Once errors have been fixed it is important to remember to request Google recrawl the site
    - This can be done by clicking the validate fix button
  - Fixing Mobile Usability Errors
    - First take a look at the Summary page and examine the reported errors
    - Are any of the errors to do with the site templates, look for error spikes appearing
    - Check if a single issue may be causing multiple errors
    - Applying a fix here might fix multiple errors
    - Fix errors that are affecting  the most pages or important pages
    - If in doubt then fix errors in the order that GSC presents them as GSC prioritise them

Security
  -
  - HTTPS - Hypertext Transfer Protocol Secure
    - This protocol is the secure version of the HTTP protocol as HTTP is insecure
    - It is the protocol that transfers documents
    - HTTP delivers HTML pages, other data including meta data
    - It can be used to redirect a request to another
    - It is used to encrypt the delivery of webpages
    - It also secures incoming communications and outgoing data
  - Why is HTTPS needed
    - To protect the privacy and security of both visitors as well as site owners
    - The site can be verified, authenticity is important
    - It reduces the risks of malware infections
    - Visitors to the site can be more confident which helps with branding
    - Using HTTPS is an SEO ranking factor
  - SSL/TLS
    - SSL and TLS are the technologies that are used to secure HTTPS
      - SSL or Secure Sockets Layer is the older technology
      - TLS or Transport Layer Security
    - TLS is preferable to any version of SSL
      - Using SSL is definitely better than using plain HTTP
      - TLS is considered as being more secure as well as being faster
      - They use a combination of symmetric and asymmetric cryptography
    - Always use the most recent version of the protocol where possible
    - A digital certificate called an SSL certificate is generated on the server to use HTTPS
      - This is then used at the beginning of all future communications
      - The certificate has a pair of keys, public and private
    - SSL certificates are issued by a Certificate Authority or CA
      - A CA is vetted to ensure that they are trustworthy
      - Certificates are only issued to verified requesters
    - Once the certificate is in place then secure communications can take place
      - TLS and SSL use asymmetric encryption to start a session and decide upon a single shared key
      - This key can be used for the remainder of the session
      - The way that the key is used is called symmetric encryption
    - Client communicating with the server
      - Firstly a secure connection is requested by the client
      - The server responds to the request with the SSL cert, which allows for verification of server legitimacy
      - Responding to the server results in a secure connection being established
      - All communications at this point at encrypted and secure
    - SEO does have some reasons for using HTTPS
      - HTTPS has been a ranking factor on Google since 2014
        - Impact was small but maybe getting larger and is often a tie breaker between 2 similar sites
        - Many small factors together can have a large impact on SERP positioning
      - Reduced visitor bounce rate due to no longer having warnings about site (in)security 
      - There is better referral data in Google using HTTPS <br/> <br/>
  
<p align=center>Schema <br/>
  
What is Structured Data
  -
  - Structured Data is also known as Schema Markup Language
  - Structured Data is organised pieces of information in the form of code snippets
    - It helps search engines better understand what the website content is about
    - They can also appear as rich results on SERP's
  - It is the result of a collaboration between Google, Bing and Yahoo
  - It provides information to search engines so that they have much more clarity about a site content
  - Purpose of Structured Data
    - Its primary purpose is to help search engines properly understand a site's content better than normal text
      - The meaning of content is conveyed to a search engine much more efficiently
    - Structured Data can be used to enhance the appearence of a website on a search result page
      - This includes using code to display logos, contact information, events or other information in the SERP's
    - Users are provided with better and more accurate information in rich snippets which will benefit them
      - This can increase the click through rate (CTR) of a page
  - What Structured Data looks Like
    - Structured Data can be implemented as markup on a page by using one of the 3 major formats
      - JSON-LD
        - This acronym stands for JavaScript Object Notation for Link Data
        - It is the most popular markup script that can be used for structured data
        - Google has JSON-LD as its preferred version for structured data
        - Its main advantage is that it can be implemented as a block of code without messing up the rest of the HTML doc
        - The scripts can be copied and pasted into either the head or body of the page
        - This approach keeps the page both clean and well structured
      - Microdata
        - It is based on a set of tags that highlight values and items on-page for structured data individually
        - Types and properties of structured data are marked up by itemtype and itemprop HTML attributes
        - It's major downside is that every entity or attribute of the content has to be marked up individually
        - This is done within the HTML body of pages 
        - This means that is can be very disorganised and difficult to maintain
        - This is opposed to one big block of code used in JSON-LD
      - RDFa
        - This stands for Resource Description Framework in Attributes
        - This is an extension to HTML5
        - It can be used to markup items for structured data and is very similar to microdata
        - It marks up content pieces in the HTML body of a page
        - It uses the typeof and property HTML attributes when marking up content
  - Why is Structured Data important for SEO
    - Structured data provides a lot more detailed information to Google about a page
    - Google has said that the better a schema is, the better results would be for users

How to choose Structured Data
  -
  - There are many types of Structured Data that can be applied to web pages
    - Event, Review, Recipes, FAQ's and Job Posting articles that appear directly on Google
  - The type you choose will depend on the type of site
  - When searching for events for example the SERP's will return some events with rich results
    - These rich results will show several facts about an event, date, time, address, thumbnail of event images etc
  - When looking at review websites it is different
    - The rich results here will show what is the review of the item being searched for, rating such as stars and marks as well as the reviewer
  - Recipe websites will again differ
    - These rich results will show ingredients, reviews, marks and stars and links to other similar recipes
    - Recipe rsults can also be improved with a few more properties to be guided recipe schema
    - These guided schema will work with Google home and smart displays
      - These results will provided step by step tutorials on whatever recipes are being searched for
  - FAQ types of Structured Data
    - This will provided a list of questions and answers related to a searched topic
    - This can be useful for most websites
  - Job posting Structured Data
    - This has a huge benefit as job specs can be displayed in the job search experience on Google
    - It will show a thubmnail logo, company reviews, ratings and job details
    - It enables users to filter by various factors meaning more and better applicants who are searching for exactly the posting
  - Google has a gallery where each type of schema supported is listed and examples are shown
    - https://developers.google.com/search/docs/appearance/structured-data/search-gallery
  - It can be difficult at times to describe pages via Structured Data
    - This is especially true when there is an endless list of types and properties that can describe a site
  - There are some best practices that should be kept in mind
    - Use the most specific, applicable types and properties that are possible
    - Prioritize any pages that are already ranking high in the search results
      - This increases the likelihood that they will appear as rich results
    - Supply fewer but complete and accurate properties
      - This is opposite to many but badly put together properties that will not inform or help search engines
    - If possible use the JSON-LD format when implementing Structured Data
  - There are a few options available when implementing Structured Data
    - Markup Generators
      - There are many online markup generators available for automatically generating Structured Data
      - The most popular are
        - Google Structured Data Markup Helper - https://www.google.com/webmasters/markup-helper/
        - Google Data Highlighter - https://www.google.com/webmasters/data-highlighter/sources
        - Schema Markup Generator (JSON-LD) Merkle - https://technicalseo.com/tools/schema-markup-generator/
    - Schema Plugins if using a CMS
      - These are a great way for creating Structured Data on a website
      - Yoast SEO - https://yoast.com/
      - Yoast WooCommerce SEO - https://yoast.com/wordpress/plugins/yoast-woocommerce-seo/
      - Schema App - https://www.schemaapp.com/
      - Checkout the official Wordpress site to see more plugins
  
Testing Implementation
  -
  - Google Rich Results Test tool
    - https://search.google.com/test/rich-results
  - Structured Data Testing involves using Structured Data to monitor the health of a page
  - The testing tools provide insights on a page health that is visible to a search engine
  - Testing will help to both identify and resolve any issues with Structured Data
  - Tools will help test Structured Data as it is being deployed
    - They will also help validate Structured Data and snippets on a site
  - The testing tools will provide a lot of information including
    - What format the Structured Data is using
    - Are there any errors in the Structured Data
    - What are the details of any issues with Structured Data
  - Structured Data testing is of critical importance
    - Even a single issue can prevent Google from reading the markup
    - GSC will highlight any warnings that relate to wrong or missing code in the markup
    - In some cases a warning ig given when a single field is not filled out
    - It will allow for detection of errors that are not visible on inspection
    - Testing will multiple tools is the recommended practice for proper implementation
  - The 2 main tools are Google Rich Results Test tool and Schema.org validator
    - https://validator.schema.org/
    - The rich results tool is a simple no-frills data testing tool
    - It is designed to replace the tool the preceeded it
    - Rich results tests enable either code snippets or URL's to be validated
    - As the rich results tool will tell whether a site can use rich results or not, it is still a valuable tool to use alongside others 
    - The rich results tool will show a preview of the page with Structured Data applied
<br /> <br />
  
<p align=center>Status Codes<br />

What are HTTP status codes
  -
  - HTTP is an acronym standing for HyperText Transfer Protocol
    - It is a protocol used in the exchange of data by clients and servers
  - A HTTP status code is a server's response to a browser request
  - Understanding status codes will help diagnose any site errors quickly
    - This will help in minimising downtime on a site
    - These codes can even help search engines when people access a site
  - The first digit of each code is a number from 1 - 5
    - 1xx or 5xx etc are used to demonstrate codes that would be in that range
  - Each range will cover a different class of server responses
  - When a body payload is present in a HTTP responsed it is used to render and display a page to a user
    - The body payload is optional for some errors
    - There is no body payload for redirects
  - There are 5 classes of HTTP codes, each one is a group of similar or related code meanings
    - Knowing these can help with understanding the general substance of an error code
    - 1xx are informal codes indicating that the request indicated by the browser is continuing
    - 2xx are success codes returned when the browser was received, understood and processed
    - 3xx are redirection codes which are returned when a resource has been substituted for the requested resource
    - 4xx are client errors indicating that there was an issue with the request
    - 5xx are server errors indicating that requests were received but the server could not fullfil the request
  - Within each class are a variety of errors which maybe returned by the server
    - Each individual code has a specific meaning
  - Why HTTP errors matter for SEO
    - The main goal of SEO is to drive organic traffic to a site
    - In order to get traffic to a site, content must be accessible to search engine crawler
    - Sites need to return codes indicating that everything is OK
    - Returning codes indicating an issue is not what any site wants (5xx and 4xx)
    - Keep 3xx codes to a minimum
  - HTTP codes can impact how a site is crawled and indexed by a search engine
    - They also affect how search engines see the health of a website
    - 1xx and 2xx codes will not have much SEO impact
    - 3xx codes can have a complicated relationship with SEO
    - 4xx and 5xx codes can prevent bots from crawling and indexing a site
      - Too many of these can indicate that a site is not of good quality

200 OK
  -
  - This is the best type of HTTP code to receive
    - A 200 level response says that everything is working as it should
  - Incorrect use of HTTP Status Code 200
    - A soft 404 is when a page does not exist according to a website but a 200 response is returned
    - This happens on a site when a 404 Page Not Found is created without returning a 404 response code
    - This is when a site indicates a page does not exist to users but indicating it does to search engines
    - Soft 404 codes can be damaging to a site's performance on search engines
    - Soft 404 pages are generally thin content pages
  - 201 code means created
    - A server has fulfilled a browser request and created a new resource
  - 202 codes means accepted
    - A server has accepted a browser request but is still processing, it may or may not result in a response
  - 203 code means Non-Authoritative Information
    - This code may appear when a proxy is being used
    - It means that the proxy server received a 200 code but has modified the response prior to passing it to the browser
  - 204 code means No Content
    - It means that the sever has successfully processed the request but will not return any content
  - 205 code means Reset Content
    - It is similar to 204 in that the server will not return content after successfully processing a request
    - However it also requires that the browser resets a document view
  - 206 code means Partial Content
    - You might see this code if the HTTP client uses range headers
    - https://www.geeksforgeeks.org/http-headers-range/
    - This enables browsers to resume paused downloads
    - It also enables browsers to split downloads into multiple streams
    - It is caused when a range header causes the server to send only part of a requested resource

3xx Redirects
  -
  - 300 Status codes
    - Redirection is the process that is used to communicate that a resource has moved
    - There are several HTTP codes that will accompany redirections so that user can have information on where the required resource has moved to
  - 301 Status Code
    - This indicates that a resource has permanently moved to a new location
    - The request and all future requests must go to a different URL instead of the requested resource
    - All signals from the requested URL are passed to the destination URL because of the permanence of the redirection
  - 302 Status Code
    - This code indicates that a resource has temporarily moved to another location
    - Because this is temporary no popularity or relevance signals are passed on to the destination URL
    - The redirect URL will still rank instead of being consolidated
    - After a certain amount of time 302 redirects will be treated as 301's by search engines
  - 303 Status Code
    - This indicates that a server is redirecting a request URL to a different URL
    - One use is to prevent users from accidentally re-submitting forms when using a browser back button
    - This redirect tells that a follow-up regards to the temporary URL should be made using the GET HTTP method
  - 304 Status Code
    - This code indicates that the resource has not been modified since the last request
    - When this happens the request is returned to the client as the cached version can be used
  - 307 Status Code
    - This is the HTTP version 1.1 equivalent of 302 code
    - This code guarantees that no change to the HTTP method being used will happen but 302 does not
    - This code was invented to ensure that a HTTP method used to make a request did not change when a server redirected
    - 307 is also used in internal redirects where the browser is aware that HTTPS is enforced
      - The browser will know this from previous requests or if the domain is on a HSTS preload list
      - https://hstspreload.org/
      - https://support.openprovider.eu/hc/en-us/articles/360000350268-What-are-HSTS-and-the-HSTS-preload-list-
      - Using this prevents unecessary and unsafe requests
  - 308 Status Code
    - This is the HTTP version 1.1 equivalent of 301 code
    - It does not allow for changing POST to GET and 301 is recommended instead
  - JavaScript Reload
    - This is a client-side redirect that instructs browsers to load another URL
    - The function window.location.replace is recommended
    - It will send a user to a specified site upon page load 
    - It is not recommended to use these from an SEO perspective
      - Search engines are slow to pick these redirects up as the page needs to be rendered to find the redirect
      - There is no guarantee that the redirect is picked up correctly
      - JavaScript redirects make for slower user experience

4xx Client Errors
  -
  - 4xx status codes indicate that there was an error on the client side
  - Many 4xx HTTP status codes have a large significance for SEO
  - 401 Status Code
    - This code is an error and describes that HTTP authentication failed
    - The requested page requires a password/username combination and/or isn't allowed access based on it's IP address
    - This error is typically seen when crawlers try to access staging environments and HTTP authenication is implemented to prevent this
    - 401 in this case is what is wanted to be seen otherwise search engines would be indexing staging environments
    - This could lead to 3rd parties accessing priviliged information
  - 403 Status Code
    - This code indicates that it is forbidden to request a URL
    - It is commonly used as a permanent measure to prevent crawlers from making requests after misbehaving
    - It is also returned in case the client provides wrong login credentials
    - 401 and 403 are used in different situations
    - A common example is when rogue crawlers tyr to request too many URL's on a site
    - A common misuse is when 403 errors are used instead of using 401 errors
  - 404 Status Code
    - This code indicates that a requested resource cannot be found
    - It is the most common code that is seen by web users
      - This is because it is the code that they see when clicking on broken links
    - 404 errors can also incorrectly occur for existing content
      - It can happen when there is maintenance being done on a server or a bug from development 
    - Search engines will drop from the index pages after hitting 404 errors a few times
    - 404 errors are also bad from a user experience viewpoint
  - 410 Status Code
    - This indicates that a requested URL has been permanently removed
    - Search engines will quickly remove 410 pages from the index due the page not coming back
    - The above fact makes this code a very powerful tool for an SEO to have at their disposal
  - 429 Status Code
    - This code indicates that a client has been making too many requests within a certain amount of time
    - It can be seen as a temporary version of the 403 code
    - Google may remove pages that are returning 429 codes for too long
    - 403 should be used instead of 429 when fighting off rogue crawlers

5xx Server Errors
  -
  - 5xx codes happen when clients make valid requests but servers are unable to complete them
  - If sites keep returning 5xx codes this will have an effect on SEO
    - Crawling will be slowed which means a reduction in crawl budget
    - Pages can also be demoted within the index or removed if issues are persistent
  - 500 Status Code
    - This code indicates that a server had issues when processing a request
    - The server is not able to explicitly state what the error that has occurred is
    - The 500 code is therefore a generic Internal Server Error
    - It is usually generated by faulty plugins, PHP issues or broken db connections
  - 501 Status Code
    - This code occurs when a server does not support the functionality required to process a request
    - It can also happen when a server does not recognise a request method
    - It is also a generic code that effectively means that something has not been implemented
    - There is also an implicit part in that the functionality maybe implemented in the future
  - 502 Status Code
    - This error occurs when there are proxy servers that have not been configured properly
    - It may also occur when there are client servers overloaded or even when there is poor IP communication or even firewall issues
    - This code happens when a server acting as a proxy\gateway gets an incorrect response from an upstream server
    - This is a generic code that simply means Bad Gateway
  - 503 Status Code
    - This indicates that a server is temporarily unavailable but will be available again soon
    - It can happen when scheduled maintenance is happening or if a server is too busy
    - It allows for inclusion of a retry-after value in the response
    - It is similar to 429 code
  - 505 Status Code
    - This code happens when a web server does not support the HTTP version that has been specified
    - It can be caused by the protocol version not being specified properly by the client computer in the request
    - The error that occurs should contain an explanation of why that HTTP version is not supported 
 
  
<p align=center>Content & On-Page <br />
  
On-Page Basics
  -
  - What is On-Page SEO
    - On-Page SEO is everything that appears within the HTML of a page
    - It includes Metadata, Body Content, Content Quality, Headers, Images, EAT signals
      - https://www.semrush.com/blog/eat-and-ymyl-new-google-search-guidelines-acronyms-of-quality-content/
    - It is one of the most important signals that a site can have
    - If all on-page signals are bad then content will not easily appear within SERP's
  - Viewing And Understanding HTML
    - https://www.w3schools.com/html/
    - This is the most important skill for an SEO to have
    - You should be able to look at a page, analyse the HTML and understand what is happening
    - A lot of information about a site can be revealed by looking at the HTML
    - This is not always the case ie some newer JavaScript frameworks such as Angular and React handle pages differently 
    - To view the HTML of a page, open it up then right click and select View Page Source
    - There is also right click and select Inspect
    - There are some differences between these 2 different methods
      - View Source shows the raw HTML that was delivered from the server to the browser
      - Inspect allow for examination of the DOM tree after JS has manipulated the HTML and HTML errors have been fixed
      - There is also an extension called View Rendered Source which allows for seeing pre and post rendered page versions 
      - https://chrome.google.com/webstore/detail/view-rendered-source/ejgngohbdedoabanmclafpkoogegdpob?hl=en
  - The DOM
    - DOM is an acronym standing for Document Object Model
    - It shows the overall structure of a webpage split into logical sections
      - These are bulit with HTML attributes and elements
      - https://medium.com/crowdbotics/understanding-the-dom-e137c18f3cba
    - The DOM is not what you see in the browser
    - What is seen in the browser is the render tree which is a cobination of the DOM and the CSSOM
      - https://developer.mozilla.org/en-US/docs/Web/API/CSS_Object_Model
    - What separates the DOM from the render tree is that the render tree consists of what will be painted on screen
    - The render tree is only concerned with what is rendered, it excludes visually hidden elements
      - Elements with a display:none styling will not appear
    - Where there is invalid HTML or the DOM has been modified by JS is where differences between View Source and the DOM are seen
  - How are Webpages built
    - The DOM is built from the raw HTML response
    - After that then the CSSOM is built
      - This is a representation of the styles that are associated with the DOM
      - It is represented similarly to the DOM but with the associated styles for each node
      - This is whether they are explicitly declared or inherited
    - JS is then run against the DOM and the CSSOM and changes are applied
    - Then the DOM and CSSOM are combined to create a render tree
    - Then they layout is worked out, this determines what the viewport size is
    - This provides context for the CSS styles which are dependent on it (eg Viewport units)
    - Viewport size is determined by the meta viewport tag found in the document head
    - If there is no tag provided there will be a default viewport width
    - Finally the content is then painted to the browser
  - Viewing HTML
    - There are a number of useful pieces of information seen by viewing the source code or DOM
      - The head which contains directives such as Hreflang, meta tags, Schema, Canonicals
      - Images and Alt text
      - The actual content that is in place
      - Headers such as H1, H2
    - The DOM and HTML should be similar unless a JS framework is used which would create difference
      - The DOM is built from HTML, when a JS framework is used there is not a lot of HTML returned 
      - There is a large amount of JS code though which will pull in most of what is considered on-page
      - Any site using JS like this, viewing the DOM should prove more useful than just looking at the source code
      - In the above case inspect is the preferred method rather than View Source
  - Common On-Page Issues
    - There are a number of on-page issues that are worth exploring
    - Metadata issues whether duplicate, long, short or missing altogether
      - Also there may be multiple meta titles or meta descriptions in place
    - Images where they are large and\or unoptimised
      - They may also be missing relevant alt text
    - Canonical tags
    - Hreflang implementation issues
    - AMP issues, AMP standing for Accelerated Mobile Pages
      - https://www.searchenginejournal.com/ranking-factors/amp-google/
    - Internal Links such as anchor text, redirects etc
    - Issues with content
  - On Page Best Practice
    - There are a large number of relevant on-page factors but some best practice guidelines are
    - Ensure that every page offers users a uniques value
      - Use the Google Quality Rater guidelines as well as observing EAT signals
      - https://www.searchenginejournal.com/google-eat/quality-raters-guidelines/
    - Ensure that all internal linking is good
      - Have good information architect as well as good linking structures both vertically and horizontally
    - Ensure the if canonicals are being made use of that they reference a correct location
    - Ensure that href lang not only works but is error free
    - Both metadata and images should be optimised
  
Thin Content 
  -
  - Thin Content is usually defined as content that offers little to no value to users
    - If content is of little to no value to the user then it will be the same to Google
    - The term Thin Content is used interchangeably with Low Quality Content
    - It can be seen in a number of locations on many, many websites
    - It usually involves minimal text and content on a page and can be accidental or the result of oversight
  - Thin Content -- Google
    - The Panda algorithm created by Google in 2011 targeted this type of content and impacted nearly 12% of all queries
    - Numerous previously high ranked sites with low quality, duplicated, sometimes spammy and keyword stuffed content were affected
    - There have been studies since the release and they show that sites that were affected have certain qualities
      - The sites have less attractive designs
      - Ads on the sites are more intrusive than they should be
      - There are either inflated or low word counts
      - The editorial quality of the site is very poor
      - There is a large amount of content repetition]
      - The content is not helpful or trustworthy and seems to be poorly researched
    - These factors were a precursor to EAT updates
    - Google Phantom which opeated from 2015 - 2017
      - This affect sites which displayed lower quality content
      - This was not officially confirmed by Google
    - After this Google documentation states that they will focus on users to provide the best possible experience
    - The Quality Rater Guidelines (QRGs) mention Quality many times, it's purpose is to ensure that search results are the highest quality
      - Quality Raters follow these guidelines to rate and test the effects of algorithm changes on search results
      - This feedback does not directly affect SERP rankings but helps Googhle understand changes that are seen
    - Any content that is deemed to be of lower quality will not stay long in the upper positions in SERP's
    - EAT which stands for Expertise, Authoritativeness and Trustworthiness
      - This is also touched on by the Raters Guidlines
      - After the Google Medic algorithm update in 2018 this term gained popularity
      - It is referred to in a Google blog post from August 2019
      - The most comprehensive resource comes from Marie Haynes
        - https://www.mariehaynes.com/resources/eat/
      - If a website is a medical, legal or financial site then having good EAT is essential
        - These type of sites are known as Your Money Your Life (YMYL) sites
        - Sites that offer advice maybe considered as YMYL if they help users make important decisions
        - Some sites that sell certain types of products may also be YMYL sites
    - Outside of the QRG Google has documentation which has a number of scenarios that show what would be Thin Content
      - Automatically Generated Content
      - Thin Afilliate Pages
      - Content that has been scraped
      - Doorway pages which are pages that are specifically created to rank using similar keywords
        - https://searchengineland.com/doorway-pages-seo-deep-dive-389786
      - If a site has some or all of these then they will likely lose search position
  - How to identify Thin Content
    - There are a number of methods that can be used to identify Thin Content
    - The most popular and arguably the easiest is to use the Screaming Frog tool
    - Although not a perfect indication, having low word count is indicative of having Thin Content
    - Normally 200 -300 words per page is the minimum
      - https://datayze.com/thin-content-checker
    - Other methods include having a review of the site analytics
      - Pages with a high bounce rate will often offer less value to users
    - Review how site content is formatted
      - Any content or headers that are in place should be logical and make sense
    - Is there any ad-pressure occuring
      - This is when a lot of low quality or spammy ads are seen
      - Is the percentage of the page taken up with ads more then the percentage used by actual content
    - Does your site use pop-ups
      - Google actively recommends against using popups especially on mobile devices
      - This is changing as they will take page experience factors into account for desktop rankings
      - Intrusive dialogs can make it hard for Google to understand a site's content
      - Too many popups will drive users from a site and they may not return
    - Content that is heavily duplicated
      - Having a lot of duplicate content will tell search engines that a site does not offer unique value to a user
      - This may lead Google to decide that there is no point having the site high in the SERP's
    - Check content for grammatical and spelling errors
      - Screaming Frog and other similar tools can be used to do this
  - How to fix Thin Content
    - The easiest is to add more useful and valuable content to a site
    - This approach is the recommended on where pages have word counts that are low
    - If content cannot be added then using a noindex tag to stop search engine indexing and displaying thin content in SERP's should be looked at
    - Other methods include reducing ad numbers, removing interstitials, improving content EAT and ensuring that all pieces add value to a user

Duplicate Content
  -
  - What is Duplicate Content
    - This is where content is duplicated either on your own website or copied on an external domain
    - If the same content appears in more than one location that is duplicate content
    - Minimal duplicate content is not a major issue
    - It only becomes an issue when it happens at scale.
    - Large amount of duplicated content can hurt a site potential
  - What impact can Duplicate Content have
    - There is no specific penalty from Google but it can make a site structure harder to understand
    - Excess duplicated content from other sources can indicate overall site quality is low
    - Being seen as a low quality site will hurt site visibility in the SERP's
  - Internal Duplication
    - If content duplication is internal then a site competes against itself in the SERP's
    - There are some steps that can be performed to check for internal duplication
      - Firstly perform a search for a specific piece of content 
        - This can be done using the site:example.com operator on Google and a piece of content
      - If multiple pages appear then there maybe an issue to fix
      - It may also be worth using the site: operator again but this time for a term rather than a content piece
  - How can Internal Duplication occur
    - There are multiple reasons that Internal Duplication occur
      - Poorly handled site parameters
      - There maybe both HTTP and HTTPS versions of URL's
      - There maybe .html on the end of some URL's
      - There maybe PDF files which are duplicates of HTML content
      - There might be both trailing and non-trailing slash versions of URL's
      - URL's that use different letter cases eg /subfolder vs /SUBFOLDER
      - Internationalisation that is poorly implemented ie no hreflang
  - External Duplication
    - If content duplication is external the 2(+) sources are competing with each other for SERP visibility
    - Google does understand who the original owner and source of the content is
    - Google uses publish dates, canonicals, links etc to determine originating owners of content
    - If a site is getting higher ranking than the original source of content then a DMCA takedown can be requested
  - Preventing issues from External Duplicates
    - Content duplication is sometimes inevitable
    - There can be large amounts of exact matches for popular topic
    - Sometimes using manufacturers descriptions can be good but sites will need to find ways to increase user value
  - How to Identify Duplicate Content
    - Use Google as well as internal site searching to find duplicate content
    - Again tools like Screaming Frog will allow for checking a site 

Common Tags
  -
  - Title Tags
    - Title tags are a very important part of on-page SEO
    - These are what appear within SERP's
    - Title tags look like `<title>Page Title </title>`
    - Google may change titles to those that better fit the page
      - In 2021 Google started modifying title tags within search
      - The way that Google generates title was updated to take other factors into account
      - Google sometimes does not use title tags as they can be sometimes a bit long
      - People mistakenly stuff keywords into titles assuming that it will improve rankings (it won't)
      - Sometimes title tags are missing or contain boilerplate or repetetive language
    - There are some best practices that Google recommends for titles
      - Each page should have a unique title
        - Duplicate titles may be an indicator of duplicate content
      - Titles should be of a reasonable length
        - This should usually be between 30 and 60 characters long
      - Relevant terms should be reasonably included within titles
      - There should be only 1 meta title associated with a page
        - If there is more than 1 then search engines may struggle to see which one is the main version
  - Meta Descriptions
    - These are not a direct SERP ranking factor but are important to SEO
      - This is because meta descriptions can be displayed within SERP's and can entice users
    - Meta descriptions look like `<meta name="description" content="This is a description"/>`
    - Google can re-write descriptions if they feel it better matches the query
    - There are some best practices that are recommended for using descriptions
      - Each page should have a single, unique and optimised description with it
      - Duplicate descriptions can show duplicate content so watch this
      - Descriptions should have a reasonable length or else face truncation
        - At the same time too short descriptions may not be of great value to users
      - Don't prioritise keyword inclusion as it is not necessarily important within meta descriptions
  - Heading Tags
    - These tags contribute heavily to the structure of a page
      - This means that they can play a part in rankings
    - They usually contain relevant information and terms that define a page contents
    - A h1 tag looks like `<h1>This is a h1 tag</h1>`
      - h2, h3 etc exist and have smaller sizes on a page
    - The best practices that should be used for headings include
      - Use proper relevant structuring h1 for main headings, h2 for section headings and h3 for subheadings
      - Use as many h1 tags as are needed, Google uses headings to understand different parts of a page for context
      - h1 headings and others should be optimised and make sense
      - It should also be ensures that headings are of optimal length
  - Nofollow
    - This was introduced in 2005 as a means to combat comment spam
    - It quickly became a favourite of Google to highlight advertising related or sponsored links
    - It looks like `<a href="http://example.com" rel="nofollow"><a/>`
    - Since this element was introduced many websites are implementing it by default on all external links
    - There have been some new link attributes that are similar to nofollow that have been introduced
      - Rel=sponsored which should be used for sponsored content
      - Rel=ugc used where user generated content can create links
    - The 3 attributes are used as qualifiers for outbound links
      - They help better define the relationship between a site and the sites it links to 
  - Robots Tag
    - This tag is intended to be used for on-page directives
    - These directives are used to control indexing and serving of snippets
    - A no index directive looks like this `<meta name="robots" content="noindex"/>`
    - In addition to the robots tag in the HTML there can also be a X-Robots tag
      - Meta robot tags live within the head of a HTML document
      - X-Robots live within the HTTP header response, this makes it ideal for implementing directives eg noindex on PDF's 
      - This is because the HTML content cannot be accessed
    - There are a number of directives that can be implemented with robot
      - all which means that there is no restrictions on indexing of content
        - This is a default value and has no effect if it is explicitly listed 
      - nofollow which means not to follow the links on this page
        - If not specified Google may try to discover the linked pages 
      - none which is equivalent to having noindex and nofollow
      - noarchive which specifies not to show a cached link in search results
        - If this is not specified then Google may generate a cached version of the page 
      - nosnippet which is explictly telling Google not to show a text snippet or video preview in the SERP's
        - A tumbnail image may still be available if it gives better user experience 
      - noimageindex which tells Google not to index images found on that page
      - unavailable_after[date/time] which directs Google no to show this page in the SERP's after the specified time
        - This is ignored if there is no valid date or time listed 
        - There is no specific expiration date for content by default but using this directive can be useful for things like news articles etc
      - max-snippet:[number] which is where a maximum number of characters can be utilised
      - max-image-preview:[setting] which sets the maximum image size of an image preview of the site in the SERP's
        - If not specified Google will use a default size 
      - notranslate tells Google not to offer a translation of the page within search
  - Viewport Tag
    - This tells the browser how to control the page's dimensions and scaling
    - This tag like others appears within the HTML head
    - It looks like this `<meta name="viewport" content="width=device-width, initial-scale=1">`
    - This tag is key to ensuring that users have a good quality experience when visiting a page
      - This can be especially true for mobile devices as zooming and panning is not good for a good user experience
      - This poor usability on mobile devices is definitely going to affect SERP rankings 
  - OG Tags
    - OG is an acronym standing for Open Graph
    - These are snippets which help control what URL's look like when shared on social media
    - The OG title looks like `<meta property="og:title" content="OG Title"/>`
      - The OG description looks like `<meta property="og:description" content="OG Description"/>`
      - OG Image looks like `<meta property="og:image" content="https://example.com/OGImg.png"/>`
    - These are not a ranking factor for Google but they can help social media platforms understand page content better
      - They maybe able to display the page better within their own feeds 
    - There are some best practices which should be followed
      - Use a great image to capture users attention
      - Ensure that the title is of good quality and length
      - Ensuring that descriptions are accurate
      - og:url should have the canonical URL defined
      - og:locale can be used to define locale, if not defined then the default is EN-US
  - Rel=Canonical
    - This is used to define the master version of a page
    - This is what it looks like in HTML `<link rel="canonical" href="https://example.com/master"/>`
    - This can be implemented in both the HTML head section as well as HTTP headers
    - Consistent signals should be used to avoid the canonical from being ignored or overridden
      - This is because it is more of a suggestion to Google which it may ignore   
  - Rel=Alternate
    - This is to specify alternate versions of a webpage
    - It looks like this in HTML when defining a different language
      - `<link rel="alternate" hreflang="en_GB" href="https://example.co.uk"/>` 
    - It looks like this when defining a mobile version of a site
      - `<link rel="alternate" media="only screen and (max-width: 640px) href="https://example.com"/>` 
      - The m. version should be used in conjuction with a canonical tag to point to the master version of a page
    - Alternate is not needed for responsive websites, it is only needed where there are different URL's

Split Testing and SEO
  -
  - Split Testing is also known as A/B testing or Multivariate testing
    - It is the process of testing multiple variables for multiple users 
  - One example is where a user User A gets the original version of a website
    - User B would then get a modified version of the same site 
  - Performing split tests is great when trying to understand user engagement and trying to improvement
    - The page which gets the user to click more and stay longer will be the one carried through after the test    
  - These tests can be very straightforward for users but it is not as easy for search engines
    - Google does provide resources and information on how to get it right
    - https://developers.google.com/search/docs/crawling-indexing/website-testing
    - https://marketingplatform.google.com/about/optimize/
    - The documentation covers 2 types of split testing
      - A/B testing which is where 2 or more variations of a change are tested eg button fonts
      - Multivariate Testing which is where more than one change is tested at a time, impact of changes and their relationship is tested
    - Split tests would usually be implemented using different URL's
      - It can also be done by using JavaScript to dynamically inject content 
    - When users try to access the original URL they are redirected to a variation
      - User behaviour is then compared to find the best 
    - JavaScript can also be used to dynamically insert page variations
    - There are some things that must be watched out for and considered when doing split testing
      - Depending on the testing being done, it may not matter if google crawls and indexes some of the content 
      - Small changes eg fonts can have surpising effects on user interaction with the page
      - These changes will not have a huge effect on search engine understand of a page
      - When using multiple URL's, Google should not be allowed index both versions of a page
    - What to watch out for
      - Do not cloak test pages
        - https://en.wikipedia.org/wiki/Cloaking   
        - This is where one set of pages is shown to users and another to Google
        - This is a practice which goes against all guidelines and can get a site demoted in the SERP's
        - It is possible that sites using cloaking maybe removed from search results altogether
        - Cloaking counts whether it is done by serverlogic, robots.txt or JavaScript
        - https://www.seozoom.co.uk/a-b-test-what-it-is-and-how-to-use-it-to-improve-the-site/
        - Instead use links or redirects to send users to different page versions
      - If running a test with multiple URL's then use Rel=Canonical links on altered versions of the original URL
      - If running tests that redirect users from one place to another then use 302 (temporary) redirects
      - Run tests for only as long as is needed and then remove all artifacts (pages etc)

Edge SEO
  -
  - Edge SEO is the process of implementing SEO recommendations through utilising service workers on a CDN 'Edge' server
    - https://www.searchpilot.com/resources/blog/edge-seo/
    - https://www.cloudflare.com/learning/cdn/what-is-a-cdn/
    - https://www.sara-taher.com/service-workers-seo/ 
  - Cloudflare offers a technology called Cloudflare Workers which allows for executing JS on an edge server
  - SEO's are familiar with amking recommendations that do not get implemented
    - This can be down to platform constraints or other bottlenecks 
  - The Service Worker sits between the client and the origin server
    - This allows for pages to be modified without affecting the original in place on the main server 
  - Using Edge SEO means that anything can be changed on a website after it has left the server and before the user sees it
  - There are a large range of circumstances where this ability is very useful
    - Adding schema to pages where it is difficult or impossible to do it via CMS
    - Managing redirects where access to htaccess file is not easily
    - Implementation of hreflang on a wide set of pages
    - Implementing custom HTTP headers
    - Collection of log files
    - Editing robots.txt file where the platform will not let you do this, one example is Shopify
      - Edits can be made prior to hitting users to prevent access to certain parts of a site 
      - Also it may be edited to give access to things that Shopify by default will not allow
    - A wide variety of content can also be injected
  - There are some things that should be considered if thinking about implementing Edge SEO
    - Cost, using Cloudflare to implement their service workers means paying for the service
    - Platform, other CDN providers may not offer the same service that Cloudflare does
    - Bugs and conflicts can be introduced between server and user content
    - There may be issues with adapting to new processes needed when implementing Edge SEO


<p align=center> Internal Linking <br />
  
Information Architecture
  -
  - Internal linking helps users and crawlers discover pages on a website
  - Internal linking is crucial from an SEO viewpoint to establish a good site architecture
  - Information Architecture is the design and organisation of content, pages and data into  a structure
    - This structure will aid users and crawlers understanding of that system
  - The most relevant content should be within as few clicks as possible
    - Typically Google will not click beyond 5 clicks
    - The deeper pages are on a site the less important their pages are seen
    - These pages are also less likely to be indexed
  - Important of Information Architecture (IA)
    - Indexing uses IA to improve crawlability and navigation
    - IA will have an effect on SERP's, the better the IA the higher rankings are likely to be
    - IA helps people to find required information and improve conversion processes and rates
  - Good Information Architecture
    - Navigation on a site should be simple for users and search engines
    - Main navigation links to sub-categories which are the linked to pages
    - Having good navigation and linking will improve discoverability and visibility of content
    - Good IA is needed to limit click depth on a site
    - The most important pages add to sitemap.xml should take less clicks to get to
  - Information Architecture Components
    - Main Category is usually a broad keyword which provides users and SERP's broad understanding of a topic
      - Category pages are great for organising content
    - Sub-categories are sub-topics that have more specific keywords
      - They will explain page content more than more broad keywords
    - URL's should reflect a site's structure
      - This helps a search engine to understand that structure
      - The URL structure should be Category/subcategory/topic
    - Providing users with proper links to important content will reduce bounce rate
      - https://blog.hubspot.com/marketing/what-is-bounce-rate-fix
      - https://cxl.com/guides/bounce-rate/seo-impact/
    - Search engines tend to reward sites with Sitelinks
      - Sitelinks are the most visited pages of a site
      - They appear alongside the target page when a user performs a site search
      - Having pages appear for sitelinks indicates to the search engine that a site has good IA
      - This also helps to increase brand awareness and trust
    - Redirects
      - Even in the best sites there will come a time when content needs to be moved which is where redirects come in
      - The proper redirect codes should be used to indicate whether moves are permanent or temporary
  - Keywords are key
    - In order for a user to make sense of website content, the site owner must first understand user needs
    - Content should be structured to these needs as well as any goals, expectations and behaviours
    - Keyword research is key in this process
    - The most competitive keywords are typically short-tail keywords for broad traffic
    - Getting the best matching keywords both short and long tail will help with the IA foundation
    - Keyword research may also provide ideas for content that will further benefit site users
  - Research Information Architecture
    - It is important to do some research prior to implementing a site's IA
    - Competitor research can help with implementing IA
      - This does not automatically mean what works for someone will work for others
    - The type of IA to be implemented is completely optional but should be done with a target market in mind
    - A\B testing should used when implementing a site IA
      - Analysis of the 2 versions of IA will reveal which has engaged users the most
  - Information Architecture Tools
    - There are some tools which are available to help determine a good IA
    - Screaming Frog is an excellent site auditing tool
      - This can be used to identify what some of the site pillars are
      - https://terakeet.com/blog/pillar-page/
    - SEMRush is a great tool to assess the search volume of a site
      - https://www.semrush.com/
      - This is great for keyword research and keywords with the highest volume could be used to create content pillars
    - Google Search Console is also a great tool for understanding target audience behaviour
  
Vertical & Horizontal Linking
  -
  - What is Vertical Linking
    - This is providing users and search engines with an easy path to follow when navigating a site
    - This is done by linking to important pages vertically
    - Vertical linking where there is a link from a category to a subcategory and then to a topic page
  - Importance of Vertical Linking
    - It is important to help users navigate the site and keep track of what page that they are on
    - This type of linking helps search engines understand site structure as well as easily accessing important pages
    - Vertical linking will help to reduce a site's bounce rate due to easier finding of wanted content
  - Ways of Vertical Linking
    - Keywords can be implemented in primary navigation which helps both users and search engines
      - Primary navigation offers vertical linking to important pages
      - By linking from categories to subcategories to pages users find what they are looking for with minimal clicks and searching
      - Primary Navigation links to the most important pages and appears at the top of the homepages as well as others
      - Primary navigation is important for a couple of reasons
        - Firstly it helps search engine understand the hierarchy of the site
        - User's journeys are improved by using Primary Navigation becuase important pages can be accessed at the top of the page
        - Ranking can be improved using primary navigation as content is easier to locate on a site
    - Breadcrumbs which help a user to know which page that they are on
      - Using breadcrumbs may help keep users on a site for longer
      - This is a very common type of vertical linking
      - Breadcrumbs will act as a secondary navigation bar
      - Even though they appear horizontally on a site they are vertical links
      - Why are Breadcrumbs useful
        - They offer improvements to user experience due to being easier to use for site navigation
        - Breadcrumbs can also help search engines to categorise content by using keywords which will improve site ranking
        - Users have been seen to spend longer on a site that uses breadcrumbs
        - Implementation of breadcrumbs is fairly straightforward but there are some considerations to be borne in mind
        - Guideline For Breadcrumbs (Vertical Linking)
          - Breadcrumbs should be easy to see without intruding on the user's experience
          - Breadcrumbs should be in addition to navigation not instead of
          - Clear anchor text should be used for each breadcrumb
          - Have a link for each level except the current page
          - Add a BreadcrumbList type schema markup to further improve search engine usnderstanding of breadcrumbs
            - https://schema.org/BreadcrumbList
            - https://developers.google.com/search/docs/appearance/structured-data/breadcrumb
          - Google prefers using JSON-LD for schema markup
    - Faceted Navigation which allows linking to more specific pages
      - This allows users to find what they want in a more specific and targeted way
      - This would not be possible using primary navigation
      - Faceted Navigation can be found on the category pages of a site
      - It is commonly used on E-Commerce sites
      - Multiple filters based on listing attributes can be used to find desired products
  - What is Horizontal Linking
    - This is a way of allowing users to find information related to the current page
    - There are some reasons that this method of linking is important
      - It allows site owners to link from a product category to another category of similar products
      - Links related to content such as articles similar to the article being read by the user can increase user engagement
      - The same applies to site such as jobs sites where users can be shown related jobs being advertised
    - Content Tagging
      - Content Tagging is 1 or more keywords added to a page
      - This is then added to a piece of information which describes and makes it easy to locate
      - Content Tagging in important for some reasons
        - Search engine visibility is improved by using content tagging
        - Improves both UX and conversion rate as users can access desired content in 1 click
        - Site owners can use content tagging to link to fresh content such as trending news 
        - Used properly content tagging can offer valuable insights about a site's performance
  
Faceted Navigation
  -
  - Faceted Navigation is normally an extension of a site's categories
    - This is usually presented in a sidebar or sub-menu on e-commerce sites
  - It contains multiple categories, filters and facets
  - Site owners can display many listing of the site's products
  - It allows users to refine their search by filtering the category pages
  - Issues with Faceted Navigation
    - Faceted Navigation may cause issues with Duplicate Content
      - This is because of the multiple navigational paths available when filtering for products
      - Every filtered search creates a new URL which results in multiple page variations
    - Wasted Crawl Budget
      - Due to many duplicate pages and filter combinations the search engines may spend too much time crawling these
      - Less time will then be available to crawl more important pages
    - Diluted Link Equity
      - Link Equity is also referred to as Link Juice
      - This is a search engine ranking factor where value and authority is passed from one page to another
      - Having duplicate content because of Faceted Navigation can result in link equity being split between pages versions
  - Best Practices using Faceted Navigation
    - These best practices will stop unimportant pages that were created using Faceted Navigation from being indexed
    - Use noindex directives so search engines know what not to index
    - Use the disallow protocol in Robots.txt to keep Google away from unimportant pages
      - https://searchfacts.com/robots-txt-allow-disallow-all/
      - There is still a possibility that Google will still index these pages even with the disallow in place
    - Use canonicalisation to allow Google to understand which page is the master version of a page
      - Even with canonicalisation, Google will still crawl pages but more link equity goes to the master version
    - Breadcrumbs should be added to each page in addition to Faceted Navigation
  

<p align=center>Crawlability<br />
  
Robots
  -
  - What is a Robots.txt file?
    - Search engines crawl websites to discover content
    - There are however some things such as images etc that you will not want a search engine to crawl
    - A Robots.txt file contains directives on how a site should be crawled
    - It uses allow and disallow directives to tell a crawler which areas of a site are allowed and which are not
    - It is a .txt file which is uploaded to the root directory of a website
  - Why do you need a Robots.txt file
    - Preventing search engines crawling certain parts of a site
    - Preventing search engines from indexing certain parts of a site
    - It is also best preactice to add a link to the sitemap to the file
    - It helps sites manage their crawl budget
  - How to add a Robots.txt file
    - You can use any text editor to create the file
    - It must be named in lowercase so robots.txt and added to the root of a site
    - There should be a separate file for each website eg subdomains will have their own Robots.txt
    - The Robots.txt file can be audited within GSC to see if there are any errors with the sitemap
    - No URL in the sitemap should be blocked by the Robots.txt file
    - One common error is to leave directives in place when migrating from a staging area to a production environment
  - Robots.txt file directives
    - The file starts with the user agent which is the name given to the spider that's crawling
      - An asterisk can be used to apply to all crawlers or specify one eg Googlebot
    - Following this is either allow or disallow
      - This will depend on the instructions that are being given
    - Certain folders or sections of the website can then be specified
    - The noindex tag stopped being supported by Google in 2019
      - Do not include this directive, one option is to use a 404 or 410 another is using a Meta tag
    - Crawl-delay is an unofficial directive
      - It is intended to slow down the rate at which a site is crawled
      - It is not supported by many search engines including Google
  - Robots Meta tag
    - This is another way to prevent sections of the site from being crawled
    - This is a page specific piece of code which instructs crawlers how to crawl a site
    - Robots.txt is a general guide whereas a meta robots tag can stop certain pages from appearing in SERP's
    - They are either part of the HTML page or sent by the server
    - There are several common directives used in a Robots Meta Tag
      - Follow which follows the links on a page
      - Nofollow which stops page liks from being followed
      - Index which allows the page to be seen in search results
      - Noindex which tries to stop a page being seen in the SERP's
      - These directives are not case sensitive but must be comma separated
  - X-Robots tag
    - If resources such as PDF, Video or image files then it is best to use an X-Robots tag
    - This adds a greater level of flexibility compared to the Meta tag
      - This is because specific file types can be blocked
    - These directives can be setup via the server using either a .htaccess file or a header.php file
      - This means that they are served globally across the site rather than being page specific

Crawl Depth
  -
  - What is Crawl Depth
    - Crawl depth focuses on a site's architecture and hw close pages are to the homepage
    - A website homepage with have a crawl depth of 0 (zero)
    - Pages linking directly from a homepage will have a crawl depth of 1
    - Crawl Depth is how many steps\clicks to reach a page from the homepage
    - Ideally there should be nothing with a crawl depth more than 3
    - The Screaming Frog tool can identify anything with a crawl depth greater than 5
    - These should be focused on to improve internal linking and reduce crawl depth
  - How does Crawl Depth affect SEO performance
    - The deeper a page the less likely it is to be crawled
    - Page depth affects page importance
    - Deeper pages are crawled less frequently so site owners must try to ensure that some pages are not forgotten
    - Pages nearer the homepage are given more weight
      - This is because the homepage is linked to the most and it passes that benefit to other pages
  - How to improve crawl depth
    - Improve internal linking between and to certain pages
    - On sites with lots of products or blog posts use horizontal linking
    - Implement breadcrumbs on the site to improve usability as well as navigation
    - Link important pages from the homepage
  -

URL Structure
  -
  - What is a URL
    - URL is an acronym for Unified Resource Locator and is an address of a web page or resource 
    - It is presented in a human friendly form rather than an IP address
    - A URL is made up of a host, a domain and a path
      - The host is the protocol usually HTTPS
      - The domain is something like www.example.com
      - The path is then appended to the end of the domain www.example.com/example/example
    - URL's also form part of the site architecture and determines part of the structure
    - URL structure can improve user understanding on how to navigate a site
    - Presenting a URL in a user friendly way will influence whether a user will click on a URL or not
  - Why a good URL structure is important
    - URL's that are logical and relevant to the page will help both users and search engines understand what content is about
    - A good URL structure will show in SERP's as it can have an impact on ranking
      - A user is much more likely to click on a URL that matches their search query than one that is unfriendly
    - URL structures affect ranking as URL's with lots of unfamiliar word are parameters will not get as many clicks
      - Use keywords in URL's but do not over do it
  - URL Structure best practices
    - Keeps URL's simple, short and logical
      - Try to allow users to see the whole URL in the SERP's
      - URL's should follow the hierarchy of the site eg use sub-directories to group content
    - Use hyphens not underscores to separate multiple words
    - The URL should use words that are relevant to that page
    - Always use lowercase letters in URL's
    - Always have consistency in URL structure across the site
  - URL Parameters
    - URL parameters usually follow a ?, & or =
      - They are made up of a key value pair eg ?color=blue
    - Parameters are often used on e-commerce sites to filter through products or search functions
    - Parameters can cause issues with Duplicate Content and Keyword Cannibalisation
      - https://ahrefs.com/blog/keyword-cannibalization/
      - Parameters do not change a URL, they add options that point to the same page but with different results
      - Keyword Cannibalization occurs when multiple pages target the same keywords
      - Cannibalization occurs because there are large amounts of parameterised URL's with the same content
    - Crawl budget being wasted is another consequence of having large amounts of parameterised URL's
      - This is especially true with using Faceted Navigation and different parameterised URL's for each combination
  - Avoiding Parameter Issues
    - Parameter issues across the site can be fixed by ensuring that the parameters do not contain URL's
      - One example is ?color=blue&color=
    - Avoid duplicate content and crawl budget issues by ensuring that all URL's follow an order across the site
    - Robots.txt can be used to prevent crawling of parameterised URL's which should free up some crawl budget
      - Ensure that only the parameterised URL's that are not to be crawled are blocked
    - Canonicalise parameter URL's
      - Users can filter their way through parameterized pages while the search engines get the proper pages
      - This is only good if all the pages to be canonicalised are considered similar enough
    - Use Static URL's
      - This approach works very well on smaller sites but may not be the best for larger ones
      - Each filter has its own static page, therefore unique content is needed for each page
      - This is a good approach when looking for all pages to be indexed
      - This is not easy to implement and does not solve issues with crawl budget and duplication

Sitemaps
  -
  - What is a sitemap
    - This is a file which gives information about URL's on a site, it is in HTML or XML format
    - An XML file is used by crawlers to understand what content is on a site
      - This makes a site easier to index
    - There is various information included in a sitemap
      - URL's of pages
      - When a page ws last updated
      - How frequently a page is likely to change
  - Sitemap Best Practices
    - A sitemap should contain no more than 50000 URL's
      - Any sitemap that is larger than this should be split into multiple sitemaps under 1 sitemap index
      - One example is to split sitemaps by purpose eg  Products, Services, News, Blog etc and is ussually the method used by larger websites
    - Sitemaps should only contain absolute URL's
      - Make sure that the correct protocol etc is stated within the URL ie HTTP or HTTPS
  - What to include in a sitemap
    - Only 200 OK status codes shuld be in a sitemap
      - If there are any 3xx, 4xx or 5xx error codes then they should be removed
    - There are some futher things that should not be included in a sitemap to prevent wasting crawl budget
      - Search Results
      - Paginated pages
      - Pages that are not indexed
      - Pages that are not canonical
      - Pages that come from filtering
    - The sitemap should be as clean as possible to ensure that the search engines get clear and consistent signals
  - How to create a sitemap
    - Again the Screaming Frog software can be used or a plugin if using a CMS
    - A sitemap can also be created manually if the site is small
    - There are also online tools available for generating sitemaps
      - https://www.xml-sitemaps.com/
    - Once the sitemap has been created it should be submitted to Google using the GSC
    - A link to the sitemap should also be in the Robots.txt file
  - News Sitemaps
    - A news sitemap is a separate sitemap especially for news articles
    - It will help Google news find a site's news articles quicker
    - It should only contain URLs from the previous 2 days
  
Pagination
  - What is Pagination
    - Pagination is the breaking up of content over multiple pages to make it easier to navigate
      - This is more common on e-commerce, news and blog sites
    - Google used to use the `rel=prev` and `rel=next` tags but this is no longer the case
  - How does Pagination affect SEO
    - Paginated pages are treated the same as any other page according to Google
    - Pagination is an important factor in helping crawlers find links in a site
    - If internal linking and crawl depth are an issue then many paginated pages will not be indexed
  - Common mistakes
    - Websites should have anchor links to paginated pages
      - They should link between each other using the href attribute
      - This is especially true of infinite scroll websites
        - https://www.seoclarity.net/blog/infinite-scroll-seo
      - If there are no anchor links then View More buttons should be used
    - It is not recommended to canonicalise pages to the first set in the series
      - Each page is a standalone page and should have a self-referencing canonical
    - It is not usually the practice to add paginated pages to the sitemap
      - This is not true of infinite scroll websites which may benefit from a sitemap
  - Pagination Best Practices
    - As previously stated each page should have a self-referencing canonical
    - Paginated pages should have a crawl depth that is as close to the home page as possible
    - Paginated pages should also contain unique metadata
    - Ensure that paginated pages are not being blocked from being indexed
    - Paginated pages should have anchor href links to other paginated pages
      - For example page 2 in a series should have an anchor link to page 1 and page 3
      - This is especially important for infinite scroll websites

User Agents and Proxies
  -
  - What is a User Agent
    - A user agent is sent with the HTTP header and gives details on what system is requesting information
    - There are many different user agents available
      - Googlebot for instance is Googles user agent
    - User agent will also serve the right website to users eg mobile users will get the mobile version of a site
      - It can also provide support for older browsers if a user is detected using an older browser
  - Crawlers and User Agents
    - Search engines also have user agents
    - Robots.txt can be used to prevent user agents from crawling certain parts of a website
    - It is important that the same content is shown to users and search engines
  - Proxies
    - SEO proxies are intermediate servers which provide anonymity when doing large intensive scraping
    - Keyword research can involve a lot of scraping 
    - Using a proxy can prevent a site from being blacklisted by Google
    - It works by identifying you as one of a regular series of visitors rather than a single IP address
    - A proxy can also help prevent geolocation issues such as country only content
  

<p align=center>Images<br />
  
Alt Text
  - What is Alt Text
    - Alt Text is Alternative Text is used in HTML to describe what an image is
    - Its key usage is to summarise the purpose of an image for people with visual imparements
      - https://accessibility.huit.harvard.edu/describe-content-images
    - Alt Text is used by screen reading software to describe what an image is to those that use them
    - Google also utilises alt text to understand what an image is about
  - Why is Alt Text Important
    - Alt Text is required for accessibility
      - Screen readers will read out text to users
      - Alt Text is also useful for people with low bandwith broadband
      - It can also be used to used to describe images that fail to load
    - Alt Text is used for image ranking with Search
      - Google has various algorithms that are focused on image understanding
      - Google uses alt text to help rank images within image search
  - How can you see what Alt Text is
    - There are a number of ways to see the alt text that is in place on a site
    - Again Screaming Frog can be used to crawl a site, Sitebulb is another tool to use
    - If using a CMS than a plugin can be used
  - What does bad Alt Text look like
    - It is important to implement good quality Alt Text
    - One example is an image of a cow, bad Alt Text would be 'Cow', 'Brown Cow', 'Cattle' 
  - What does good Alt Text look like
    - Good alt text is text that accurately describes what an image is about
    - Alt Text should be concise as well as descriptive, bad alt text can be too descriptive
  - Best Practices for Alt Text
    - Alt Text should be as accurate and concise as possible
    - A common limit is imposed by a number of tools, this is 100 characters
    - Too long Alt Text increases the time it takes for screen readers to describe an image and why they are there
    - All images should have an alt attribute associated
    - While all images should have an alt attribute but not all images need text in this attribute
    - Empty alt attributes signify that an image is purely descriptive or used for stylistic purposes
    - Images with an empty alt attribute are ignored by by screen readers
  
Image Sizes and Basic Optimisation
  - What is image optimisation
    - Image optimisation is a common practice that is important for SEO
    - Large images can cause performance problems
    - Optimisation of images is generally done by using compression to reduce the file size
    - There are a number of other tactics available to help with image optimisation
  - Optimal File Size
    - There is not really a definitive size as circumstances wil dictate
    - Some crawling tools highlight images that are over 100kb
    - A site that relies heavily on images may not be able to reach that
    - 100kb may not be reahable for large hero or featured images
  - How to find images that need optimisation
    - Again tools like Screaming Frog can detect image needing optimisation
    - Site should be crawled making sure that images within configuration
  - Raster & Vector Graphic
    - 
  -
  
Further Image Optimisation
  - 
  -
