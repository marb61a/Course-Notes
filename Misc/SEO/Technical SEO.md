<b><p align=center>                    
  Blue Array Academy Technical SEO </br>
  Course Notes  
https://www.bluearrayacademy.com/


<p align=center>An Introduction to Technical SEO <br/>

What is Technical SEO?
  -
  - SEO has 3 main pillars - Content, Links and Technical
  - SEO is about improving a site visibility for search engines
  - Most are oriented towards Google search as it is the market leader
  - Content is ranked by algorithms when search is used
  - Prior to content ranking it is important that search engines can find the website
  - Technical SEO comes into play at this point
  - Technical SEO ensures that search engine bots can access and understand a website as quickly as easy as possible
  - There are several topics in Technical SEO 
    - Information Architecture
    - Crawling, Indexing, Rendering and Ranking
    - Various tools
    - Page Experience
    - Schema, On-Page
    - Security
    - Images
    - Accessibility

Morals & Ethics
  -
  - There are some things to consider when investigating a site from a technical perspective
  - Provide Solutions over Problems
    - When conducting a site audit it is very easy to provide problems
    - These problems observed in isolation rarely give any context to cause or solutions
    - Problems should be quoted specifically, the scale and the solutions to the problem
    - Root causes are essential to issues being fully understood
  - Work with team members
    - Analysis of sites and discovery of solutions and problems needs to be communicated to all parties effectively
    - Providing problems and solutions helps with this and can be provided in different ways to different audiences
    - Team members may not know technical jargon or reasoning behind decisions
  - Crawling Ethics
    - Ensure that permission is granted before crawling a site, check before hand
    - There will be times when anti-crawling software will be in place
  - General Ethics
    - Do not break the law
    - Always follow best practice
    - Do the best thing for a client
    - Do not provide clients with misleading information
  - Environmental Considerations
    - Try do the right thing for the environment
    - Send on the data that needs to be sent and reduce C02 emissions
    - Have webpages as light as possible

The Basics - Hosting
  -
  - There are 2 types of host that need to be known
    - Web Hosts which is where content is stored
    - Domain Hosts which is where information on domain names is stored
  - Web Hosting
    - In order for content to be published on the internet there needs to be a place to publish it, in this case a website
    - This site has to be hosted somewhere
    - When a user requests a web address that is on your site, the server that hosts the website talks with the request and if able then fulfills it
    - The web site hosting can either be with a company or hosted yourself.
  - Hosted vs Self-Hosted
    - Hosted sites are typically built with site builders such as Wix, WordPress or Squarespace
    - Self-Hosted sites are usually easier to customise but this needs to be carefully done
    - When self hosting a lot of work that external hosts do needs to be done such as running backups
  - Shared vs Dedicated Hosting
    - Once a hosted solution has been decided on then it is time to decide on either shared or dedicated hosting
    - Shared is much cheaper and is better for those with limited technical skills as there will not be as many options
    - Dedicated Hosting is good for technical minded people and is like owning your own home
  - What should a good host do
    - Provide excellent technical support when needed
    - There should be the option to have either a dedicated or shared server
    - There should be no problem scaling as demand increases
    - There should be a focus on security
    - Additional functionality should be available such as the ability to whitelist IP Addresses
  - Content Delivery Networks (CDN's) 
    - Although not strictly necessary for a website there are advantages to using them
    - They are a group of servers which are distributed around the globe allowing for faster worldwide content delivery
    - They can be used in addition to web hosting as they allow for content caching on their servers
    - They inprove both performance and security and they reduce dependence on a single server thus improving uptime

Networking
  -
  - Protocols
    - There are multiple protocols used to ensure that servers talk to one another and users get their data
       - IP which is used for network routing
       - TCP which ensures data is delivered reliably
       - HTTP which transfers data between devices
       - HTTPS which is an encrypted version of HTTP and is used more
       - TLS/SSL which is the encryption that HTTTPS uses 
  - Header Requests
    - After a web address has been typed into a browser a request is then sent to a server
    - It contains various information that allows the server to tailor it's response
     - Requested URI, Host, The type of data accepted by the browser, a User Agent, Source IP
  - Header Responses
    - Depending on the information that is sent to the server in the header request, the server will then respond with its own headers 
    - After this then if the browser is allowed to view content, it will begin downloading HTML from the server using details from the response

Standards
  -
  - What is W3C
    - This is an organisation which develops standards for the Web
    - Due to the advent of website builders these standards are not as rigouously adhered to as they once were
    - Most website builders are used by people who may not know HTML
    - W3C offers a HTML validator whichallows people to test & check on a per site basis
      - https://validator.w3.org/ 
  - Why should you care?
    - If invalid HTML is used there is no guarantee it will render correctly in all browsers
    - Validation may not be important for the Googlebot as long as the site is rendered and structured data can be extracted 
    - Valid HTML is still important as it can help pages render and load faster
    - Broken HTML within the head section of a page can cause problems for the Google crawler which can cause other elements not to be seen
    - There maybe rendering issues for users if there is invalid HTML
  - What are RFC's
    - Request For Comments are used to develop a network protocol that is seen as standard
    - Almost all netowrk protocols made for the internet are built on RFC's
    - IP, TCP, SMTP, FTP, HTTP etc are built on RFC's
    - There are a lot of Non-SEO focused RFC's being published
  - Robot Exclusion Protocol
    - It was never turned into a standard when created in 1994
    - This has led to many different interpretations down the years
 
  <br/><br/>

<p align=center>A Technical View of how search works <br/>
  
Crawling
  -
  - Search engines return a list of webpages based on a user's search
  - These are known as SERP's or Search Engine Results Pages
  - It is not possible to search the entire internet for every single query in real time
  - An index is a list of words, phrases etc that can be used to return predetermined results
  - Index requires crawling every site prior to searching
  - Google will be the search engine focused on as it is by far the market leader
  - Crawling in it's simplist terms is discovery, it is what the search engine do to find pages
  - The Crawling process is as follows
    - A webpage is discovered and its URL is noted as well as the HTTP response code
    - The webpage DOM (Document Object Model) is then gone through to find and note each link found
    - These will then be crawled as well
    - Sometimes a URL is not stored, this is done by using a HTML directive telling the search engine not to follow a link
    - This is called a nofollow directive
    - Site hierarchies are also stored for indexing purposes
  - Other pieces of software that crawl around the web can be thought of as crawlers, bots, etc but here the search engine crawlers are of interest
  - When crawling search engines do not restrict themselves solely to webpages, other items gathered include
    - Images, Videos, Documents, Book scans, some databases, user generated content such as map locations 
    - This helps monetise searchs
  - Only in an ideal world would a search engine be aware of all sites and updated their change immediately
  - Resources however large are still limited and crawling everything is not feasible
  - Crawl Budget is the amount of effort that a search engine puts into crawling a site
    - Crawl Budget is mostly a concern for big site as only a certain amount of pages will be crawled before moving on
    - This is because search engines want to crawl a large variety of pages on a large variety of websites 
    - Sites may change regularly but may only be recrawled monthly or weekly
    - On large sites it is important that the most important pages are crawled rather than least important pages 
    - There are 2 factors which determin a site's crawl budget
      - Crawl Limit which is how much crawling a site can handle
      - Crawl Demand which is the perceived demand for a site's contents 
    - Crawl Budget is not just for HTML pages but for all document type resource eg JS files
    - It is not solely based on a single site, page load speeds can affect the amount of pages being crawled
    - Search engine capacity can also affect site crawl budget
    - Other factors include content freshness, page popularity and whether the page is optimised for Desktop, Mobile etc
  - Search engines will not follow paid for links so this is not a way to get crawled
  - To appear in SERP's it is essential that a site can be found and crawled
  - Search engines can be supplied with a list of pages to be crawled on a site so that they don't miss any
  - There are a few methods available so that search engines find all the pages that the should see
    - Google Search Console aka GSC
    - Robots using the robots.txt file
    - Sitemaps using the sitemap.xml file
    - Internal links within the website
    - Passwords protected files and directories   
  - Some of the above methods can be generated automatically or developed manually
  - Ensure that there is a logical hierarhy within a site to ensure that crawlers can find pages
  - Robots.txt can hold search engine directives, these can be disregarded and are more suggestion than order
  - Crawlers do not use passwords and usernames so if pages are hidden behind these then they will not be seen
  - There are some steps to ensure that pages are found by search engines
    - First verify that the search engine can find the site
    - Then ensure that the page submitted looks correct
    - Submit a single url if there are changes and recrawling needs to be done ASAP
    - Ensure good navigation so that all pages can be found (Especially on the home page)
    - If there is to be only one page submitted then it is best to submit the homepage 
    - If possible then get links from pages that have already been crawled

Indexing
  -
  - Whereas crawling is akin to discovery then indexing is akin to understanding
  - Search engine must understand what a search query has found and provide reliable and quick results
  - Google uses a reverse index which is a list of words/phrases that are stored in a database as well as pointer to related docs
    - A document is this case is a web page, pdf, image etc
  - Tokenisation is used to reduce searches to their core meaning which reduces the resources need to perform the search
  - If the index was not inverted then a list of documents would need to be searched for the words which would be a lot slower
  - An index is more like a catalogue of everything that a search engine has found
    - URL's which point to various things, Web Pages, Images, Videos etc
    - Enteries are also made for different language versions of documents which are considered as separate documents
  - Indexing helps return more intelligent and more useful results
    - Google's index is called Caffine and it was introduced in 2009
    - Recent changes are referred to as medic as misinformation was being dropped down the page results
    - This had an effect on YMYL pages, (Your Money Your Life)
  - As well as indexing documents there are highly compressed versions stored too
    - This will contain only the textual elements and is the latest version of the page that was seen
    - The process helps to speed up search engine indexing
    - Sometimes the search engine will allow for the cached version to be viewed
  - Where there are several similar versions of a page the search engine selects what it considers to be the canonical version
    - This is the authoritative or master version of a site
  - Search engines may get the canonical version wrong so it should be set in a meta tage in the HTML head section of a page
    - If the page is not the canonical version then tags should point to the canonical version
  - Selecting an incorrect canonical version may lead to the wrong page being listed in the search
    - Similar pages may cause confusion for search engines and using tags removes this.
  - There are a few ways to improve indexing of a page
    - Firstly start off by creating page titles that are short and meaningful (30 - 60 characters)
    - Make use of page headings that show the subject of the page
    - Use text that is both useful and meaningful to visitors
  - Images and videos should be annotated with alt text etc as needed
  - There are some methods available when you want to stop a page being indexed
    - There are multiple reasons to avoid indexing a page mostly to avoidwasting crawl budget
    - It should be said that even with methods available to stop indexing the best way to avoid data on the internet is to not put it there
    - HTTP header Meta tags such as content=noindex
    - HTTP response header returns X-Robots-Tag: noindex
    - Robots file which should be in the site root directory (robots.txt)
    - Server configuration using the .htaccess file
    - Some CMS software has plugins to help block indexing
  - It should be remembered tha non indexing are requests and may be ignored especially by malicious crawlers

Rendering
  -
  - Rendering is performed during indexing, it is the process of retrieving a page, executing the code within and assessing results
    - Pre-render and post render is examined to identify the structure and layout of a site
  - By taking the site HTML, CSS, JS and other needed files the search engine can build a DOM
    - The Document Object Model (DOM) takes all the data and markup the browser needs to render a page
  - The search engine creates its own DOM which means it is effectively rendering the page similar to the browser
    - Information derived from the rendering is used for search engine ranking
  - The main reason that search engines render is that many pages replace content using JS eg user interaction
    - Another reason is that the content is time dependent eg news and would need to be replaced to avoid staleness
  - It is important that JS is working properly in browsers from an SEO perspective
    - Website that disable JS should still show content properly
  - At some point webpage were relatively easy to understand programmatically
    - This has changed due to the need for more advanced functionality on webpages
  - The search engines have had to adapt to as they need to see both pre-renders and versions of the post render page
  - Google introduced its AJAX crawler to make those type of pages accessible to the googlebot
    - This necessitated developers to add special hash tags to url to indicate AJAX was being used
  - Webpages have more JavaScript than ever and the AJAX scheme has been phased out
    - In its place Google will render the JavaScript itself
    - It does this by taking a url which is in the crawl queue
    - A request is then made for the URL and all files are retrieved
    - These are then put into the render queue for processing
    - A headless version of the Chrome browser is then used by the render which renders by executing the internal JS
      - Headless refers to the fact that there is no browser window available to view this 
    - Once rendering is complete any links found are stored for future processing by putting them into the crawl queue
    - The page is then examined so that it can be added to the index
  - There can be issues with the above approach
    - What happens if the JS executes correctly in the browser environment but not in Googles headless environment
    - It is vitally important the this not happen as if a search engine cannot render a site, it cannot see it and thus cannot index it
  - There are some factors that can cause a search engine to stop rendering content
    - Important to remember that if there is no pre-render then there is no rendering at all
    - URL's that have been submitted have tags stopping indexing
    - There are blocked URL's in the robots.txt file
    - There was no page returned by a page (5** HTTP Codes)
    - There is a redirect chain happening
      - If there are more than 5 redirects then Google will stop looking
    - What is referred to as a soft 404 coded where a 200 OK status is returned but appears to be a 404 Page Not Found code
  - There are also some reasons that content will be rendered incorrectly
    - There is blocked Javascript, cookies or style files which can cause the below factors
      - Site text is too small to read
      - Clickable elements have been placed too close together
      - Website content is larger than the screen viewing
      - There are images with text that cannot be read
        - There should be alt tags available for all images
      - There are animations that convey meaning

Ranking
  -
  - A search engine will return a ranked list of results that best fits a search query
  - Because different search engines return different results a place in the rankings can change
    - When search engines change what they are looking for (Algorithm updates) this can also affect ranking results
  - In 2006 there were 200 signals that effect rankings, there are likely to be many more today
    - There were 8 main categories such as site, domain, page, brand etc and each have their own factors
    - In page for example there are factors such as keywords in the title, page loading speed, content length etc
  - There are too many categories to try and rank for each and everyone of them
    - It is best to adhere to good practice guidelines and follow standards which will take a site a large part of the way
  - Search engines try to determine intent
    - For example searching for a thai restaurant is likely to mean restaurants need your location rather than restauants in Thailand
  - There are several things that can be done to improve ranking
    - Follow the search engine webmaster guidelines
      - This is advice given by the search engines themselves so should be followed
    - Ensure that the site is quick to load
    - Sites should also be responsive namely mobile device friendly
    - Content should be useful, relevant, fresh and up to data as well as being authoritative
  - The above factors when taken together are known as Technical SEO
  - Google uses a link analysis algorithm known as PageRank to rank pages
  - PageRank is also known as Link Juice because of the way the rank flows from link to link
    - It also passes sometimes to pages it links to which is known as RankFlow
  - The higher the PageRank score, the higher it is likely to appears in SERP's
    - Other tools use similar metrics to Googles page rank
      - AHRefs uses URL rating
      - DeepCrawl uses DeepRank
      - Moz uses Page Authority
    - Links that have a nofollow tag will not count towards ranking
      - Backlinks are links to our site from other sites and to our pages from our other pages
      - These are important as pages gain importance the more that they are cited elsewhere
      - Links that are paid for or advert links may be penalised
  - Intent in the context of search engines is the intention of the searcher
    - Satisfying what the searcher is actually looking for is the primary goal of search engines
    - Content should be aimed at the searcher's intent too
  - Search intent can be classified into 4 types
    - Informational intent is one of the most common as the intent is to simply look for information
    - Commercial intent which is where a searcher is looking for information about something they are considering purchasing
    - Transactional intent which is closely related to commercial as a searcher is looking for a place to buy something they have decided to purchase
    - Navigational intent which is where a searcher is trying to find a specific website
    - The first 3 are related as they follow a searchers journey from searching for something to but to buying it
    - To help with intent, in June 2021 Google announced it's new development, the Multitask Uniformed Model
    - This approach uses artificial intelligence to gain insights into searcher intentions
    - The idea is to find what a searcher actually wants without having to make multiple searches 
    - One example is looking a landmarks in a foreign country, google aims to provide best ways to get them, recommended accomodation etc
    - This provides the opportunity to gain results from other languages as that will not be problem with Google translating
    - These sites are likely to be more authoritative if the searcher destination is located in their country
  - There are a number of ways of displaying content within search results and not just the standard list of results
    - Some other features that Google may choose to use are Featured snippet, Image Pack, Local Pack, Knowledge Panel, Featured Snippet
    - To appear in these formats it may be necessary to use structured data or schema in a page
    - There are multiple different types of schema that can be used, details are available at
      - https://schema.org/
    - The Google search gallery also lists the different types of schema that are currently used
    - There are different schema available for blogs, books, podcasts etc and all can increase website traffic <br/><br/>
 
<p align=center>Tools <br/>

Google Tools 
  -
  - Users should be logged in to their Google account in order to use these tools
  - Google Search Console (GSC)
    - https://search.google.com/search-console/about
    - This tool is used for several different things
      - Reviewing and monitoring site performance within organic and Discover
      - Review indexation of URL's
      - Review the reporting of warnings & errors regarding indexation within the index coverage report
      - It can be used to monitor sitemaps and see URL indexation within them
      - Errors, warnings and valid results from Rich Results can be seen
      - Examine crawl stats from a high level
      - Information on core web vitals as well as page experience can be viewed
      - How mobile compatible pages are can be checked
      - Manual actions and security issues as well as inspecting URL's
  - The index coverage report maybe the most important report when it comes to Technical SEO
    - Testing the live url within an error in the report allows for understanding the current status of a URL
    - Recrawling a page may fix errors due to temporary noindexing
  - Google Page Speed Insights
    - It is used to review not only page speed but core web vitals
    - It can highlight possible opportunities as well as provide detailed diagnostics when looking at performance
    - Can be useful in providing an overall performance audit
  - Google Lighthouse
    - This is what powers page speed insights
    - It is used to provide automation of core web vitals reviews
    - It can provide sitewide analysis for opportunities and diagnostics
  - Mobile Friendly Test
    - It tests the mobile friendliness of a site 
    - It will provide a fairly detailed report
    - There can also be testing for any mobile realted rendering issues
  - Rich Results Test
    - This is used for structured data testing
    - There seems to be better rendering than other tools
    - This tests for rich results inclusion and schema issues that result in rich results being implemented
    - The schema.org validator needs to be used when testing all schemas
    - The rich results test only tests schema types that result in rich results
  - Google Analytics
    - This is a very powerful tool and can be used on for report creation on live and past data
    - It can uncover insights into customer behaviour
    - It can report on performance, understand audience demographics, trend extraction

Bing Tools
  -
  - Using Google Search Console is the only way that verified data from Google searches can be gotten
  - There are benefits however to using Bing tool suite
    - Sanity checking of issues occuring in GSC or Google can be checked to see if it occurs in Bing tools
    - More data from multiple search engines can be beneficial
    - Bing search engine is not as big as Google but will still make a contribution to your search engine strategy
    - There is more information provided in Bing tools than the Google suite
  - There are some very useful tools in the Bing tools suite
    - Search Performance
    - URL Inspection
    - Site explorer
    - Sitemaps
    - SEO -- Backlinks, Keyword Research, SEO Report and Site Scan
    - Crawl controls
    - Robot.txt tester
  - There are a lot of similarities to the Google suite of tools
  - Bing does offer some functionality not offered by google such as viewing previously inspected URL's

3rd Party Tools
  -
  - There are a huge amount of 3rd party tools available to help with SEO
  - These tools combined with SEO tools will help get the best insight into a site as is possible
  - Using different tolls can highlight issues in certain areas and fixes or recommendations that can be made
  - Its important though to understand what a tool is saying as sometimes a tool can label something as critical and it isn't and vice versa
  - Schema Validator
    - https://validator.schema.org/
    - Google Rich Results test only test schemas that results is rich results within search
    - The schema validator will validate any schema that is in place on a site
    - It can be ran against any URL or JSON code
  - GTMetrix
    - https://gtmetrix.com/
    - It is an excellent site speed testing tool that shows a different view to the PageSpeed Insights tool
    - There are multiple features that make this tool worthwhile using such as the waterfall view, overall performance and structure test
    - There are numerous config settings such as location or device type
  - Web Page Test
    - https://www.webpagetest.org/
    - This is a similar tool to GMetrix
    - It offers core web vitals auditing, CDN check, testing from multiple locations
  - SiteBulb
    - https://sitebulb.com/
    - SiteBulb is a crawling tool for auditing websites
    - Simply enter a URL and the tools will crawl the domain a perform a large amount of tests
  - Screaming Frog
    - https://www.screamingfrog.co.uk/seo-spider/
    - This is one of the most well know SEO tools
    - There is a very good log file analyser in addition to the crawler
  - ContentKing
    - https://www.contentkingapp.com/
    - This is a real-time SEO auditing and monitoring platform
    - Users can monitor multiple sites as well as reporting on a number of different areas
    - It integrates with GSC and Google Analytics and can export to Data Studio
    - It can identify issues before they cause problems
  - Python & SQL
    - Python can be used to speed up data analysis quite considerably
      - It shines when finding trends and other information from larg crawl files
    - SQL is also very useful when analysing data
      - Recommendations for using =QUERY which is a SQL based formula which saves a huge amount of time when bringing in large spreadsheet dats
  - Plugins
    - There are several highly recommended Chrome plugins that should be looked at in addition to tools
    - SEO Meta in One Click
      - Allows users to see the high level SEO features of a page
    - AYIMA Redirect Path
      - Allows for seeing page status codes as well as any redirects that were needed 
      - Useful information froma high level especially when testing individual pages
    - User Agent Switcher
      - Users can switch between agents such as Chrome and Googlebot
      - Useful when checking to see if different agents are seeing different versions of a site
    - View Rendered Source
      - Source code can be compared against rendered source code 
      - It allows users to see which elements are rendered using JavaScript
    - Axe Dev Tools
      - Axe is used for accessibility testing
    - Web Developer
      - Offers a huge amount of functionality such as highlighting links, showing alt attributes
  

Crawling Technology
Why We Crawl
  -
  - Google constantly crawls the web to understand the relationship between websites and to help them rank content
  - It does this by following links that are in place on a site as well as going through sitemaps and any URL's that have been directly submitted
  - The company has huge server farms which constantly crawl the web which update as they go
  - SEOs perform something similar to Google albeit on a much smaller scale
    - They do this to be able to better understand the technical setup and layout of a site
  - There are multiple software applications that allow for simply entering a URL and will crawl a website based on configurations
    - These will return data, normally this is in the form of reports but graphics will be available on most as well
  - There are different options when looking at crawling software
    - Cloud based which are more expensive but more effective at crawling enterprise websites
      - DeepCrawl - https://www.lumar.io/
      - OnCrawl - https://www.oncrawl.com/
      - Botify - https://www.botify.com/
      - ContentKing - https://www.contentkingapp.com/
    - Local Based
      - ScreamingFrog - https://www.screamingfrog.co.uk/seo-spider/
      - Xenu Link Sleuth - https://home.snafu.de/tilman/xenulink.html
      - SiteBulb - https://sitebulb.com/
  - A huge amount of information can be gotten from crawling a site including
    - Overall site information architecture and how pages link or don't
    - Internally linked redirects and where they are linked from
    - Any internal 404's that maybe occuring and reducing them
    - MetaData that is above or below certain expected levels
    - Any duplicate content that is on a site
      - Hashing will show any pages that are nearly identical 
      - Some tools eg SiteBulb will highlight pages that suffer from a portion of duplicate content
    - Security issues which may affect HTTP
      - Any pages that use protocol relative URL's which is a potential security issue
      - https://en.wikipedia.org/wiki/Wikipedia:Protocol-relative_URL
    - HRef lang set so that internationalisation strategy can be audited
    - Schema auditing for any schema related issues that may be happening

Crawling with SiteBulb
  - Demo using the SiteBulb tool

How to interpret and use data
  - Getting the most from the data generated by SiteBulb reports <br/><br/>


<p align=center>Page Experience <br/>
  
Page Speed
  -
  - Faster download of page material means better user experience (UX), less bandwidth and less server work
  - Google reckons 53% of visits to sites are abandoned after 3 seconds
  - PageSpeed Insights is a tool which can be used to check page load speed
  - There are multiple ways to improve download speed such as:
    - Use a content delivery network
    - Optimise and serve any images correctly
    - Remove any assets that are not needed
    - Use a compression technique 
  - Mobile vs Desktop devices
    - It used to be the case that the web was accessed using computers with big monitors
    - Mobile devices were not available and many sites were designed with this reality in mind
    - Layouts were often fixed to common screen resolutions
    - As monitor size increased so did the issues with some sites looking squashed as they only took a small area
    - When smartphones started to come in developers started to develop sites that responded to different screen sizes
    - Enhancements to HTML and CSS were made to allow for these new screen sizes
    - Things such as tags and attributes allowed for graceful responsiveness to different screens
    - Google responded by implementing mobile first indexing
      - It is very important to note that this is mobile first not mobile only 
    - This means that it is always beneficial to have websites work well with desktop and mobile devices
    - Sites that cater only to desktop devices will still be in the index
      - There will however be ranking penalties as these sites will not rank as high as sites that cater for both mobile and desktop devices
    - Even though mobile sites were available the desktop was still considered the primary version of a site
      - Some mobile versions of sites were stripped down versions of the desktop sites
    - Mobile first as made mobile site versions the primary version of a site
    - A good design will work well for all platforms and can ensure good page load speed
  - Page Load Speed Importance
    - Good load speeds for a page is important for a few reasons
    - It provides a better experience for users
    - There are lower bounce rates and higher conversion rates for sites
    - Search rankings will be higher the quicker pages load
    - There are cost savings as well as being environmentally friendly
    - The techniques for making pages load faster also result in smaller files, less bandwidth and fewer CPU cycles
  - Tools
    - There are numerous tools available for help improve page load times
    - They include GTMetrix, GSC, Lighthouse, PageSpeed Insights, Digital Beacon
    - There are many areas that can be tackled to improve page loading speeds
      - They include using a CDN, caching files, using optimised image sizes 
  - Data Types
    - When using tools to measure page experience be careful to check scores are using lab data or field data
      - Lab Data which is data that has been obtained in a controlled manner using predetermined devices and configurations
        - Tools include PageSpeed Insights, Lighthouse, Dev Tools, GTMetrix
      - Field Data is actual data for real visitor to the site
        - Tools include PageSpeed Insight, GSC, BigQuery project, CrUX report
        - CrUX stands for Chrome User Experience Report
        - https://developer.chrome.com/docs/crux/
        - This is a Google database which has real measurements taken from users of Chrome browser
        - These users must have opted into syncing their browser history
        - CrUX data is used in PageSpeed Insights
    - Sometimes field data is more important than lab data
      - One case is when a site's users are predominantly local 

Core Web Vitals
  -
  - Web vitals are a series of metrics devised by Google
  - Core web vitals are the metrics that are most valuable for good UX
  - It uses field data from the CrUX database
  - There is a focus on 3 areas of the user experience
    - Loading, interactivity and visual stability
    - These are First Contentful Paint (FCP), Largest Contentful Paint (LCP), First Input Delay (FID) and Cumulative Layout Shift (CLS)
    - Underlying metrics include time to interactive, total blocking time, speed index and others
  - It is important with mobile first that any issues with mobile versions of sites are fixed
    - However ensure that any issues with desktop versions of sites are fixed too
  - The phrase Good Page Experience should be kept in mind
  - Google classifies sites into 3 categories
    - Good, needs improvement and poor
    - Google recommends that a site be in the top 75th percentile to be considered as a good page experience
    - These metrics should be exceeded wherever possible
  - FCP First Contentful Paint
    - This measures the time it takes from an initial request of a page until the time some content appears in the viewport
    - Content in this case means text, images, anything visible counts as content
    - FCP is the time taken from request until things appear on screen
    - FCP should be equal to or below 1.8 seconds to qualify as a good user experience
  - LCP or Largest Contentful Paint
    - This is very similar to FCP
    - The difference is that LCP is the time from request to the largest content piece appearing on screen
    - Again this can be an image or a text area
    - Elements that are considered for LCP are image or video elements, background images loaded using the URL function and block elements containing text
    - The size to be used to determine the largest element is based on the size of the image displayed
      - It is not the actual image size but how it appears in the viewport 
      - CSS features such as borders and padding are not used in the calculation
    - LCP should be equal to or below 2.5 seconds to qualify as a good user experience
  - FID or First Input Delay
    - This is not the time the page takes until it is able to accept user input
    - It is the time it takes from when a user starts to interact with the page until the page starts to respond
    - To have a good user experience FID should be under 100ms
  - CLS or Cumulative Layout Shift
    - This is a metric which measures visual stability within a page
    - It is of particular importance to mobile sites
    - Sometimes layout shifts happen when trying to go to a certain part of a site and endup somewhere else, this can be more than inconvienient
    - CLS issues are classed as bad user experiences and Google uses CLS as a ranking factor
    - CLS is measured by looking at unexpected layout shifts and how much the layout shifts by
    - A burst of shifts is when a group of shifts occur with less than a second between them
    - The largest of the shifts is used to calculate the CLS metric
    - Largest is defined as being the burst with the largest cumulative score of all shifts in the burst
    - Google used to use all shifts in calculations but changed in 2021
    - A CLS score of 0.1 or less is classed as a good user experience
    - To calculate the score the input fraction is mutiplied by the distance fraction
      - This is the size of an element that has moved between frames and how far it has moved 
    - The larger the asset that moves and the furthre it moves the worse the CLS score will be
    - Layout shift had been resolved but in order to make pages more responsive CSS was used which reintroduced the problem
    - Image optimisation can again help lower CLS scores
  - Tools for measuring Core Web Vitals (CWV's)
    - PageSpeed Insights is an excellent tool for measuring CWV's, GTMetrix is another tool, automated tools have some benefits
      - They can measure from multiple locations and use different devices
      - These tools can diagnose performance issues
      - They will display page loading timelines
      - They will identify protential areas for improvement, list areas for attention and calculate potential savings.
      - They will also provide a list of site diagnostics
  - It is recommended that site owners sign up for GSC
    - If working with a client then ask for access to their console
  - Page Load Timelines
    - These are visual reports that show a page at various points during it's loading
  - Digital Beacon
    - https://digitalbeacon.co/
    - This is a tool which calculates the envionmental impact and carbon footprint of a site
    - It also shows what improvements can be made to make a site even more environmentally friendly

Mobile Usability
  -
  - Mobile Usability is how easy it is for a visitor to interact with a mobile site
  - It is very important as a large portion of site visits are with a mobile device
  - Over 50% of users access a site through a mobile device so this is why getting a device mobile friendly is more important than the desktop
  - Check for Mobile Usability issues
    - Using GSC this is a easy as selecting Mobile Usability from the left hand side menu
  - Errors Reported by GSC
    - Incompatible plugins such as Flash being used on a page
      - Flash is not support be the majority of mobile browsers
    - The viewport is not set which stops the browser from being able to adjust dimensions
    - The viewport is set to a fixed width rather than a device width 
    - Content wider than the screen usually caused by CSS using fixed width blocks which then causes content to go off screen
    - Text too small to read which can happen if it is reduced too much from when a large monitor is used
    - Clickable elements on the screen are too close together
  - Once errors have been fixed it is important to remember to request Google recrawl the site
    - This can be done by clicking the validate fix button
  - Fixing Mobile Usability Errors
    - First take a look at the Summary page and examine the reported errors
    - Are any of the errors to do with the site templates, look for error spikes appearing
    - Check if a single issue may be causing multiple errors
    - Applying a fix here might fix multiple errors
    - Fix errors that are affecting  the most pages or important pages
    - If in doubt then fix errors in the order that GSC presents them as GSC prioritise them

Security
  -
  - HTTPS - Hypertext Transfer Protocol Secure
    - This protocol is the secure version of the HTTP protocol as HTTP is insecure
    - It is the protocol that transfers documents
    - HTTP delivers HTML pages, other data including meta data
    - It can be used to redirect a request to another
    - It is used to encrypt the delivery of webpages
    - It also secures incoming communications and outgoing data
  - Why is HTTPS needed
    - To protect the privacy and security of both visitors as well as site owners
    - The site can be verified, authenticity is important
    - It reduces the risks of malware infections
    - Visitors to the site can be more confident which helps with branding
    - Using HTTPS is an SEO ranking factor
  - SSL/TLS
    - SSL and TLS are the technologies that are used to secure HTTPS
      - SSL or Secure Sockets Layer is the older technology
      - TLS or Transport Layer Security
    - TLS is preferable to any version of SSL
      - Using SSL is definitely better than using plain HTTP
      - TLS is considered as being more secure as well as being faster
      - They use a combination of symmetric and asymmetric cryptography
    - Always use the most recent version of the protocol where possible
    - A digital certificate called an SSL certificate is generated on the server to use HTTPS
      - This is then used at the beginning of all future communications
      - The certificate has a pair of keys, public and private
    - SSL certificates are issued by a Certificate Authority or CA
      - A CA is vetted to ensure that they are trustworthy
      - Certificates are only issued to verified requesters
    - Once the certificate is in place then secure communications can take place
      - TLS and SSL use asymmetric encryption to start a session and decide upon a single shared key
      - This key can be used for the remainder of the session
      - The way that the key is used is called symmetric encryption
    - Client communicating with the server
      - Firstly a secure connection is requested by the client
      - The server responds to the request with the SSL cert, which allows for verification of server legitimacy
      - Responding to the server results in a secure connection being established
      - All communications at this point at encrypted and secure
    - SEO does have some reasons for using HTTPS
      - HTTPS has been a ranking factor on Google since 2014
        - Impact was small but maybe getting larger and is often a tie breaker between 2 similar sites
        - Many small factors together can have a large impact on SERP positioning
      - Reduced visitor bounce rate due to no longer having warnings about site (in)security 
      - There is better referral data in Google using HTTPS <br/> <br/>
  
<p align=center>Schema <br/>
  
What is Structured Data
  -
  - Structured Data is also known as Schema Markup Language
  - Structured Data is organised pieces of information in the form of code snippets
    - It helps search engines better understand what the website content is about
    - They can also appear as rich results on SERP's
  - It is the result of a collaboration between Google, Bing and Yahoo
  - It provides information to search engines so that they have much more clarity about a site content
  - Purpose of Structured Data
    - Its primary purpose is to help search engines properly understand a site's content better than normal text
      - The meaning of content is conveyed to a search engine much more efficiently
    - Structured Data can be used to enhance the appearence of a website on a search result page
      - This includes using code to display logos, contact information, events or other information in the SERP's
    - Users are provided with better and more accurate information in rich snippets which will benefit them
      - This can increase the click through rate (CTR) of a page
  - What Structured Data looks Like
    - Structured Data can be implemented as markup on a page by using one of the 3 major formats
      - JSON-LD
        - This acronym stands for JavaScript Object Notation for Link Data
        - It is the most popular markup script that can be used for structured data
        - Google has JSON-LD as its preferred version for structured data
        - Its main advantage is that it can be implemented as a block of code without messing up the rest of the HTML doc
        - The scripts can be copied and pasted into either the head or body of the page
        - This approach keeps the page both clean and well structured
      - Microdata
        - It is based on a set of tags that highlight values and items on-page for structured data individually
        - Types and properties of structured data are marked up by itemtype and itemprop HTML attributes
        - It's major downside is that every entity or attribute of the content has to be marked up individually
        - This is done within the HTML body of pages 
        - This means that is can be very disorganised and difficult to maintain
        - This is opposed to one big block of code used in JSON-LD
      - RDFa
        - This stands for Resource Description Framework in Attributes
        - This is an extension to HTML5
        - It can be used to markup items for structured data and is very similar to microdata
        - It marks up content pieces in the HTML body of a page
        - It uses the typeof and property HTML attributes when marking up content
  - Why is Structured Data important for SEO
    - Structured data provides a lot more detailed information to Google about a page
    - Google has said that the better a schema is, the better results would be for users

How to choose Structured Data
  -
  - There are many types of Structured Data that can be applied to web pages
    - Event, Review, Recipes, FAQ's and Job Posting articles that appear directly on Google
  - The type you choose will depend on the type of site
  - When searching for events for example the SERP's will return some events with rich results
    - These rich results will show several facts about an event, date, time, address, thumbnail of event images etc
  - When looking at review websites it is different
    - The rich results here will show what is the review of the item being searched for, rating such as stars and marks as well as the reviewer
  - Recipe websites will again differ
    - These rich results will show ingredients, reviews, marks and stars and links to other similar recipes
    - Recipe rsults can also be improved with a few more properties to be guided recipe schema
    - These guided schema will work with Google home and smart displays
      - These results will provided step by step tutorials on whatever recipes are being searched for
  - FAQ types of Structured Data
    - This will provided a list of questions and answers related to a searched topic
    - This can be useful for most websites
  - Job posting Structured Data
    - This has a huge benefit as job specs can be displayed in the job search experience on Google
    - It will show a thubmnail logo, company reviews, ratings and job details
    - It enables users to filter by various factors meaning more and better applicants who are searching for exactly the posting
  - Google has a gallery where each type of schema supported is listed and examples are shown
    - https://developers.google.com/search/docs/appearance/structured-data/search-gallery
  - It can be difficult at times to describe pages via Structured Data
    - This is especially true when there is an endless list of types and properties that can describe a site
  - There are some best practices that should be kept in mind
    - Use the most specific, applicable types and properties that are possible
    - Prioritize any pages that are already ranking high in the search results
      - This increases the likelihood that they will appear as rich results
    - Supply fewer but complete and accurate properties
      - This is opposite to many but badly put together properties that will not inform or help search engines
    - If possible use the JSON-LD format when implementing Structured Data
  - There are a few options available when implementing Structured Data
    - Markup Generators
      - There are many online markup generators available for automatically generating Structured Data
      - The most popular are
        - Google Structured Data Markup Helper - https://www.google.com/webmasters/markup-helper/
        - Google Data Highlighter - https://www.google.com/webmasters/data-highlighter/sources
        - Schema Markup Generator (JSON-LD) Merkle - https://technicalseo.com/tools/schema-markup-generator/
    - Schema Plugins if using a CMS
      - These are a great way for creating Structured Data on a website
      - Yoast SEO - https://yoast.com/
      - Yoast WooCommerce SEO - https://yoast.com/wordpress/plugins/yoast-woocommerce-seo/
      - Schema App - https://www.schemaapp.com/
      - Checkout the official Wordpress site to see more plugins
  
Testing Implementation
  -
  - Google Rich Results Test tool
    - https://search.google.com/test/rich-results
  - Structured Data Testing involves using Structured Data to monitor the health of a page
  - The testing tools provide insights on a page health that is visible to a search engine
  - Testing will help to both identify and resolve any issues with Structured Data
  - Tools will help test Structured Data as it is being deployed
    - They will also help validate Structured Data and snippets on a site
  - The testing tools will provide a lot of information including
    - What format the Structured Data is using
    - Are there any errors in the Structured Data
    - What are the details of any issues with Structured Data
  - Structured Data testing is of critical importance
    - Even a single issue can prevent Google from reading the markup
    - GSC will highlight any warnings that relate to wrong or missing code in the markup
    - In some cases a warning ig given when a single field is not filled out
    - It will allow for detection of errors that are not visible on inspection
    - Testing will multiple tools is the recommended practice for proper implementation
  - The 2 main tools are Google Rich Results Test tool and Schema.org validator
    - https://validator.schema.org/
    - The rich results tool is a simple no-frills data testing tool
    - It is designed to replace the tool the preceeded it
    - Rich results tests enable either code snippets or URL's to be validated
    - As the rich results tool will tell whether a site can use rich results or not, it is still a valuable tool to use alongside others 
    - The rich results tool will show a preview of the page with Structured Data applied

  
Status Codes
What are HTTP status codes
  -
  - HTTP is an acronym standing for HyperText Transfer Protocol
    - It is a protocol used in the exchange of data by clients and servers
  - A HTTP status code is a server's response to a browser request
  - Understanding status codes will help diagnose any site errors quickly
    - This will help in minimising downtime on a site
    - These codes can even help search engines when people access a site
  - The first digit of each code is a number from 1 - 5
    - 1xx or 5xx etc are used to demonstrate codes that would be in that range
  - Each range will cover a different class of server responses
  - When a body payload is present in a HTTP responsed it is used to render and display a page to a user
    - The body payload is optional for some errors
    - There is no body payload for redirects
  - There are 5 classes of HTTP codes, each one is a group of similar or related code meanings
    - Knowing these can help with understanding the general substance of an error code
    - 1xx are informal codes indicating that the request indicated by the browser is continuing
    - 2xx are success codes returned when the browser was received, understood and processed
    - 3xx are redirection codes which are returned when a resource has been substituted for the requested resource
    - 4xx are client errors indicating that there was an issue with the request
    - 5xx are server errors indicating that requests were received but the server could not fullfil the request
  - Within each class are a variety of errors which maybe returned by the server
    - Each individual code has a specific meaning
  - Why HTTP errors matter for SEO
    - The main goal of SEO is to drive organic traffic to a site
    - In order to get traffic to a site, content must be accessible to search engine crawler
    - Sites need to return codes indicating that everything is OK
    - Returning codes indicating an issue is not what any site wants (5xx and 4xx)
    - Keep 3xx codes to a minimum
  - HTTP codes can impact how a site is crawled and indexed by a search engine
    - They also affect how search engines see the health of a website
    - 1xx and 2xx codes will not have much SEO impact
    - 3xx codes can have a complicated relationship with SEO
    - 4xx and 5xx codes can prevent bots from crawling and indexing a site
      - Too many of these can indicate that a site is not of good quality

200 OK
  -
  - This is the best type of HTTP code to receive
    - A 200 level response says that everything is working as it should
  - Incorrect use of HTTP Status Code 200
    - A soft 404 is when a page does not exist according to a website but a 200 response is returned
    - This happens on a site when a 404 Page Not Found is created without returning a 404 response code
    - This is when a site indicates a page does not exist to users but indicating it does to search engines
    - Soft 404 codes can be damaging to a site's performance on search engines
    - Soft 404 pages are generally thin content pages
  - 201 code means created
    - A server has fulfilled a browser request and created a new resource
  - 202 codes means accepted
    - A server has accepted a browser request but is still processing, it may or may not result in a response
  - 203 code means Non-Authoritative Information
    - This code may appear when a proxy is being used
    - It means that the proxy server received a 200 code but has modified the response prior to passing it to the browser
  - 204 code means No Content
    - It means that the sever has successfully processed the request but will not return any content
  - 205 code means Reset Content
    - It is similar to 204 in that the server will not return content after successfully processing a request
    - However it also requires that the browser resets a document view
  - 206 code means Partial Content
    - You might see this code if the HTTP client uses range headers
    - https://www.geeksforgeeks.org/http-headers-range/
    - This enables browsers to resume paused downloads
    - It also enables browsers to split downloads into multiple streams
    - It is caused when a range header causes the server to send only part of a requested resource

3xx Redirects
  -
  - 300 Status codes
    - Redirection is the process that is used to communicate that a resource has moved
    - There are several HTTP codes that will accompany redirections so that user can have information on where the required resource has moved to
  - 301 Status Code
    - This indicates that a resource has permanently moved to a new location
    - The request and all future requests must go to a different URL instead of the requested resource
    - All signals from the requested URL are passed to the destination URL because of the permanence of the redirection
  - 302 Status Code
    - This code indicates that a resource has temporarily moved to another location
    - Because this is temporary no popularity or relevance signals are passed on to the destination URL
    - The redirect URL will still rank instead of being consolidated
    - After a certain amount of time 302 redirects will be treated as 301's by search engines
  - 303 Status Code
    - This indicates that a server is redirecting a request URL to a different URL
    - One use is to prevent users from accidentally re-submitting forms when using a browser back button
    - This redirect tells that a follow-up regards to the temporary URL should be made using the GET HTTP method
  - 304 Status Code
    - This code indicates that the resource has not been modified since the last request
    - When this happens the request is returned to the client as the cached version can be used
  - 307 Status Code
    - This is the HTTP version 1.1 equivalent of 302 code
    - This code guarantees that no change to the HTTP method being used will happen but 302 does not
    - This code was invented to ensure that a HTTP method used to make a request did not change when a server redirected
    - 307 is also used in internal redirects where the browser is aware that HTTPS is enforced
      - The browser will know this from previous requests or if the domain is on a HSTS preload list
      - https://hstspreload.org/
      - https://support.openprovider.eu/hc/en-us/articles/360000350268-What-are-HSTS-and-the-HSTS-preload-list-
      - Using this prevents unecessary and unsafe requests
  - 308 Status Code
    - This is the HTTP version 1.1 equivalent of 301 code
    - It does not allow for changing POST to GET and 301 is recommended instead
  - JavaScript Reload
    - This is a client-side redirect that instructs browsers to load another URL
    - The function window.location.replace is recommended
    - It will send a user to a specified site upon page load 
    - It is not recommended to use these from an SEO perspective
      - Search engines are slow to pick these redirects up as the page needs to be rendered to find the redirect
      - There is no guarantee that the redirect is picked up correctly
      - JavaScript redirects make for slower user experience

4xx Client Errors
  -
  - 4xx status codes indicate that there was an error on the client side
  - Many 4xx HTTP status codes have a large significance for SEO
  - 401 Status Code
    - This code is an error and describes that HTTP authentication failed
    - The requested page requires a password/username combination and/or isn't allowed access based on it's IP address
    - This error is typically seen when crawlers try to access staging environments and HTTP authenication is implemented to prevent this
    - 401 in this case is what is wanted to be seen otherwise search engines would be indexing staging environments
    - This could lead to 3rd parties accessing priviliged information
  - 403 Status Code
    - This code indicates that it is forbidden to request a URL
    - It is commonly used as a permanent measure to prevent crawlers from making requests after misbehaving
    - It is also returned in case the client provides wrong login credentials
    - 401 and 403 are used in different situations
    - A common example is when rogue crawlers tyr to request too many URL's on a site
    - A common misuse is when 403 errors are used instead of using 401 errors
  - 404 Status Code
    - This code indicates that a requested resource cannot be found
    - It is the most common code that is seen by web users
      - This is because it is the code that they see when clicking on broken links
    - 404 errors can also incorrectly occur for existing content
      - It can happen when there is maintenance being done on a server or a bug from development 
    - Search engines will drop from the index pages after hitting 404 errors a few times
    - 404 errors are also bad from a user experience viewpoint
  - 410 Status Code
    - This indicates that a requested URL has been permanently removed
    - Search engines will quickly remove 410 pages from the index due the page not coming back
    - The above fact makes this code a very powerful tool for an SEO to have at their disposal
  - 429 Status Code
    - This code indicates that a client has been making too many requests within a certain amount of time
    - It can be seen as a temporary version of the 403 code
    - Google may remove pages that are returning 429 codes for too long
    - 403 should be used instead of 429 when fighting off rogue crawlers

5xx Server Errors
  -
  - 5xx codes happen when clients make valid requests but servers are unable to complete them
  - If sites keep returning 5xx codes this will have an effect on SEO
    - Crawling will be slowed which means a reduction in crawl budget
    - Pages can also be demoted within the index or removed if issues are persistent
  - 500 Status Code
    - This code indicates that a server had issues when processing a request
    - The server is not able to explicitly state what the error that has occurred is
    - The 500 code is therefore a generic Internal Server Error
    - It is usually generated by faulty plugins, PHP issues or broken db connections
  - 501 Status Code
    - This code occurs when a server does not support the functionality required to process a request
    - It can also happen when a server does not recognise a request method
    - It is also a generic code that effectively means that something has not been implemented
    - There is also an implicit part in that the functionality maybe implemented in the future
  - 502 Status Code
    - This error occurs when there are proxy servers that have not been configured properly
    - It may also occur when there are client servers overloaded or even when there is poor IP communication or even firewall issues
    - This code happens when a server acting as a proxy\gateway gets an incorrect response from an upstream server
    - This is a generic code that simply means Bad Gateway
  - 503 Status Code
    - This indicates that a server is temporarily unavailable but will be available again soon
    - It can happen when scheduled maintenance is happening or if a server is too busy
    - It allows for inclusion of a retry-after value in the response
    - It is similar to 429 code
  - 505 Status Code
    - This code happens when a web server does not support the HTTP version that has been specified
    - It can be caused by the protocol version not being specified properly by the client computer in the request
    - The error that occurs should contain an explanation of why that HTTP version is not supported 
 
  
Content & On-Page
On-Page Basics
  -
  - What is On-Page SEO
    - On-Page SEO is everything that appears within the HTML of a page
    - It includes Metadata, Body Content, Content Quality, Headers, Images, EAT signals
      - https://www.semrush.com/blog/eat-and-ymyl-new-google-search-guidelines-acronyms-of-quality-content/
    - It is one of the most important signals that a site can have
    - If all on-page signals are bad then content will not easily appear within SERP's
  - Viewing And Understanding HTML
    - https://www.w3schools.com/html/
    - This is the most important skill for an SEO to have
    - You should be able to look at a page, analyse the HTML and understand what is happening
    - A lot of information about a site can be revealed by looking at the HTML
    - This is not always the case ie some newer JavaScript frameworks such as Angular and React handle pages differently 
    - To view the HTML of a page, open it up then right click and select View Page Source
    - There is also right click and select Inspect
    - There are some differences between these 2 different methods
      - View Source shows the raw HTML that was delivered from the server to the browser
      - Inspect allow for examination of the DOM tree after JS has manipulated the HTML and HTML errors have been fixed
      - There is also an extension called View Rendered Source which allows for seeing pre and post rendered page versions 
      - https://chrome.google.com/webstore/detail/view-rendered-source/ejgngohbdedoabanmclafpkoogegdpob?hl=en
  - The DOM
    - DOM is an acronym standing for Document Object Model
    - It shows the overall structure of a webpage split into logical sections
      - These are bulit with HTML attributes and elements
      - https://medium.com/crowdbotics/understanding-the-dom-e137c18f3cba
    - The DOM is not what you see in the browser
    - What is seen in the browser is the render tree which is a cobination of the DOM and the CSSOM
      - https://developer.mozilla.org/en-US/docs/Web/API/CSS_Object_Model
    - What separates the DOM from the render tree is that the render tree consists of what will be painted on screen
    - The render tree is only concerned with what is rendered
  - 
