<b><p align=center>                    
  Blue Array Academy SEO Manager </br>
  Course Notes  
https://www.bluearrayacademy.com/courses/seo-manager-certification

<br />
<h1><p align=center>Welcome </h1><br/>
  
Course Update History
  -
  - A list of updates to the course is shown
  - The course is updated regularly in order to keep it relevant

About this course
  -
  - A quick intro to the course
  - The course shows the way Blue Array do SEO for clients <br /><br /><br />
  

<h1><p align=center>How Does Search Work </h1><br/>  
  
Market Share of Search Engines
  -
  - There are 5 dominant search engines
    - Google - https://www.google.com/
    - Bing - https://www.bing.com/
    - Yahoo - https://ie.yahoo.com/
    - Yandex - https://yandex.com/
    - Baidu - https://www.baidu.com/
  - Duck Duck Go is another search engine available but the course concentrates on the 5 above
    - https://duckduckgo.com/
  - Google is by the market leader with about 93% share
  - Bing and Yahoo largely return the same result as Bing powers a lot of Yahoo
  - Yandex is Russian and it's audience is mostly Russian speaking, Baidu is the same but for China

SERP features & Google Horizontals  
  -
  - Google's results have evolved considerably but not affected their dominance
  - Google SERP's continue to change and new features are added and removed
  - Google adverts will have a box saying Ad beside the URL
    - This kind of result will not be covered in the course as they are not organic search results
  - Structure data and Rich results will have a large impact on rankings
    - More information on the kind of technical factor is available in notes at
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md
  - Depending on what is being searched for there will be a knowledge card on the right of the search results
    - This can contain aggregated information from different sources
    - It is hard for a single site to fully own this
    - It is easier for companies to own this if they have filled out Google MyBusiness of Google Brand Account
    - Using MyBusiness and Brand Account can help getting location specific maps appearing on the main SERP page
  - Horizontals refer to the list of search tabs such as News, Videos, Images etc
  - Many of the listings found on the Shopping horizontal are paid for listings

Crawling, Rendering and Indexing
  - More information on the kind of technical factor is available in notes at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - Crawling, rendering and indexing is how a page appears in the SERP's
    - These are independent of a page's rankings in the SERP's
  - https://developers.google.com/search/docs/crawling-indexing/javascript/javascript-seo-basics
  - If Google encounters a noindex prior to executing JavaScript then the page will not render
  - Orphan pages are pages that a page lacks links from other pages on a site
  - https://www.danielmorell.com/guides/htaccess-seo

The Notion of Crawl Budget
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - Most sites do not need to worry abount crawl budget as it is only sites over 1000 pages that start having issues

Basics of HTML DOM, What it is and why it's important
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 

JavaScript and modern Web frameworks
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - JavaScript is a very technical topic for anyone new to SEO
  - Search engines find it easier to process pure HTML versus JavaScript
  - JavaScript is great for aesthetics and features but not so good for search engines
  - https://www.linkedin.com/pulse/ultimate-guide-javascript-seo-syed-faraz-abbas-rizvi
  - There is no reason to use JS for aesthetics if they can be created without it
  - Search engines cannot do onClick() events
  - Not using JS properly can result in Google thinking that a site is trying cloaking
    - https://en.wikipedia.org/wiki/Cloaking
    - Using cloaking will have consequences for a site's SERP rankings

Black Hat SEO vs White Hat SEO
  -
  - Black Hat SEO describes optimisation techniques that go against guidelines
    - Techniques that follow guidelines are called WhiteHat
  - Avoid Black hat techniques like the plague
    - They are usually only successful for a short period of time
    - The penalties that a site will receive often far outweigh any benefits
  - Google updates the search algorithm several times a year
    - Each time there are winners and losers but not every loss is a penalty
  - Greyhat is a term which describes techniques which are not against search guidelines but do contradict ethics
    - Link building can be an example of Grayhat activities
    - https://developers.google.com/search/docs/essentials/spam-policies?hl=en&visit_id=638114020697901476-3303849116&rd=1#link-spam
    - Technically anybody reaching out for links can be seen as greyhat but as long as honest efforts are done there is little risk
<br /> <br /> <br />
  
<h1><p align=center>SEO Strategies </h1><br/>

How to identify your SEO strategy
  -
  - Identifying an SEO strategy early is important so that you are aware what needs to be done with a site
    - This should take into account any challenges that there are within the business
  - 4 common pillars of strategy to maximise results
    - How your site currently performs, what are its strengths and weaknesses
      - This shows where is performing and where performance is not meeting expectations
      - Without knowing where a site is at the moment performancewise it will be very hard to know where to target improvements
    - Who your real competitors are
      - When working with businesses they should know who their competitors are
      - In this case the real means competitors in the SERP's rather than shops etc
    - What are the goals of the business
      - This is something that often be overlooked by businesses
      - The purpose of SEO is not just to rank high for certain queries
      - If this cannot be measured against business goals then calculating returns on any investment is going to be tough
      - Everything that will be done should always start with understanding what purpose it serves
    - Identify who the key enablers are
      - SEO is not something that is easy because there are many different elements
      - Find the things that will drive results as fast as is possible
      - An SEO strategy is usually a 6 to 12 month exercise
  - Start thinking as early as possible about SEO
    - This should include yourself and how you will approach things
  - The one size fits all formula does not exist
    - There are multiple factors that will affect your strategy
    - The strength of a site such as its size, authority, reputation and others
      - Is the site a new one or has it been online for several years
      - There should be as much understanding of these as possible as they will affect SEO strategy and it they are wrong so then will be the strategy
    - The industry that a size is involved in
      - Different niches will have different strategies as what works in another industry will not work in yours
    - The Competition Landscape
      - Some niches will have low volumes of queries
      - If competition is low this can still work well
      - Again if there are low volumes for searches in a niche this will have an effect on SEO strategy 
    - The site market (Is it local or international)
      - Is the main part of where a site is selling to located locally or will there be shipments abroad
      - Understand where your market is today as well as how it is likely to evolve otherwise opportunities may be missed
      - This covers a period of the next 6 to 12 months
    - The type of site that a site is such as whether its an e-commerce site, blog, forum etc
      - Each of these type of sites comes with their own unique challenges
      - E-commerce sites for example can be very challenging to put together an SEO strategy for
      - Blogs that have lots of comments, guest posts etc and other type of content need to be fully understood
    - What are the business goals
      - How much brand awareness or how much revenue is expected from organic search will affect SEO strategy
      - Not having an initial expectation or initial goals may not be a bad thing as it will give time for proper early discussions
    - Budget and Resources available
      - Simply put the more resources available then the more options are available
      - One example is having access to developers or specialist SEO experts eg Backlink specialists
      - Being in a smaller business without access to some of these is not necessarily going to prevent success
      - SEO is not tied to how much money can be spent on an SEO strategy
  - The course structure
    - Improving SEO skills
      - The course will drastically improve SEO skills as well as confidence
    - Answering what the 4 pillars of SEO are
    - Maximise the chances of success and the ROI
  
SEO Roadmap Template
  -
  - Downloadable template

Creating your mid-term SEO strategy
  -
  - Quick review of the Roadmap Template which will be updated by the student as the course progresses
    - The 6 - 12 month timeframe is envisaged
  - Examples of Situations
    - Where a site is new or has low visibility
      - There are not any magic formulas but only guidelines and pointers
      - Focus on content as if there is little content it will be very tough to bring traffic
        - Look within the niche to see what the audience is interested in or talking about
        - If running an ecommerce site it maybe worth starting a blog around the niche your site focuses on
        - Make sure that the content is properly optimised and uses keywords that will convert
      - Go after low volume and low competition keywords
        - New sites with low visibility and low authority will not be able to go after competitive keywords
        - Newer sites will probably be better targeting low volume long tail keywords
      - Structure the site properly
        - Being low visibility gives an opportunity for information architecture to be implemented correctly
        - Make sure that everything on site flows well for users as well as search engines
        - While a site is still new it means that changes needed can be made much more easily 
      - Quality over Quantity
        - Its better to have less than 50 pages that are well organized than 1000's that are not
        - Make sure that the small number of pages are keyword optimised and offer great value to users
        - Remember that thin content does not bring any value
    - A site is up against fierce competition
      - Most industries are very competitive today, any that aren't usually will be within a couple of years
      - Focus on sub-niches when it is possible
        - If there is a specific part of the niche which has good demand then that should be focused on
      - Identify the weak spot of competition
        - Are there things that competitors are not doing well that you could focus on and do better
        - Breaking dow things may show pockets of opportunities when you can get ahead despite the fierce competition 
      - Set the right expectations internally
        - Stakeholders may not appreciate how much effort is involved when competition is very high
        - It is the responsibility of the SEO to set those expectations correctly
        - Demonstrate that you have taken into account the high competition to give confidence to stakeholders and avoid disappointment
      - Go after highly targeted keywords
        - These will convert well because the volume is low
        - Even with fierce competition there will be room for you if using the correct long tail keywords
    - A site that does not have a strong backlink profile
      - Backlink profiles are always difficult because it will be an area where a site is not in control
      - A high quality keyword can be better for a site than lots of low quality backlinks
      - Identify the gap in the backlinks profile
        - Find out how big this gap is, look at competition and how many backlinks that they have
        - See which are genuine backlinks and how quickly these backlinks were acquired
        - Is there a process that the competition uses to acquire backlinks
      - Outsource digital PR (If possible)
        - If resources are available then it maybe possible to work with an agency
      - Improve and optimise internal linking
        - It acquiring backlinks is proving very challenging then one thing to do is improve internal linking
        - Although they are not backlinks internal links can be very powerful as they help search engine crawlers
          - Optimizing with anchor text will help to improve SERP ranking
        - This approach can bring very good results so might be a good place to focus some of an SEO strategy on
      - Have a process in place which makes you consistent
        - Many businesses are not consistent and tend to work in waves
        - Instead of not doing anything for a period and then trying to rush instead try and do some work regularly even if it is small
        - SEO can be a lot about consistency
    - A site has technical issues
      - Fix in priority can be usually quick wins
        - Sites for example that are fast on desktop but slow on mobile can be first for fixes
        - Google rewards sites that are quick on mobile devices so getting issues fixed quickly can be rewarded quickly  
      - Migrate the site
        - Changing the website hosting can be a fix for several issues 
        - This can be a stressful time but it is often something that is better to get done rather than leave it until later on
        - If there are too many issues that are not being fixed on present hosters then it may be the only answer
      - Outsource if it is needed
        - If the knowledge is not available in-house to fix things then contracting out maybe the best option
        - This can in some circumstances a one time expense
      - Prioritise the issues
        - Not all issues can or will need to be fixed all at once 
        - The main thing is that issues are identified so that they are at least recognised
      - Monitor any fixes
        - Have processes in place that monitor fixes to ensure that the same issues do not reoccur 

Producing Your SEO Roadmap
  - A run through of the different elements of the SEO roadmap template
  - Tips and Best Practices
    - All sites evolve as do users and search engines
      - User behaviour and search engines are changing significantly 
      - Google for example releases core and minor updates on a regular basis
    - The competitors of today may not be the ones of tomorrow
      - Newer competitors may enter the market so there is a need to be proactive in understanding these   
    - SEO is an ongoing processes
      - What maybe successful for a site now may not be in the future
      - Doing nothing to prepare for these changes will see a site's rank fall over time 
  - Key Takeaways
    - SEO strategy should have answers to the 4 pillars
    - Update the SEO roadmap as the course progresses
    - Be consistent
    - Focus on getting the quick wins first
  
<br /> <br /> <br />

<h1><p align=center>Site Audit : Checklist & Requirements </h1><br/>

Why Audit your site and how often should you do it?
  -
  - Downloadable resource
  - The document covers in some braod terms some of the things needed in an SEO technical audit as well as what to look out for
 
Audit Requirements
  -
  - What is a new client checklist
    - This is a document provided to clients with checklists to fill out and questions to answer
    - It allows for understanding of a client's industry as well as asking for certain things which allow for sites to be audited
    - There will be multiple things asked for in a client checklist
  - What should the new client checklist contain
    - There should be access to a number of tools such as GSC, Google Analytics, Bing Webmaster Tools, Google Tag Manager
    - Information about the site such as Internationalisation, Domain History and any major competitors
    - Detailed technical information about the site such as crawling permissions and their speed, user agents to whitelist as well as any pain points
      - User agents may be provided to the client so that they can be whitelisted to avoid crawling being blocked
      - Pain points should also be disclosed in case the client knows and already has fixes being processed 
    - Log Files to determine which pages Googlebot is visiting vs user vising
      - These can be used with other technical findings 
    - Some wider business questions such as company objectives within the next couple of years
      - What is the Unique Selling Proposition (USP)
      - What is the core company mission and what are it's core values  
    - Marketing such as what campaigns are currently ongoing
      - What campaigns have been ran previously
      - Has there been an SEO agency been used before and if so then what did they do 
    - Having all of the above answered in not completely necessary but it is a good thing to have
  - What we need to perform a technical audit of a site
    - Access to the site and permission to crawl
      - This is needed to understand the site and the technical setup   
    - Google Search Console (GSC) and Bing Webmaster Tools
      - This is Google's interface with webmasters, Bing tools should also be used if the client has them
      - Google Search Console will be sufficient if Bing is not available
    - Google Analytics
      - This will make available in-depth data for the client's site 
      - If there are other packages available such as Adobe Analytics that would be good but not absolutely necessary
    - The specific site pain points that the site owners are aware of
    - Domain History such as if the domain was owned by someone else
      - If the domain was recently bought or the client has other domains is valuable information to know 
    - Log Files have an abundance of information for performing a technical audit
  
Enabling & Configuring Google Search Console
  -
  - Search Google for 'Search Console' and this is the first result that will be found
    - https://search.google.com/search-console/about 
  - It is an interface between you and Google for reporting on your website
  - It can really help improve site performance within the SERP's
  - GSC offers reports on multiple areas
  - Users will need to add a domain manually
    - There are also instructions for adding GSC to your DNS provider
    - The whole domain does not have to be tracked, the URL prefix option allows for adding GSC to a certain part of the site
    - It is important when using the URL Prefix option that the canonical version is use
  - After the site has been added then it must be verified so that GSC can start receiving data
    - One option is to download a html file and add it to your website
    - Another option is to use Google Tag manager and add a tag to your site 
  - In order to keep GSC working it is important not to delete either the tag or html file which ever was added to the site
  - If GSC has already been added to a site then you will need to be added as an admin
    - Users can then be added with specific permissions 
  - Getting added by the site owner as a contibutor is much easier than verifying a site
  - Fetching a url for the first time may take a couple of minutes
  
Enabling & Configuring Google Analytics (GA)
  -
  - Search for Google Analytics and the first result will be the analytics site
    - https://analytics.google.com
    - Users will have to be signed in to their Google account 
    - Google Analytics is a free tool
    - It provides a lot of analytics in one place which allows for organisations to make smarter decisions
  - Why you should use Google Analytics over a different provider
    - Google Analytics has a lot of room in the free tier, approximately 10 million hits per month
    - In some instances Analytics 360 which is paid might be a better option
    - If you go over the limits of Google Analytics then you might start getting sampled data
    - Unless your site is getting an enormous amount of traffic then GA is the perfect tool
  - Enabling Google Analytics
    - Simply click the start measuring button
    - This takes you to a create account window where account details and other things will need to be entered
    - Try making the account name something that makes sense
    - You can track websites, mobile apps or both together
    - Once successfully set up, you should be presented with a gtag.js tag
      - This is the tracking code for the site   
      - This tag has to be in the head section of every page that you wish to track
      - Different platforms such as WordPress will have their own ways to allow this tag to be added eg the tracking id can be added in WP
    - GA has multiple customisable filters and settings to allow for fine grained data analysis

Technical Audit Checklist
  - 
  - Downloadable text resource
  - This is an Excel spreadsheet allowing for checking for site issues

Working with your Audit Checklist
  -
  - What is Technical Auditing
    - Technical auditing is the process of looking at a site from a technical perspective
    - We will be looking for any technical issues or problems
    - There will not be any looking at backlinks or keywords
    - If any issues are found they should be documented
    - There can be 3 stages within the documentation process
      - What is the issue that is occuring
      - Why this issue is an issue in the first place
      - How can this issue be fixed 
  - Why do we audit
    - Audting is a vital part of any SEO initiative
    - It not only highlights issues but guides strategy over the next 3 - 12 months 
    - There is little point in working on other SEO items if a site is not technically sound
    - Sites with a solid technical foundation help search engines understand a site better
    - Sites that are better understood by tsearch engines tend to rank better
    - Auditing helps identify issues that a search engine may see and provides recommendations for fixes
  - A quick run through of the checklist provided

Task Assignment 1 - Configuring and Checking GSC
  - 
  - A practical assignment to get your site working with GSC

Task Assignment 2 - Configuring and Checking Google Analytics
  -
  - A practical assignment to get your site working with GA
  
<br /> <br /> <br />

<h1><p align=center>Technical Site Audit Part 1 </h1><br/>

All Technical SEO Material including Auditing can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md

Understanding HTTP Codes
  -

Robots.txt
  -

Checking Your Site Speed
  -

Crawling Your Site with Screaming Frog
  -

Checking Your Internal Linking
  -

XML Sitemap
  -
  
Task Assignment 1 : Perform a Technical Audit of Your Site (Part 1)
  - This is a practical assignment where students will perform a technical audit of their own site
  - Use the provided Technical Audit Checklist
  - Note any areas that are being flagged as issues
  
<br /> <br /> <br />

<h1><p align=center>Technical Site Audit Part 2 </h1><br/>

Again all Technical SEO Material including Auditing can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md

Checking Legacy URL's with Majestic
  -
  - https://majestic.com/
  - This tool scrapes the web and creates a database of encountered URL's
  - The historic index contains a 5 year backlog of any URL's encountered by their crawlers during that time
  - Checking URL's
    - Ideally these URL's should respond with a 200 status code
    - Or 301 redirects which redirect to a new location within 1 hop
    - Large amounts of 4xx errors can be problematic
    - The same applies for long redirect chains
      - Follow redirect chains until the end of the chain is reached
      - Alternatively only follow for as many hops as is of interest
      - These long chains should be flagged as an issue
      - The same will apply to any errors that are encountered
      - Any URL's with backlinks should be of particular interest 

Canonicals
  -
  - Using Screaming Frog to perform checks on canonical links
    - Also use the coverage reports section of GSC
    - Searching for excluded URL's can yield benefits 
  - Simple checks can be done using devtools and searching the DOM for the phrase 'Canonical'
    - If it is not present in the DOM or source code then it may need further investigation
  - Paramaterised URL's that canonicalise elsewhere are fairly standard practice
  - Paginated pages that canonicalise elsewhere are not good practice and warrants further investigation
  - Site canonical hygene is important and should not be left to google to decide
    - Google can on occasion can select the wrong canonical tag 
  
Pagination
  -
  - Rel=next/prev used to be the recommended way to handle pagination
    - It was used to denote logical pagination structure 
    - In 2019 Google anounced that it had not been using this method for a while 
    - Although not necessary any longer, it can be harmless if implemented correctly if left in HTML
    - When incorrectly implemented rel=prev/next can cause erros such as bot traps
      - https://www.contentkingapp.com/academy/crawler-traps/ 
    - Bot traps are often seen when crawlers are sent down a non-existent chain of URL's linked through rel=prev\next markup
  - To search for any instances of rel=prev\next
    - Open up dev tools on a page and search for next which will find any occurences of the word 'next' in the DOM including tags 
  - Canonical Tags
    - Check that the canonical tag is implemented properly when using pagination
    - Each page in a paginated series should have a self referential canonical
    - Often seen is page 2+ of a series canonicalising back to page 1 which is bad practice
  - Nonindex Tags
    - Pages in a series are supposed to be indexable
    - It used to be considered good practice to noindex pages 2+ of a paginated series
    - A noindex on a page for extended time can result in the page being no longer crawled 
  - Load More
    - Often 'Load More' buttons are used instead of numbered pagination
    - If the button does not contain a real link it will not be properly crawlable
    - Check the DOM to identify whether it contains any <a href> links to further content sections 
    - Inspect the button in the DOM
  - Infinite Scroll
    - A similar issue exists with infinite scroll
    - JS events which are often uncrawlable are used to load content after a certain point in the page
    - Pages with infinite scroll should support paginated loading with unique links to each section
    - The DOM can be searched for what would be the next logical URL in the series
  
JavaScript
  -
  - JavaScript heavy sites
    - When dealing with JS heavy site it is important to identify whether links are buried in content that relies on JS
    - One way to do this is to use a Chrome extension to disable JS and compare by observing how the page looks
    - Also take the time to compare the DOM to the page source
      - The DOM is the rendered version of the page, page source is the HTML
  - Crawling JS heavy sites
    - A JS heavy site will affect how the site is crawled
    - Using a tool such as screaming frog, run 2 crawls, 1 for text only and the other using JS rendering
      - This will show the impact that using JS has had on a site 
  - SPA or Single Page Applications
    - These are JS web apps which use JS to dynamically update instead of loading new pages from the server
    - When they are done correctly they will be indistinguishable to users
    - Under the hood SPA's are very different to normal web pages and can cause SEO issues
    - SPA's have become reknowned as being bad for SEO
    - Since google switched to Evergreen Chromium in 2019 the issue has lessened
      - https://searchengineland.com/evergreen-googlebot-chromium-rendering-engine-316652
      - This is because from this point Googlebot could render JS
    - Other search engines may not be able to render JS
    - Relying on Google to render a large amount of JS pages is not efficient
      - JS heavy pages can take longer to index due to having to render a large amount of content
  - Pre-rendering
    - Pre-rendering is a strategy for ensuring that JS heavy sites can be crawled by search engines
    - Pages are rendered and then cached server side
    - The cached version is then delivered to search engine user agents
      - Users will see the live version of the page
    - This means that bots will see a slightly different page to users
      - This should not be a problem if the cache is updated regularly
    - Again Screaming Frog can be used to check if the cached pages and user page differences are an issue
  
What is AMP and how to test AMP pages
  -
  - AMP is an acronym standing for Accelerated Mobile Pages
    - https://instapage.com/blog/amp
    - These are pages that use a special stripped down HTML framework
    - It was created to make things faster on mobile
    - It strips non-essential components and sometimes replaces JS with AMP library components
  - AMP Setup
    - All AMP pages must contain some boilerplate code in the head tag
      - https://amp.dev/documentation/guides-and-tutorials/learn/spec/amp-boilerplate
  ```
  <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;
    -ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start
    {from{visibility:hidden}to{visibility:visible}} @-moz-keyframes -amp- start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes 
    -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}} @keyframes 
    -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript>
  <style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
  ```
    - Normal HTML tags can be used on AMP pages, there are some AMP specific alternatives though
        ```
        <img> is replaced by <amp-img> 
        <iframe> is replaced by <amp-iframe>
        ```
    - A number of HTML tags are prohibited
       ```
          <embed>
          <frameset>
       ```
    - AMP pages must be setup as counterparts to non-AMP pages
    - AMP pages need a rel=canonical tag pointing to the non-AMP version
  - AMP Restrictions Javascript
    - Because AMP needs to be streamlined there are restrctions on what developers can use
    - Third party JS is banned other than in iframes
      - It can be loaded in sandboxed iframes using the following tag
            ```
            <amp-iframe>
            ```
      - This is done so that the critical rendering path is not blocked
    - Custom JS must be loaded asynchronously, again to stop the page rendering from being delayed
    - There is access to a library of AMP components
    - AMP components use JS under the hood, it is not editable however
    - The following tag captures analytic data from an AMP document
      ```
      <amp-analytics>              
      ```              
    - These components are to be used where there are gaps due to JS restrictions
  - AMP Restrictions Images
    - Images must have a predetermined size and position which is stated in the HTML
      - This is called static layouting
    - The layout attribute can be used to make images responsive
    - The following tag must be used with images
      ```
        <amp-img>
      ```
    - This tag allows the AMP renderer to understand the page layout prior to assets being loaded
      - It also relies on JS so a fallback img tag should be provided within a noscript tag
      - Only using the img tag will results in images being rendered in the browser but not validated as AMP
  - AMP Restrictions CSS
    - Stylesheets in AMP are limited
    - External stylesheets are prohibited
    - Styles can be either inline or internal which is created in the document head section
    - Only one internal stylesheet can be included on each page
    - There are some disallowed styles such as the use of a reference to the !important qualifier
    - Stylesheet size must remain below 50kb
  - When to use AMP
    - AMP is most often seen on contentful pages
    - There is a case to be made for use on sites with high mobile traffic      
    - AMP may also benefit certain types of pages such as news articles       
    - Only AMP pages can be used in Google News Carousel          
  - AMP Benefits 
    - AMP reduces the number of requests so reduces page load time
      - The critical rendering path does not get blocked by resource downloads
    - To speed up AMP pages, Google crawls and caches them         
    - Pre-rendered AMP pages are served in mobile search results          
  
Manual action checks using GSC
  -
  - How to check for manual actions in GSC
  - Manual Actions
    - These occur when Google guidelines are breached
    - These breaches can be either intentional or unintentional          
    - These are submitted after a human person has deemed a site to be breaking guidelines 
    - They are found in Google Search Console        
    - Once issues are resolved a resubmission request can be submitted to GSC
    - The resubmission request should contain some items
      - Exact details of the issue and the steps taken to amend it
      - Actions that have been take to stop the issue happening again
  
Index Coverage, Sitemap, Speed, Links using GSC
  -
  - An examination of various reports in GSC
    - Index Coverage, Sitemap, Speed, Links reports are covered
  
Task Assignment 1 : Perform a Technical Audit of Your Site (Part 2)
  -
  - This is a practical assignment where students will perform a technical audit of their own site
  - Use the provided Technical Audit Checklist
  - Note any areas that are being flagged as issues
  
<br /> <br /> <br />

<h1><p align=center>Producing a technical backlog </h1><br/>

Interpreting your crawl data
  -
  - How can we investigate crawl data
    - Google Sheets is used in the example and a number of different reports can be filtered and viewed   
    - Data can be sliced as much as is needed to extract as many trends as possible 
    - The same data used in the technical audit module is used as part of the demonstration          
  - Quick Wins
    - Using the example dataset there are some quick wins available, these should be taken where possible
    - Redirection of trailing and not-trailing slash versions of websites is relatively is easy and could be a quick win
    - Removing title duplication could be seen as another quick win
    - There would have to be enough of these to be seen as a priority          
  - How to present the data
    - Data should be presented in a way that means something to stakeholders
    - This can include things like graphs or summary sheets
    - Summary points within audits and backlogs can help summarise issues          
  - How to add issues to the backlog
    - Issues should be added to the backlog with areas given particular attention
    - The title of the issue
    - The type of the task and whether it is a technical focused task
    - An issue description with as much detail as required for understanding 
    - A brief solution to the issue, it should have as much information as necessary to understand the solution
      - There can be a link to further documentation if it is needed            

Prioritising your tasks
  -
  - How can tasks be prioritised
    - Tasks that are added to a backlog or provided to a client or internal stakeholder should be prioritised
    - This approach ensures that the most important tasks which could cause damage are tackled first
    - Minor tasks can be identified and completed at a later time
  - Critical, High, Medium & Low level Tagging
    - When a technical backlog is being produced eask task should be assigned a priority
      - Critical which is when an issue needs to be resolved as soon as possible
        - If this issue is not resolved there will be a hit to SEO performance
        - One example is when there would be a noindex tag on a site homepage
      - High for when an issue is not 100% urgent but will have a positive effect when fixed
      - Medium is when an issue is not as critical as high, resolution may provide some benefits
        - Some issues maybe tricky to fix and may not be that beneficial to fix
      - Low is for when a task would be a positive to fix but offers no immediate effect on SEO performance
  - Impact vs Ease of Change
    - When tasks are being prioritised there are a number of factors to look at
    - 2 of these factors are Impact and Ease of Change
      - There will need to be identification of what sort of impact will happen if a task is completed
      - As to be identified is the ease of completion of a task
    - Understanding the scale of issues is very important
  - Uplift
    - This is usually in the form of a high, medium or low estimate in the uplift in traffic
    - This can usually be done by experience or working with large datasets
    - What usually improves technical performance will be classed as high, otherwise is will be classed as low
  
Tips to produce a backlog using a template
  -
  - Text based hints on producing a backlog
    - Give tasks a concise title
      - This makes it easier to refer to in communcations
    - Issues and Solutions
      - Backlogs should be usually be in the format of a spreadsheet
      - This will allow for columns for issues, explanations and solutions
      - Issues and solutions should be concise
    - Explanation
      - Explanations are usually included as teams using the backlog may not have been present during creation
    - Prioritisation
      - People who may not know SEO might need to see the pirority of tasks
      - Flags should be used to indicate priority levels as well as being put into order
      - Quick win tasks should be marked out as such
    - Task Allocation
      - Tasks should be appropriately assigned as multiple teams may be using the backlog
      - Clarity is very important here as it stops confusion or more than 1 person working on the same thing
      - Accountability is also improved here as people will know what is expected of them and where to raise any issues
    - Audit Resources
      - Any resources collated during the audit process should be linked within the backlog
      - It can be things like exports from software which are linked to the pages that are needed to be fixed so that time is not wasted searching for things
    - Task Status
      - Everybody involved should be aware of where the issues in the backlog are in terms of progress
      - There are 3 categories, Not Started, In Progress and Completed
      - The actual titles are not that important as long as the meaning is clear
    - Templating the Backlog
      - There will likely a need for multiple audits so backlog reuse is a maybe
      - Create a backlog template that can be used each time will ensure consistency in deliverables
      - Explanations for expected issues should also be included
      - Auditing may at first create best practices but that may not last so hold on to any meterial on recurring issues
          
Technical Backlog Template
  -
  - Downloadable spreadsheet file provided    
  
Task Assignment 1 - Create Your technical backlog
  -
  - Use the template provided to help create a detailed technical backlog 
          
Task Assignment 2 - Work with developers to understand the effort needed on each task
  -
  - Work with people who are responsible for different tasks

<br /> <br /> <br />

<h1><p align=center>Benchmarking Against Competitors </h1><br/>

Finding Your Competitors
  -
  - Why look at competitors
    - Looking at competitors and what works for them can help find opportunities for any business and it's website
    - Research should look at
      - What keywords they are using and how it is affecting their ranking
      - What type of content works for them and how you could replicate it for yuor website's needs
      - Where backlinks are coming from and where those backlinks are sourced          
  - Industry Competitors
    - These type of competitors are usually in the same industry as you
    - If your potential customers have you in mind then they will likely have your competitors too          
  - Search Competitors
    - Looking at organic search to see which sites are ranking well for your target keywords will show search competitors
    - Search competitors will not always fit into a single market or niche
    - These can be found by using a few methods
      - Use tools like SEMRush to see which domains are ranking for those target keywords
      - Use te SERPs to see which sites are ranking well for keywords
      - Look at who is bidding for paid space for the target keywords
      - Use social media, forums and customer feedback to see what brands are getting mentioned for you keywords
      - Use YouTube to check your keywords, those channels at the top are likely to be in SERPs too          
  - Aspirational Competitors
    - This would be competitor is usually a dominant player in the market you are in and in popular with your target audience
    - This competitor will usually be in the industry competitors list based on it's brand and trust level
    - You will probably not be able to compete with them at first but it would be foolish to ignore them completely
    - They should be used more as a benchmark of where you would like to be
    - Using some of their strategies may be possible and it is important to remember that they will have had wins and losses previously          
  
Domain Authority, Trust Flow and Citation Flow
  -
  - The concept of backlinks
    - Backlinks play a significant part in SEO along with technical optimisation and keyword targeting
    - Backlink strategy is catgeorised as off-page SEO
      - This is because it is something that cannot be controlled or influence through code on pages          
    - A backlink is a link from someone else's website to our own
      - It is more aligned with PR and general marketing or using other things like Social Media to amplify content         
    - Backlinks are a signal to Google that others trust our content 
    - The more good quality backlinks in a backlink profile the stronger a site would be          
    - This will manifest itself through SERP ranking inprovements and maybe overtaking competitors
  - Why are these metrics important
    - It is important to look at competitors and see which have the strongest and highest quality links
    - This should also show the type of content attracting strong backlinks
    - A strong backlink profile should balance both quality and quantity links as these metrics are both indicators
  - Domain Authority (DA)
    - There are 2 accepted and often used definitions of domain authority
      - It is a score between 1 and 100 by Moz to esitmate how likely a website is to rank within organic search results
        - High scores indicate a better chance to rank     
        - This is not a SERP ranking factor, it is not considered by search engines
        - Domain authority is also a catch-all term used for the perceived general strength or quality of a site 
        - It is based on how many backlinks it has and how authoritative these links are 
      - Trust Flow is a metric that is used in the application Majestic
        - Majestic is a tool which can help determine how strong or authoritative a site is
          - https://majestic.com/
        - It uses a similar scale to Moz with 0 - 1 being the least trustworthy and 100 being the most
        - It measures the authority of the websites that are linking to your site
        - Trustworthy sites are the sites that are good to get backlinks from
      - Citation Flow is also a metric that is used by Majestic
        - This is used to measure how many inbound links a particular domain has
        - It is used in addition to Trust Flow
        - It is based on how the quality of inbound backlinks that a site has acquired
        - It is also measured 0 - 100 with 0 - 1 indicating the lowest amount of backlinks and 100 the heighest
    - Spam Score
      - Trust Flow and Citation Flow scores can be used to calculate a site Spam Score
      - This metric balances the number of a backlinks a site has as well as well as how trustworthy the linking domains are
      - Spam scores is the trust score divided by the citation score
      - Spam score should be 1 or higher
      - Spam scores below 1 could indicate an unbalanced link profile
        - This could be from too many poor quality links
        - https://searcharoo.com/why-low-quality-links-will-tank-your-website/   
      - Use spam scores with competitor sites too
  
Analysing Backlink Profile using Majestic
  -
  - Demo of the Majestic Tool to find citation scores, flow scores and spam scores of sites
          
Competitor backlink profile template
  -
  - Downloadable text file template
          
Performing a keyword gap analysis using SEMRush
  - 
  - Demo using the SEMRush tool
  - A keyword gap occurs when competitors rank for keywords that we don't
  - It can also be when competitors rank higher for certain keyword than we do       
  - https://back2marketingschool.com/semrush-keyword-gap-analysis/          
          
Keyword gap analysis template          
  -
  - Downloadable text file template          
  
How to spot key opportunities
  -
  - Analysing the data - what do we have
    - This reuses the example data from the previous lesson          
    - We have a single and comprehensive set of keywords that we and competitors are ranking for
    - A list of gap keywords that we would aim to rank for
    - The URL's and the positions for each of these keywords  
    - A rank order of ours and our competitors domains in terms of quantity and quality of backlinks
    - A collated list of all backlinks to our website and competitors websites          
  - Gap keywords - Irrelevant
    - It is important to remember that not all keywords will be relevant to a company
      - Discounted keywords will need to be manually removed from consideration
  - Backlinks
    - Any data collected will form the basis of a future backlink strategy 
    - Based on a spam score you will be able to see who are the main players and also the least competitive ones
    - Focus should only be on the strongest competitors and their domains
      - Look at the domains that their links are coming from
      - What is the content type that has attracted the link
    - Disavow poor quality links on your website where needed
  - Calculating the size of the prize
    - The size of the prize refers to the amount of search volume opportunity available
      - Content would need to be optimised poperly and a ranking of number 5 for each relevant query in this calculation           
    - This is based on numerous large scale studies
    - Ranking in position 5 would translate approximately to a 5% CTR
    - This means that 5% of the average monthly search volume would be for gap keywords          
  - Calculating the size of the prize - Caution
    - It must be kept in mind that CTR can vary immensely based on a number of factors and are indicative rather than guaranteed
    - Rankings and visibility with SERPs fluctute contstantly
    - SERP features such as paid ads and featured snippets can influence CTR by pushing pages further down the SERPs
    - Some industries and therefore their keywords have higher CTR than others (the same is true of lower)
  - Organic vs Paid Opportunity
    - A good way to get people invested in SEO initiative is to work out the cost for traffic grabbed by paid ads
    - What would be the cose of getting the traffic highlighted by GAP if it was being paid for
    - Again use the 5% of the search volume opportunity and multiply it by Cost Per Click (CPC)
    - The same cautions must be applied to CPC as to CTR          
  - Prioritisation
    - Quick wins here would be keywords in middle positions that coould do better   
    - Also create a content strategy to rank for these gap keywords          

Task Assignment 1 - Perform a backlink profile analysis
  -
  - Take 5 competitor website and download their historic data from Majestic
  - Use the template provided and create a table filled with trust, citation and spam scores 
          
Task Assignment 2 - Create a keyword gap analysis 
  -
  - Using the same competitors as the above exercise, create a full Keyword Gap Analysis
  - Again make use of the template provided          
          
Task Assignment 3 - Estimate the size of the 'Prize'
  -
  - Work out the size of the prize for all Gap keywords

          
<br /> <br /> <br />

<h1><p align=center>SEO Common Pitfalls </h1><br/>
          
Manual Actions
  -
  - These occur when there are breaches of Google guidelines
    - They do not occur from algorithms but are the result of people reviewing the site
    - The same goes for penalties as there is a difference between manual actions and SERP ranking penalties
      - Algorithmic penalties do not directly punish a site but instead favour competitors who better meet the algorithm targets
      - Manual Actions on the other hand are punishments for specific breaches of guidelines that have a specific cause          
    - There are several different types of Manual Actions and they have varying consequences
    - Manual Actions are rarely found by accident and are usually the result of trying to manipulate or harm users and search engines 
      - They can occasionally be from icompetence where warnings have been ignored          
  - Submitting Manual Action Fixes
    - Once any issues identified have been fixed then it is time to submit a request to Google to review fixes
    - This request should contain details of the exact issue, steps taken to amend the issue and steps taken to prevent recurrence as well as outcomes of changes           
  - Types of Manual Actions
    - User Generated Spam
      - This is guideline breaking content posted by external people posting to a site
      - Although this content is not under your control the ability to manage it and moderate it is and failure to do so will cause problems
      - If it is possible Google will take action on the affected pages
      - If the issue is present on large parts of the site it could be entirely deindexed by Google
      - A lot of User Generated Spam can be handled by improving moderation methods
      - Some methods include limiting post numbers within a certain timeframe or reviewing certain user's posts
      - Spammers often post links in comments or profiles and they should be nofollowed
      - Using nofollow ensures that there is no link equity passed on to these links          
    - Spammy Free Host
      - Web hosting services should be aware of the sites that use this service
      - Google will punish if there are things like spam or even worse malicious malware
      - Moderation will again take care of a lot of this issue
      - Make sure to use captcha to stop automated account creation
      - Log files should be monitored for any increase in redirects which may indicate cloaking techniques being used
      - Use the Google Safe Browsing API on a regular basis to test URL's using the web service
        - https://developers.google.com/safe-browsing
    - Structured Data Issues
      - If Structured Data has been use to manipulate Google Rich Results then there will be a penalty
      - Usually it is intentional manipulation that is punished but continued failure to meet Structured Data guidlines can be punished if there are ignored warnings
      - If there is a manual action for this a site can face deindexing but usually the site will be prevented from generating Rich Results
      - The simple fix here is to either amend or remove the structured data that is causing the issue         
    - Unnatural Links to a site
      - These type of links usually come from buying as part of a scheme from a spammy domain
      - They can be either a site owner or agency acting on their behalf
      - There are cses where it was competitors trying to cause issues for another site
      - Google will always penalise these if they are trying to manipulate PageRank
      - If there are any on a site it is recommended to remove them even contcting webmasters if necessary
      - Disavowing those demains may also be needed          
    - Unnatural Links from a site
      - Google will hit a site with penalties if it thinks that site links out are trying to manipulate rankings
      - Penalties are especially likely if there are signs that links were sold as part of a linking scheme
      - Identify any links on a site that break Google guidelines
        - This will included any links which may not be malicious but would be unnatural
      - Remove any links from any manipulative scheme
      - Where a manual action comes because of an unintentional spammy linking it is recommended to change them so they will no longer pass PageRank
        - This can be done with a nofollow attribute or else make them inaccessible to bots
    - Thin content with little or no added value
      - The Panda algorithm introduced penalising for thin content
        - Google will devaulue an entire site if they feel the issue is widespread enough         
      - Spammy like practices such as scraping or designing pages to rank for terms can be problematic
        - Especially if they draw in users and funnel them to unrelated parts of the site, these are called doorway pages
        - https://searchengineland.com/doorway-pages-seo-deep-dive-389786
        - https://developers.google.com/search/docs/essentials/spam-policies#doorways
      - Removing scraped content and doorway pages is recommended
      - Review content as often as possible and ensure that it offers value to users
    - Cloaking and Sneaky Redirects
      - Cloaking is where a site shows different content to users than to bots
        - Sneaky Redirects take users to pages hidden from bots    
      - Both are against google guidelines as the intention is to deceive the search bots
        - They both are methods of preventing appropriately assessing page content
      - To avoid cloaking issues assess how the site treats bots
      - Use Screaming Frog to spoof bots and check for on-page differences
      - Live tesing an inspected URL in GSC can shpw hpw Google sees the page          
    - Hidden Text and Keyword Stuffing
      - Keyword stuffing is the practice of using keywords so often in text that it seems unnatural
        - https://contentwriters.com/blog/keyword-stuffing-avoid/          
      - This is done to explicitly manipulate search engines and improve SERP rankings
      - One way to do this is to set the colour of text to match the background
        - This renders the text almost invisible upon viewing    
      - It is also possible to hide text by using CSS or set set font-size to 0
        - This is known as hidden text          
      - Another example of hidden text is hiding links on a small character without context
      - Keyword stuffing will also occur in unnatural prose
      - Removing keyword blocks from unnatural prose is recommended as search no longer works this way
    - AMP Content Mismatch
      - AMP or Accelerated Mobile pages are specifically designed to function better on mobile devices
      - Content should be similar to non-mobile pages but the source code is going to be different
      - If there is a large difference in the content served on both sites then Google will penalise
      - If there is a penalisation then review AMP pages and the non AMP counterparts
        - AMP pages should be amended to better reflect content
    - Sneaky Mobile Redirects
      - Mobile redirect are occasionally used to redirect users to a more appropriate service such as the mobile site version
        - This can be using a domain beginning with m. to indicate a mobile subdomain
      - There can be issues though with sneaky redirects which redirect users to possibly dangerous content
        - These specifically target mobile users and are often caused by malicious code
      - Use the security issues section in GSC to check for any problems
        - Another way is to use SEO Spider with user-agent set to Googlebot
        - This crawl will pick up and resources used and any redirects present which allow for assessment          
      - Manual spot checks should be done using mobile device emulation in Devtools
      - Any sneaky redirects should be removed, removal of third party code in it's entirety is recommended          
    - Pure Spam
      - Pure Spam is any intentional combination of the above techniques
      - Usually a site will receive a manual action when there is not doubt that it is deliberately engaging in practices against Google guidelines
      - If hit with a Pure Spam manual action the it is best to audit all of the above techniques and resolve quickly          
  - Impact of Manual Actions
    - Impacts of manual actions will depend on how severe the guideline breaches are
    - If it is only a few pages then it is likely only those pages that will be penalised
    - If however the breach is across an entire site then the entire domain will be penalised and maybe deindexed
    - A manual action may not be a big deal in some cases eg Schema Manipulation which results in Rich Results being hidden rather than deindexing
          
Google Core Updates
  -
  - Panda
    - This algorithm targets thin or low quality content
    - It is a common pitfall for sites to have content that is targeted by this algorithm
      - Sites with no results pages, this occurs when searching on a site and nothing appears but the page is indexed
      - Low word count articles which are usually sub 100 words
      - Indexable tag pages where blog articles are tagged with a specific tag that could be deemed as low quality content 
      - Low quality search results pages, this can include mass duplication of results
    - These issues are easy to fix as they only need addition of content that is better quality
    - Although noindexing pages can be an option, improving content is always the better option          
  - Penguin
    - This targets link schemes, unnatural links and keyword stuffed pages
    - This is still a common pitfall for sites but not as common as Panda
    - It is easy to spot if Penguin has affected a site by using Google Analytics
      - Ensure that seaonsality is filtered out and check over all linking practices          
    - It there are practices that could be deemed spammy then traffic drops can be attributed to Penguin  
    - Also any content that is seen to be over optimised to the point of being spammy will be targeted by Penguin  
    - The immediate action to take after discovering Penguin penalties is to stop spammy linking practices
    - In addition a disavow file which can be uploaded to Google informing them of very bad links will help penalty recovery
      - https://www.semrush.com/blog/how-to-disavow/
      - https://www.google.com/webmasters/tools/disavow-links-main          
    - Removal of spammy links can help prevent them being linked with a site
    - A review of site content is also good to ensure no over optimisation         
  - Intrusive Interstitial Penalty
    - This is a penalty focuse on the mobile version of site
      - https://www.searchenginejournal.com/google-intrusive-interstitials/375923/  
    - If an intrusive interstitial is used on the first page of results there will likely be a penalty from Google    
    - An intrusive interstitial can include the following
      - A pop-up, a stand alone interstitial or another larger type of standalone        
    - There are some intersitials that would not be affected
      - There are interstitials which highlight cookie usage
      - Where a user needs to enter an age into an interstitial prior to accessing a site
      - A banner that uses a reasonable amount of space 
        - What is considered a reasonable amount of space can be debated
        - It should not really take up much more than 10% of a page
    - Resolving this issue is fairly easy and easy to spot when auditing         
  - Further Updates
    - Google update their algorithm many times a year
    - Most of these updates are smaller and are mainly iterations on the existing algorithm adding minor tweaks  
    - There are some larger updates some of which have codenames in the industry   
      - They are usually discussed at length on SEO twitter
    - There are some very good sources of information
      - Barry Schwartz - https://searchengineland.com/author/barry-schwartz  
      - The official Google Webmaster blog is also a recommended source of information
    - All SEO's shoould keep up to date with the latest happenings
      - This will include being aware of the latest updates and how that affects websites          
          
Using IFrames and their impact on SEO
  -
  - What are iFrames
    - Inline Frames or iFrames are a way to embed HTML within a webpage
      - https://www.w3schools.com/tags/tag_iframe.ASP
      - https://developer.mozilla.org/en-US/docs/Web/HTML/Element/iframe
    - If an iframe is visible on a page any user interaction will occur separate from the page including scrolling         
  - Use Cases
    - Embed Visible on-page elements
      - Iframes are an easy way to embed content from another page, this is often as widgets
      - There are multiple content types that are allowed such as maps, videos and PDF files
      - This content can be viewed without leaving the page that they are on
      - Google Adsense can be used in iframes but only with express permission from Google
    - Other Uses 
      - Iframes are often used with noscript tags to enable external code when JS has been disabled          
  - Effect on SEO
    - Iframes were frowned upon by SEO's since they could confuse bots
      - https://searchengineland.com/how-googlebot-handles-iframes-388243
      - https://www.searchenginejournal.com/google-iframes-debunking-myths-understanding-seo-impact/484037/
      - The bots would either not see the content or would crawl iframe content and not be able to return to the host          
    - These are not issues anymore as Google has improved significantly how it handles iframes
    - Google does state in Rich Media Guidelines however that iframe content may not be indexed
    - Any content indexed will be attributed to source rather than host
    - Avoid relying on iframes to deliver content to users
    - Iframes can impact page speed when pulling content from an external location
      - This is because there is a limit imposed by how fast the source domain is
      - If there is an issue it can prevent the onload event working
      - The onload event happens when a page has loaded and is used by browsers to indicate when to stop loading          
  - Blackhat practices and misconceptions
    - Iframes have come to be associated with blackhat practices due to past abuses
      - https://blogs.halodoc.io/iframe-security-threats-and-the-prevention/          
    - One of the abuses was from when sites would use iframes for Google Adsense
      - An iframe could be hidden if wrapped in a div tag and it would still receive an impression
      - This was because Google was not able to check whether it had been hidden
      - This is the reason Google banned them unless explicitly authorised          
    - Iframes have also been used to attack users using mehtods like clickjacking
      - https://owasp.org/www-community/attacks/Clickjacking
      - https://portswigger.net/web-security/clickjacking          
      - In this attack hidden iframes are used to overlay innocent looking links
      - The iframe that receives the link causes the user to download malware
      - It is difficult to implement them as they need access to source of the site hosting the iframe so they aren't seen as much anymore
      - Some of the reduction in these attacks is also a result of search engines removing them from their pages
      - These attack are also prevented by browsers and are usually only seen from legitimate sites being hacked          
    - There are still situations where iframes are involved in malicious attacks
      - Because of how iframes operate they can be used for phishing attacks
      - https://www.theregister.com/2022/03/18/browser_in_browser_phishing/
      - https://blog.bitsrc.io/4-security-concerns-with-iframes-every-web-developer-should-know-24c73e6a33e4
      - This attack trick users into entering valuable information on to a site without knowing that they are on it
      - Because the attack takes place on an external site there is very little that a host can do about it
      - It is very important to ensure that iframe content comes from trusted sources
      - Use as few iframes as possible and regularly audit the content           
    - Malicious Pop-ups are another thing that can be attributed to iframes
      - An iframe can post a pop-up to a new window if it is not properly handled
      - These pop-ups can execute JS and could be dangerous to users          
    - Untrusted iframe sources can be sandboxed using the sandbox attribute
      ```
          sandbox="allow-popups"
      ```
      - This attribute sets restrictions on the iframe content
      - It prevents them from executing scripts, using plugins and pushing pop-ups
      - If there are certain functionalities needed then they can be allowed as seen above
    - There are some misconceptions around iframes
      - Some in SEO view iframes as a form of cloaking
      - This isn't strictly true as iframes reference the source URL clearly and can be read by bots which cloaking does not allow          
      - The overall view of iframes is damaged by malicious practices and misconceptions
      - Iframes are the best solution in some areas
      - If used properly iframes will not result in manual action
      - The fundamental concern about iframes is whether or not their content can be read by bots          
  - Considerations when using iframes
    - Iframes are considered as links to the content that they show
    - If an iframe is pulling content from external sources then there is little control over what users are shown     
    - If the content is changed it maybe something that a site does not want it's users to see
    - It is important that sites secure themselves as much as possible because iframes can pose risks
    - Content within iframes will not be accredited to the host page
    - If the content is not needed to be accredited then iframes maybe a good choice
    - Although content may not be accredited association with a site can still happen aas they are considered as links
    - There are some questions to be asked when impementing iframes
      - Does using this pose a security risk to the site
      - Is there a need for the content to be accredited to this page
      - Is the iframe linking to some that is better for a site not to be associated with          
    - If the answer to the 3 questions above is no then iframes are a good choice
    - There are alternatives if security is an issue such as sandboxing
    - If content accreditation is needed then include it within the source code or dynamically render using JS
    - If the iframe is showing stuff you do not want to be associated with then like a spammy domain remove links and rethink strategy          
          
Canonical Tags
  -
  - Again all Technical SEO Material including Canonical Tags can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md
  - What are Canonical Tags
    - Canonical Tags indicate that a page is the master version
    - If a page is the master then the tag can be self-referential
    - A page that is a duplicate or similar to the maseter should have a tag pointing to the master
    - Indicating master pages to search engines allows the version you want to be indexed          
    - By canonicalising elsewhere, it tells search engine that there is another page version better suited to indexing
    - Canonical tags pass link equity to the page that they reference
    - This can make them very useful when used on parameterised which link from external sites
    - This is also important when there are multiple paramterised versions of a page all of which will pass link equity on to the master version
  - Where should you use canonical tags
    - A canonical tag should be added in the head section of a page
    - Good practice involves implementing canonical tags on all pages
    - Master versions of pages should have self-referential tags
    - Duplicate or near-duplicate pages should canonicalise to the master version
    - Cross Domain Canonical Tags
      - A cross domain canonical tag is one which points to an external domain
      - They function exactly the same as a normal tag
      - Search-engines will possibly ignore cross domain canonicals id the canonicalised page has more authority          
  - Why are duplicate pages and issue
    - Canonical tags help deal with issues form duplicate pages
    - What issues duplicate pages cause for a site is not always clear
    - Keyword Cannibalisation
      - This is an issue which happens when pages on the same domain fight to rank for the same query
      - If it is important for both pages to be accessible then canonical tags help as the link equity will be passed to the master
      - This will eliminate or reduce the issue of split suthority
      - https://www.searchenginejournal.com/on-page-seo/keyword-cannibalization/          
  - Canonical tag problems 
    - Incorrect canonical tag usage
      - It is very easy for canonical tags to be misused
      - Canonical tags must always point to the master version, they will be invalid if they point to a non-indexable page
      - A further misuse is using paginated series where page 2 onwards canonicalise down to page 1 of the series
      - Pages in a series should stand on their own and use a self-referential canonical          
    - Non-optimal canonical tags
      - Some sites allow canonical tags to be dynamically updated by JS
      - This is allowed but will cause problems if the canonical tag in the source code differs
      - Due to search engines not seeing the fully rendered page the dynamically tag might be ignored
    - Canonical and redirect chains  
      - Canonicalising to a URL which then canonicalises elsewhere will mix the signlas sent to the search engines
      - They may follow the chain and may understand the effect, mixing the signals reduces adherence to canonicals          
    - Canonicalise pages in sitemaps
      - Canonicalised page being found in sitemaps is not an uncommon occurence
      - Again this has the potential to cause confusion for search engines
      - If a sitemap has canonicalised URL's then they should be replaced by the master versions          
    - Mixing canonical and noindex tags
      - Implementing both canonical and noindex tags on a page is not recommended
      - They are completely contradictory abou the value of a page          
    - The problem with canonical tags
      - Canonical tags are great for informing search engines of the relationship between master and duplicate URL's
      - The problem lies in the fact that they are only a suggestion and not an explicit directive          
  - Considerations
    - Canonicals are a great tool to have but there are occasions when other options are better
    - Canonical tag usage is both subjective and situational          
    - Would a redirect be better
      - Canonical tags do pass authority but there are occasions where a redirect may be better
      - One example is having both a www and a non www version of a domain
      - Some sites use canonical tags to handle this situation
      - This is not the recommended solution as searchengines can ignore the tags if other signals contradict them
      - A good rule of thumb is to think whether or not a page needs to be accessible
      - If it does then use a canonical, if not then use a redirect          
    - How important is keeping the page out of the index
      - If it is vital to keep a page from being indexed then a noindex tag may be better
      - This is because noindex is a directive which must be adhered to but canonical is a suggestion          
    - Is the page actually a duplicate
      - Sometimes canonicals can be used with near duplicates but this method must be used with caution
      - If search engines feel there is too much of a difference then the canonical will be ignored
      - If this is a widespread issue then it might damage search engine trust in a site's canonicals          

Keyword Cannibalisation
  -
  - Keyword Cannibalisation is where multiple pages target the same keywords
    - This results in pages fighting each for SERP rankings
    - Sometimes the authority of all pages involved is reduced          
   - Split Authority
    - Having multiple pages targeting the same keywords can cause internal linking to become confused
      - This can be especially true of horizontal linking          
    - This is because internal anchor text might match multiple pages but can only link to one
    - External linking can also be a problem when multiple external sites link to multiple pages on the same topic
    - This causes link equity to be spread across multiple pages instead of consolidated at one point
      - This will in turn cause all pages that target the keyword to have reduced authority          
   - A less useful page could rank higher
    - If there are issues determinining page authority and keywords are being targeted by multiple pages, less useful pages could rank higher
    - The bottom line is that the more keywords are cannibalised the less control there is over what the SERPs show          
  - Types of Keyword Cannibalisation
    - Duplicate Content
      - Sites suffering from duplicate content if not handled properly can cause keyword cannibalisation issues          
    - Canonical Tags
      - Sometimes sites have versions of pages that are similar to a master version
        - They must return a 200 status code for users (One example is campaign landing pages)          
      - Cannonicalisation will help reduce duplicate content and keyword cannibalisation issues 
        - Some of the issues seen are the result of user focused pages
        - These will include unhandled tracking parameters or campaign landing pages          
      - Canonicalisation of these pages means still returning 200 status codes
        - However link equity will still be passed to the master version          
      - Canonical tags should be used to keep these pages out of the index
        - Again it should be remembered that these tags are suggestions and not explicit directives so may be ignored by search engines
    - 301 Redirects
      - Sometimes sites can amass large amounts of content which target the same keywords without realising it
      - This often happens when a site features multiple articles around the same topic
        - Usually there are different authors that do not realise that they are repeating content
      - These pages often have amassed internal and external backlinks
        - This will mean that page authority is split between them
      - The pages involved should be consolidated to one stronger page
        - This will lead to higher ranking than a single page which has been suffering from cannibalisation
      - If the page does not need to return a 200 code then a 301 redirect is more preferable than a canonical tag
        - 301 codes are explicit and therefore more definitive than canonical tags
      - In order to proceed with consolidation, first identify the strongest page and then designate this as the master
      - This page will need to be updated to contain any content from cannibalised pages.
      - The cannibalised pages should then be 301 redirected to the master page          
    - Search Intent
      - Sometimes it can be necessary for cannibalised pages to be kept indexable
        - In this case content should be altered so that different pages match different intent
        - Long tail queries should also be targeted
    - Metadata
      - Cannibalisation can occur on pages that have meta titles targeting the same keywords
        - These are also known as title tags or page titles
      - Meta titles are a search engine ranking factor used by search engines to get context on what the page content is
      - Cannibalisation occurs in this instance when keywords are too general
      - Meta descriptions can also be affected by keyword cannibalisation
      - Meta descriptions are not a ranking factor but have the ability to affect CTR
        - This is because they give a user an understand of what the page content is          
      - Cannibalised meta descriptions can look spammy and repetitive which can also affect CTR
      - This issue can be eased or prevented by having metadata that is much more specific to the pages being represented
      - Doing this makes it easier for search engines and bots to differentiate between pages
      - It will also reduce competition for keywords on less specific pages
      - Metadata can sometimes uncover other keyword cannibalisation issues          
  - Keyword Cannibalisation Considerations
    - Cannibalisation Issues May Not Always Be Seen
      - When auditing for keyword cannibalisation don't just check in the body section, check the head too
      - Doing this ensures that metadata is not contributing to cannibaliation issues          
    - Search Intent
      - When creating content it is important to consider the search intent that it targets
      - If 2 pages target the same search queries there will usually be cannibalisation issues
      - If 2 pages offer the same experience to users, consider consolidation to ensure that authority is strong          
    - Is it Important To Keep Content Accessible
      - If content must be kept accessible then consolidation is not an option
      - It can be either cannonicalised to an appropriate location or updated to different queries          
  - Avoiding Cannibalisation in the Future
    - Know Your Pages
      - Always know what is already on the site when commissioning content
      - This includes know the keywords that pages and content target          
    - Keep Duplicate Content in Check
      - When creating new pages consider if they duplicate others on the site
      - This is especially true of user-focused pages like campaign landing pages
      - If the pages are duplicate then ensure proper canonicalisation          
    - Crawl Regularly
      - It is important to crawl the site regularly to ensure that duplicate metadata is not causing a cannibalisation issue
      - When duplicate metadata is found, it should be treated as a potential issue and investigated          
          
Keyword Stuffing
  -
  - What is Keyword Stuffing
    - Keyword Stuffing is writing keywords so often in a piece of text that it seems unnatural
    - It is used to manipulate search engines and improve rankings for queries containing the keywords          
  - What Keyword Stuffing Looks Like
    - Unnatural Prose
      - Text is considered to be unnatural prose if it contains enough keywords to make it feel clunky and unnatural
      - Search engines do not judge the style of writing but look out for manipulative ways of using keywords which are detrimental to users          
    - Keyword Blocks
      - There are some cases where sites do not attempt to use prose to disguise keyword stuffing
      - In these cases the sites simply list keywords they are targeting          
    - Hidden Keyword Stuffing
      - This is another method of stuffing, here CSS is usually used to make text the same colour as the background
      - It is often used in cases where there keyword blocks being hidden
      - Keyword blocks do look spammy whereas unnatural prose looks normal until it is actually read
  - Hazards Of Keyword Stuffing
    - Penalty
      - Keyword stuffing is against Google guidelines and may result in manual actions against a site    
      - This can mean pages beig ranked lower or even being deindexed          
    - Lower Rankings
      - As Google has improved search the view on keyword stuffing continues to decline
      - Keyword stuffing will affect site rankings even without manual actions being taken
      - Google has a focus on user intent which causes an issue for sites engaged in this practice
      - Keyword stuffing can actually have the opposite effect on site ranking as it does not meet the intent behind user queries          
    - User Experience          
  - How To Identify Keyword Stuffing
    - Spot Checking
      - If a sentence has so many keywords that it is difficult to read then there is usually a stuffing issue
      - Keyword blocks whether hidden or not can be found using Devtools
      - Check key areas of content for this to keep this issue from existing and if there are any maybe areas then further investigate
    - Custom Search
      - Screaming Frog can be used to check for keyword stuffing issues using customs earches
  - How to Fix Keyword Stuffing
    - User Experience is the key
    - It is critical that any keyword stuffing found on a site is removed
      - Any content should at least be amended so that it no longer appears as spam
    - The goal of amending content is to improve user experience
    - Writing for users rather than search enginesusually means more natural and flowing language
    - Keyword usage is fine as long as it does not affect the structure of the text          

Faceted Navigation
  - Again all Technical SEO Material including Faceted Navigation can be found at
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md
  - What is Faceted Navigation
    - Faceted navigation is a term that is used to describe search options that are usually found on e-commerce sites
    - Filters which are usually categories are used to organise search results based on product attributes
    - If done correctly facted navigation works as a great source of internal linking
    - These pages target long-tail queries with potentially high volume
  - Potential Issues With Faceted Navigation 
    - Thin Content
      - It is common to have thin content issues with faceted navigation
      - This is because bots can easily reach pages with 0 results of little content
      - The issue is most often caused by URL's that have been parameterised with lots of filters
        - Another way is having categories with very few products          
      - Applying meta robots noindex tag is the recommended way to handle a URL with a fairly large amount of filters
      - A good rule is that URL's with more than 3 filters should be noindexed
      - This will depend on the amount of products available in each category          
    - Duplicate Content
      - URL structure will depend on the order of filters being applied
      - This can be problematic as it means a site can have duplicate pages for URL's with filters in a different order
      - The best way to tackle this is to create a rule so that filters are applied in a specific order
      - A large amount of thin or no results pages can also result in duplicate content
      - Having a large number of thin and duplicate pages can cause index bloat
        - https://www.greenlanemarketing.com/resources/articles/how-to-find-and-fix-index-bloat-issues/
        - Index bloat occurs when there are unnecessary pages being indexed
        - This became an issue after Google introduced the Panda algorithm
      - A large amount of low quality pages can negatively affect the whole domain
    - Crawl Budget
      - Even if duplicate pages and thin content are handled correctly it can mean bots crawling a large amount of non-beneficial pages
      - This is because the noindex tag does not prevent a page being crawled
      - The idea is that bots should be controlled on how they crawl
      - Using robots.txt is the usual method but this can prevent a noindex tag from being seen
      - The problem of index bloat would then be a concern
      - Utilising nofollow tags on lower value pages is the recommended solution
      - Nofollow tags devalue links and make bot crawling less likely
      - Again it should be remembered that this is a hint not a directive
    - Blocking Crawling With robots.txt
      - Pages with facets and filters are usually blocked by robots.txt
      - This is done to avoid issues with thin content, duplicate content and bloat issues
      - This can make it difficult to leverage filtered pages because the content will not be seen by bots
      - It is recommended that faceted navigation is not blocked by robots.tx
    - Crawlability
      - Some sites do not use hrefs within the facets and filters menu
      - Bots will usually only only crawl links in an a tag with a href attribute
        - This means that this implementation is unlikely to be crawled          
      - Preventing bots from crawling filtered pages makes it tough to use them when targeting potentially high volume long-tail search queries
      - It is recommended that URL's that target high volume queries are crawlable          
    - Canonicalised pages and noindexed Pages
      - Parameterised pages are usually with canonicalised to sub-category pages or have noindex tags
      - This is an attempt to keep indexable URL's clean or if there are resource constraints
      - Canonical and noindex tags are often needed when using faceted navigation
      - They should only be used when necessary and do not need to be applied to all facted navigation URL's          
    - Relevancy
      - It is not unusual for faceted navigation to return search results unrelated to filters
        - This is usually caused by poor categorisation          
      - This is not just a poor user experience, it also negatively impacts how Google sees a site
        - This is because Google ranks based on relevance to user queries
      - The less consistent filters are, the less likely they will produce results relevant to queries which affects ranking
    - Unblocking Everything At Once
      - If a site has used either robots.txt or uncrawlable URL's to block crawling then unblocking all at once can cause issues
      - This is because if Google discovers a lot of potentially low quality URL's simultaneously there could be negative impacts for a site
      - A measured approach is the recommended strategy, open faceted navigation to crawling one parameter at a time
      - Any issues found are limited to smaller sections of the site so there is more control over potential issues
      - Any issues that have been identified should be fixed pirior to unblocking so that Google sees only good quality content          
          
No Results Pages
  -
  - When No Results Pages Occur
    - No Results pages often occur when users add too many filters to an internal search
      - They will also occur when there are simply no matching results
    - Bots are usualy only able to reach links in an a tag.
    - This means that they will not usually use a site's internal search
    - There is still a chance though that a URL with search parameters can be linked from somewhere
    - This is why these type of pages should always be handled carefully          
  - Why No Results Pages Are An Issue
    - Thin Content
      - No Results pages offer little value to the user and are also considered very thin
      - This is an issue due to the algorithm that Google uses to target sites with thin content
      - A large number of thin pages will bloat the Google index and negatively affect a domain
      - No Results pages should leverage useful links to funnel users back into the site
      - With the links the pages will still not be contentful but at least useful
      - No Results pages should as good practice be noindexed          
    - Duplicate Content
      - Because they are low content No Results pages are likely to be seen as duplicates
      - A site can be impacted by these as due to the Panda algorithm it can be seen as low value          
    - Soft 404's
      - Bots often see No Results pages as soft 404 pages
        - These are pages that look similar to 404 not found pages but return 200 status codes    
      - This does cause a problem becuase ordinarily you might look at letting these pages return a 404 code
        - This could lead to a number of 404 pages being internally linked which can damage site trust        
      - Bots are also less likely to return to a page after seeing a 404
      - This is an issue if product levels are fluctuating
      - A page may regain product levels but not be crawled because of encountering a 404 previously          
  - How To Handle No Results Pages
    - Noindex
      - As discussed in the faceted navigation section certain pages should contain a meta robots noindex tag
        - These pages are ones with a large amount of filters or facets or pages with a small amount of results
      - Pages should be accessible to users but nonindexable which will prevent index bloat
        - This improves domain value and avoids Google penalties
      - Bots might not see a noindex tag if internal search URL's are blocked in robots.txt
      - Bots usually will not crawl internal search so crawl budget is likely not to be an issue
      - Its better to keep pages from the index than blocking in robots.txt
      - Unless there is a large crawl budget issue any directive preventing internal search being crawled should be removed  
    - User Experience
      - As mentioned previously no results pages can be used to help users continue their journey through a site
      - For certain types of no results pages it can be beneficial to showcase a selection of other popular products
      - Another method is to show alternative items based on the users previous browsing choices
      - No reults pages can also offer a "Did you mean" section containing similarly spelled items
      - No matter what option is chosen make sure that the user experience is the best it can get

Crawling and Indexing Directives
  -
  - Robots.txt
    - What robots.txt is
      - robots.txt is a plain text file implemented at the root of a site
      - It contains directives to instruct bots on what to crawl and what not to crawl
      - Because it tells crawlers what to crawl it is valuable in conserving crawl budget
      - By ensuring that not all pages are crawled bots can be directed to crawl high value parts of a site
    - What should robots.txt contain
      - The robots.txt file should specify the user agent that is intended to be targeted by the instruction
      - If the directives are for all bots then a wild card can be used
      ```
        User-agent:*    
      ```
      - If directives were specifically meant for Google then it would look like
      ```
        User-Agent:Googlebot    
      ```
      - The robots.txt file can contain instructions for bots on what parts of the site they should not crawl like below
      ```
        User-agent:*
        Disallow:/wp-admin/
      ```
      - It does not have to contain anything and if it is blank bots assume all crawling is allowed
      - Bots will also assume everything is allowed if the users agent is declared but no directives are specified
      ```
        User-agent:*
        Disallow:
      ```          
      - Robots.txt can also be used to point to a sitemap or sitemap index
      - This is a recommended practice as it helps to ensure sitemaps are seen by bots without relying on a webmaster setup
      - Absolute sitemap URL's can be referenced by using the following at the bottom of the file
      ```
        sitemap:https://example.com/sitemap.xml    
      ```
  - Common robots.txt Mistakes
    - Blocking Access To Important Resources
      - Having the ability to block crawling of less valuable pages is a useful one to have
      - It is important though to ensure that valuable content is not being blocked as well
      - Sites sometimes block important resources with robots.txt
      - This can be checked by using Live Testing in GSC.
      - Any resources that fail to load will have the message 'Googlebot blocked by robots.txt'
    - Disallow All
      - Occasionally sites will block the entirety of the site from being crawled by using the following in robots.txt
      ```
          User-agent:*
          Disallow:/
      ```
      - This prevents crawling on any URL containing a '/' which is all of them
      - This is usually done accidentally either as a misunderstanding of how robots.tx is treated or the file is copied from a staging environment
      - Always check the robots.txt for this mistake when auditing because it prevents other SEO changes from being seen          
    - UTF-8 BOM
      - A UTF-8 Byte Order Mark (BOM) is an invisible character which is prepended to the start of a file at the time of encoding
        - https://www.gsqi.com/marketing-blog/utf-8-bom-robots-txt/
      - If robots.txt has been encoded with the BOM enabled then it will be read by search engines and the line it is on will cause an error
      - Using the robots.txt Testing Tool in GSC will show the error
      - If there is a UTF-8 BOM error then resubmit the file without BOM encoding
    - Robots.txt Does Not Prevent Indexing
      - When a search engine bot crawls a site it will check URL's against robots.txt to make sure that they are not blocked
      - This is only the case when Google is discovering URL's by spidering a site
      - If Google has reached a URL via an external link then the page will be indexed but not rendered
      - This means that a noindex tag will not be seen
    - Not Implementing robots.txt In The Right Place
      - The robots.txt file must be implemented at the root of the subdomain that it applies to
        - If it implemented on a subdirectory then it is usually ignored by search engines
      - To check that it is implemented in the correct place open the web browser and go to your main domain for example
        ```
          https://example.com/robots.txt  
        ```          
      - This will change slightly if using a subdomain
        ```
          https://subdomain.example.com/robots.txt  
        ```
    - Blocking Malicious Bots
      - Becasue robots.txt is used to tell search engine bots where and where not to crawl
      - Some use it as an attempt to block malicious bots from acessing parts of the site
      - These attempts will fail as malicious bots do not follow robots.txt directives
      - Malicious bots should be handled using a web application firewall which can block malisious activity
    - Using robots.txt To Hide A Staging Site
      - Sites sometimes use robots.txt to hide staging sites from crawlers
      - This does not definitively stop a site from being indexed
      - Other methods such as password protecting should be used to hide staging sites
  - Noindex
    - Effect of a noindex
      - Noindex is a directive instructing bots to keep pages out of the index
      - If this tag is correctly implemented then it should always be adhered to
    - Different types of noindex
      - There are 2 different methods of implementing noindexes
        - Noindex meta tag
          - A noindex meta tag should be implemented in the head section of a page's HTML
          - This allows for a more granular approach to indexing pages
          - Pages generation rules can be implemented to implement tags on a wider basis
          - This tag can be seen by inspecting the page's view source
        - X-Robots noindex in header tag
          - An X-Robot tag is an element of a HTTP response header
          - This is the meta information sent from a web server to a browser responding to a HTTP request
          - An advantage to X-Robots tags is that they can be implemented in not-HTML URL's such as PDF files
    - Why you would use a noindex
      - Sites have pages that although they require a 200 response should not be indexed
        - One example is a form submitted response page
        - These pages are static and are served upon successful form completion
        - Because they are an important part of a user journey they return a 200 response code
      - These pages are thin in content and do not belong in the index
      - Noindexed pages should be excluded from sitemap generation
    - Common noindex mistakes
      - Noindex In robots.txt
        - Google until fairly recently followed noindex directive in robots.txt
        - In 2019 Google announced that Googlebot would no longer obey robots.txt directives in regard to indexing
        - This prectice was never intended to be an official directive and was actually discouraged
      - Noindex On Valuable Pages 
        - It is important to ensure that important pages on a site are not noindexed by mistake
        - In an SEO audit it can help to examine the pages being seen by the crawler as noindexed
        - If you have high quality pages that are still not being ranked then this is an issue which should be investigated
      - Specifying A Bot
        - It is possible to specify which user agent is being targeted with a meta noindex tag
        - This implementation will be ignore by other user agents
        - This is not too much of an issue but should be considered if optimising for multiple search engines
        - There may be a large image bloat in some search engines though
      - Including Noindexed Page In Sitemap
        - Sitemap generation can sometimes include all pages or all pages with a 200 code
        - This will cause noindexed pages to appear in the sitemap
        - This contradicts the purpose of a sitemap which is supposed to index important pages
      - Dynamically Updating A Noindex Tag With JS
        - It is possible to update a meta robots noindex tag using JS
        - This is not recommended though as it probably will not be seen when the bots first crawl the page and so will be ignored
      - Unitentional And Unseen X-Robots noindex 
        - X-Robots noindex tags can be applied globally 
        - This can sometimes result in pages being noindexed accidentally
        - X-Robots noindex tags cannot be seen in the source so are often missed
        - To identify one of these issues the browser plugin AYIMA can be used
          - https://chrome.google.com/webstore/detail/redirect-path/aomidfkchockcldhbkggjokdkkebmdll?hl=en
  - Nofollow
    - What Is Nofollow
      - Nofollow is a value that can be assigned to the rel attribute of a HTML tag like below
        ```
          rel="nofollow"  
        ```
      - This implementation is used to nofollow individual links
      - If all of the links on a page are to be nofollowed then a meta nofollow tag can be added to the head section of a page
      - The purpose of a nofollow is to tell search engines that you have not endorsed where it links to
      - This will prevent PageRank from being passed to the linked pages
      - Search engines will usually not follow a link affected by nofollow but it is not always true
      - Like other things nofollow is treated as a suggestion and not an explicit suggestion
    - Common nofollow Mistakes
      - Nofollowing Everything
        - Some sites implement nofollow on all outbound links
        - This saves time on large sites where each link would have to be checked individually
        - A lot of sites are also afraid of getting manual actions for unnatural linking if linking out to a large number of sites
        - Nofollowing a lot of links may also negatively affect rankings as it stops a site being part of the normal web ecosystem
      - Nofollowing Horizontal Links
        - Sometimes the blanket nofollowing of links is extended to all links within the main body
        - This negatively affect horizontal linking as link equity cannot be passed between your own pages
      - Alternatives To Nofollow          
        - In 2019 Google started supporting 2 new rel attributes 'ugc' and 'sponsored'
        - These are more specific forms of nofollow
        - Google recommends using ugc with user generated content and paid links with sponsored
        - These may be Google specific and may damage Bing rankings
        - Google may prefer these but a traditional nofollow can be used instead
        - Which one a site should use should be done on a case by case basis
          
Link Disavow
  -
  - What Disavowing Backlinks Means
    - Backlinks are links that point from one site to another external one
    - If an external site is linking to your site that link is a backlink
    - Search engines treat backlinks as a recommendation of a site
    - This is unless the link is nofollowed since this tells the search engines that the link is not a recommendation
    - Google has an algorithm called PageRank which is the value of a page depending on how it hits certain parts of the algorithm
    - Link equity is the value passed by a back link unless it is nofollowed
    - The amount of link equity passed on depends on the value of the page they are from as well as link relevancy
    - It can also depend on the position of the link in the page
    - Links in boilerplate content do not hold as much value for search engines
    - Anchor text can also play a part in the value of a link
    - If the anchor text has strong relevance it will be higher value than generic text
    - It is important to differentiate between different lower value links
    - Some may not pass much link equity but will not hurt a site whereas some will be toxic
    - It those links are toxic then they can be disavowed
    - Disavowing a link tells a search engine that you do not want it associated with a site
    - Each search engine has it's own system for disavowing links
  - Why You Should Disavow Links
    - Some SEO's since learning about link equity have tried to exploit it by using underhand methods
    - This includes setting up Private Blog Networks and applying to link farms to manipulate the algorithm
      - https://www.semrush.com/blog/private-blog-network/
      - https://rockcontent.com/blog/link-farming/
    - This worked until search engine began punishing sites with spammy link profiles
    - There are 2 different ways that Google uses to penalise sites with spammy links
      - Algorithmic penalties which come as a result of failing to pass the Penguin algorithm update filter
      - Manual Actions which happen following a human review of a site which discovers guideline breaking issues
    - The algorithmic penalty is not a proper penalty, it just means a site has not hit the right spots in the algorithm
    - Bing has a similar approach to sites with manipulative backlinks
      - Google only punishes intentionally manipulative links whereas Bing punishes both intentionally manipulative and low value backlinks
  - Dangers of Disavowing
    - Disavowing can prevent unnatural linking being penalised by search engines
    - Disavowing links that are lower quality but not unatural can be more harmful than beneficial
    - This is because lower quality sites will not be penalised by search engines
    - While linking is natural link equity will still be passed
    - If a lot of lower quality links are disavowed these small amounts of link equity can add up
    - This is why it is important to conduct a proper backlink audit
  - Dangers Of Not Disavowing
    - As previously mentioned unnatural linking may result in manual actions from Google
      - The can also lead to a site being deindexed, blocked or devalued in Bing
    - If a site has been involved in link schemes then it is important that any unnatural links be disavowed
    - Issues with unnatural linking can be inherited when buying a domain so check
    - The same is true of competitors have involved a site in schemes without them knowing
    - It is important to audit backlinks for any suspected problems
    - Differntiate between then low quality links and the toxic and potential problem causing links
  - How To Disavow Links
    - Auditing For Unnatural Backlinks
      - If doing a backlink audit, use the Majestic SEO to identify the site spam score
      - This means identifying the authority of a site as well as the number of links going to the site
        - These are referred to as Trust Flow and Citation Flow
      - They can then be used to work out the Spam Score for a site, using the formula below
        ```
          Trust Flow / Citation Flow  
        ```
      - The lower than 1 this score is, the more spammy a site has the potential to be
      - All of this gives an initial view of the site and helps determine of a more insive review of links is needed
      - When auditing backlinks then look for any which might be seen as toxic
        - Some of these are from sites that would not likely be associated with a site such as pill or casino sites
      - Private Blog Networks (PBN's) can be recognised as inorganic site structures as either domain or subdomain level
        - These link to each other in an attempt to manipulate search engine rankings
      - If PBN's are suspected of being behind links then duplication can be checked across the network
        - The IP addresses of suspected PBN sites should be checked 
        - This is because a large number of PBN's tend to have the same IP address
      - It is also good to have a look at how similar sites in the network look
        - This is because PBN's often have the same or similar structure and themes to their sites
      - A number of tools can be used to audit unnatural links but 2 are recommended SEMRush and Majestic
        - SEMRush can be used to identify toxic links which can be added to a list to disavow
        - Majestic can be used to investigate links so that poor quality and toxic links can be differentiated
        - One link pre domain should be exported and checked by Majestic
        - There is a risk going to some sites so this may be better done on a virtual machine
        - Alternatively an organisation's security team should be involved
      - Unnatural links audits can be fairly intensive
        - When links appear toxic then add the domain to a list and blanket disavow each domain
        - It is more time worthy to disavow whole domains rather than audit each and every link individually
    - Disallowing In Google
      - To disavow links in Google they should be specified in a .txt file which can be submitted in GSC
      - To disavow on a domain basis which will need to be done if links were audited as recommended
      - Specify the domain using the 'domain:' prefix  as seen below
        ```
          domain:example.com  
        ```          
      - To submit the file in GSC firstly go to the disavow links tool page
      - Select the website property, select disavow links and upload the .txt file containing the links to be disavowed
    - Disallowing In Bing
      - Disavowing in Bing is a bit more manual than in Google
      - There is no uploading a list to the Bing disavow tool
      - Each link must be physically entered into the disavow tool at either page, domain or subdomain level

Task Assignment : Check If Any Pitfalls Apply To Your Site
  -
  - Use some of the listed tools to check your own site to see if there are any of the issues mentioned in the chapter
  - Document any of the issues and ensure that they are fixed as soon as possible          
          
<br /> <br /> <br />

<h1><p align=center>SEO Common Pitfalls </h1><br/>
          
Why Perform A Keyword Universe
  -
  - Creating a keyword universe is a lot of work but should be among the first deliverables
    - https://eu.siteground.com/blog/keyword-research-tips/          
  - Having a properly created keyword universe is essential in putting together a content strategy
  - It combines the keyword gap with a wider range of source
    - This can include industry events, conferences and other similar sources 
    - Competitors beyond what would be considered as being beyond the initial competitor list       
  - The result should be a comprehensive list of keywords, themes and topics to base a content strategy on
    - It will also directly feed into information architecture
  - There are a few factors which will influence the size of a keyword universe
    - What industry the site is in     
    - What are the business expectations or business goals
  - There are usually at least 50,000 keywords
    - This number is fairly small compared to some keyword universe sizes          
  - It is potentially endless and never exhaustive
    - There will always be generation of new ideas for keywords          
  - There should though be an understanding of how much time taken in creation is enough
  - Outputs will vary depending on the type of site
    - For example a large ecommerce firm with faceted navigation will differ from a small sport site          

Connecting Google Search Console To Google Sheets
  -
  - It is important to remember that you are only able to export upto 1000 queries from GSC
  - This will usually be enough to give a good indication what the ranking keywords are  
  - There are some criteria to be met if a site is to rank for something
    - That at least one search takes place using that query in the selected date range
    - The website has content that has been sufficiently optimised to appear within SERP's for that query
  - Depending on the site, it is possible for a site to rank for more than 1000 keywords
  - The search analytics addon for Google Sheets allows for a far larger amount of data
    - https://searchanalyticsforsheets.com/    
    - This has both free and paid options          
  - Search Analytics can only be used for sites that we are verified on    
  - This stops anyone from finding any actual competitor data
  - There is a quick demo on how to use this addon
    - Data from the previous 6 months is recommended in order to give a proper dataset
          
Finding Keywords Using SEMRush And Other Tools
  -
  - Using several tools to generate a keyword universe
  - These tools include SEMRush, Ahrefs and keywordtool.io
  - https://keywordtool.io/           
          
Aggregating Your List Of Keywords
  -
  - Using Google Sheets to generate a keyword universe
  - Different tools use different formats for generating keyword lists
  - Different sheets within the same workbook should be used for each tool when importing keyword data
  - It probably will not be possible to import all data from each source at the same time

Clustering Keyword List 
  -
  - This is a demo lesson using the Keyword Grouper Pro tool
  - https://marketbold.com/KeywordGrouperPro/
  - This can take up to 2 weeks to sort out properly depending on the keyword universe involved          

Tips To Format Your keyword Universe
  -
  - Use colour gradients to help visually help data
    - Use different shades of green for example for volume can show higher volume search keywords            
  - Use multiple tabs to separate data into sections
  - Grey out empty cells can help visually
  - Use graphs where needed as they can give lots of information quickly          
          
Your Keyword Universe Template\Example
  - Downloadable template for building a keyword universe

Task Assignment 1 : Create Your Keyword Universe
  -
  - It is recommended to use the provided template for doing the assignment
          
<br /> <br /> <br />

<h1><p align=center>Creating North Star Goals </h1><br/>

Understand The Business Goal
  -
  - What are the SEO goal at the fron of your mind, do they fit organisational goals
  - At some point SEO's will be asked to define objectives or KPI's for a period
  - No matter the period involved it is very important that SEO goals are aligned with the organisation
  - When goals are used to consider the big picture it makes doing SEO easier          
  - Understand The Business Goal
    - No matter what industry remember what organisational goals SEO can impact
    - Larger businesses tend towards longer goals where small businesses may not formally document them
    - Regardless of their medium it is important to understand the goals
    - Creating SEO strategy is easier when it is known what an organisation wants to achieve
  - Understand the Departmental Goal
    - In many companies SEO's are usually with marketing teams although they can also be with other teams such as analytics
    - Is is likely that different departments will have different goals
    - Although there may be differing goals from one department to another they all still fit together into the overall organisational goals           
  - Using Both Goals For SEO
    - What are good targets for one department such as page speed may not be good goals for another
    - It can help by thinking of the growth goals an organisation is targeting
    - If SEO goals are not specified then business goals and SEO results will be totally independent from each other
          
The North Star Goal (NSG) Concept
  -
  - An SEO focused North Star acts as a focal point that SEO strategy should revolve around
  - It should be looked at as the only goal that matters
  - A Business Aligned NSG Is Critical For Stakeholder Buy-in
    - SEO strategy needs to happen in the proper environment
    - Critical stakeholders need to understand what SEO is contributing to an organisation
    - The types of growth figures that are acceptable differ from company to company
    - Newer companies tend to want visibility whereas older companies might look at engagement levels
    - Irrespective nearly all organisations will want to see how SEO impacts their revenue or sales
  - In Practice What Does An NSG Look Like 
    - This is a single performance indicator which measures the effect of SEO
    - Good SEO performance is often thought of as an increase in organic traffic
    - There are other more specific goals which contribute to this such as improved page speed
    - Increasing traffic is a good thing but it should impact wider goals or stakeholders may not be too interested
    - Only 1 North Star should exist at any time
    - Having multiple NSG's can affect which tasks are the priority which compromises SEO initiatives          
  - Types Of North Stars
    - To set realistic achievable goal firstly understand what the current position is
    - A good North Star is something that SEO will influence over time
    - Traffic-Focused North Star
      - For some site this metric will be entrances, click or unique users
      - Some sites will want to exclude certain query performances
      - Other sites will only want the focus on certain sections of a site
      - If being strict an NSG should show how a organisation is getting authentic value from a site
      - Traffic focused NSG's are sought usually for one of a couple of reasons
        - The SEO team works in collaboration with other teams such as user experience who have their own NSG's
          - These will focus on different metrics such as conversions or sales
          - The role of SEO here is to generate the traffic while other work on how to convert
        - The website involved is currently in a growth phase
          - Newer sites tend to focus on visibility so it can gain traction and improve SERP ranking
    - Conversion-Focused North Star
      - Sometimes a NSG can be created to measure the impact of traffic on conversions and revenue
      - SEO can be a huge way to acquire large numbers of customers
      - Nearly all business will seek a profitable return on any SEO activity
      - Different examples of conversions include filling out forms and check-out activity
      - SEO can increase traffic to a site but may not increase conversions
      - There are a larger number of factors affecting whether visitors convert
      - If conversions or revenue is being measured be aware of how much traffic is needed in the NSG
          
Criteria Of A Good North Start Goal (NSG)
  -
  - Using SMART Goals
    - Setting goals in SEO is not too much different than in any other industry
    - Using the SMART framework which is a well recognised framework is a good way to set goals
      - Specific or Simple, Sensible, Significant
        - This is creating a North Star that is straightforward
        - Have clear goals which can be justified as being connected to business goals
        - This helps everyone in the organisation see what SEO success looks like
        - Always try to avoid non-specific vanity metrics which may not bring more traffic or conversions
      - Manageable or Meaningful, Motivated 
        - Do not overlook the fact that the North Star should be measurable
        - Not having measurable goals will make the North Star look like a mission statement
        - One example is being the 'biggest' in anything, unless it brings some business goal it is useless
        - Both traffic and conversion focused North Stars should have associated metrics
        - This can show a percentage or absolute increase in any single metric
      - Achievable or Agreed, Attainable
        - Do not get lost chasing unobtainable goals
        - Aim high but not so high that it is unrealistic
        - Consider performing a SWOT analysis to see what the site's current position is
          - https://searchengineland.com/seo-swot-analysis-focusing-efforts-improve-results-287197
          - Strengths
            - What are the current organisational strengths that will help a site do well
          - Weaknesses
            - What are the current weaknesses that will hinder a site's ability to do well
          - Opportunities
            - What opportunities do a site have to take on competitors
          - Threats
            - What are the threats to a site's ability to take on competitors going forward
        - Performing a SWOT analysis can show what the competitive position of a site is at that time
        - This model helps when building the foundation for an achievable NSG
        - It is important to keep reviewing things after the North Star is created though
      - Relevant or Reasonable, Realistic, Resourced, Result-based
        - There is little point creating goals which SEO cannot have an influence on
        - It is critical that all goals be relevant to the SEO strategy for a site
      - Time-Bound or Time-based, Time-limited, Timely, Time-sensitive
        - No matter the type of North Star it is important that it be given time
        - This is usually over a period of about 12 months for most websites
        - The time it takes to see results will depend on what the site position is starting off
        - SEO can often take a lot longer than other types of digital marketing to take effect
        - Using SMART to develop a North Star is an iterative process
        - When the SWOT factors change then the SMART goals should be reconsidered to reflect current positions 
  - What Makes A Bad North Star
    - If using a framework like SMART it can be hard to put together a bad North Star but it can still happen
    - Some things that show the the North Star is not a godd one and may not deliver results
      - Vanity metrics are metrics that on face value appear impressive but don't actually move the client closer to goals
      - Vanity metrics being used such as ranking No1 for a query without getting any other benefit
      - Other metrics in User Experience such as time on page, bounce rate and scroll depth
      - Metrics that are ambiguous such as the organic traffic portion of visits to purchase
      - Any North Star that is not fully and properly researched or informed
          
How To Set A Realistic Goal
  -
  - In order to properly use the SMART framework it is important to know how to set realistic SEO goals
  - There are a few different factors to consider when setting up a North Star
    - Seasonality which will show how historically targeted keywords have fluctuated during a period, this is likely to continue during the North Star period
    - Non-branded traffic should not be measured as there are other factors that influence this in addition to SEO
    - Competitiveness assessment is only an estimation which takes in to account the domain strength of sites already ranking for a specific query 
  - The North Star should only be created after understanding which keywords will be targeted
  - It is not feasible to include every keyword from a keyword universe
  - It should be remebered that a North Star exists as a guide and not an end goal
  - Calculations to be considered as part of SEO strategy include the following
    - Seasonality, CTR, Keyword Competition and Conversion Rate          

Example of an NSG And How To Formulate It
  -
  - This also includes a downloadable sample NSG to consider
  - This lesson is about how an NSG was put together for a client    
  - The tools used are as seen previously ie SEMRush, Keywordtool.io
  - Good, Better, Best
    - This is a measure of potential outcomes
    - The only difference between them are the parameters that are bounding the North Star
    - Parameters will depend on what the metric that matters is to the client, this is why there is no specific exhaustive list

Managing Expectations (For 6 - 12 months of effort)
  -
  - Once the North Star has been set up there is likely to be a lot of excitement about anticipated growth
    - It is important to manage these expectations though        
  - One way to do this is to report regulary alongside any other communication with the client
    - Many SEO firms use monthly reports to their clients to show how the SEO strategy is progressing
  - There is also the question of what to do when there is no progress towards NSG's or worse regression
    - This can include things like declining traffic or conversions declining
  - What happens if there is progress but stakeholders want it to be much quicker
  - Although results may not be where you would like them to be it is important not to checnge the North Star
  - North Stars exist because it is the one metric that matters the most
  - What can be reconsidered are some of the metrics to get there
  - Keep the SWOT analysis in mind
    - Has the SWOT analysis full considered the full impacts of both internal and external factors
    - If there are issues then it might be time to check to see if the factors causing them are actually within your control
    - If the SEO strategy is not having the impact that was being aimed for then the next thing to do is to step back and ask Why
    - This will mean a proper investigation and report back to stakeholders is needed
    - Be prepared to educate stakeholders who are not involved in SEO and will not understand issues that maybe industry wide          
  - Adjust When Needed
    - This is usually when you ask if you have been to idealistic or maybe unrealistic with NSG's
    - The runway needed maybe longer than anticipated or even the competition landscape has changed since the North Start was drawn up
    - In this case it is time to revisit projects and maybe reforming the North Star
    - Sometimes despite good intentions, North Stars can go wrong, being Agile and acting early can save a lot of waste 
          
Examples Of Data That GA Can Provide
  -
  - Using GA for North Stars
  - Why do North Stars need goals
    - GA can be used to check how a conversion focused North Star is going    
    - GA goals is needed to track user behaviour to ensure that they are aligning with your predetermined 'one metric that matters'
  - Types Of Goals
    - GA has the ability to track 4 different types of goals
      - Destination Goals which is when a specific location loads, an example is a 'Thank You For Registering' page or screen appearing
      - Duration goals for sessions that last a specific amount of time or longer, one example is the amount of time spent on a support website
      - Pages\Screens per session which is the amount of pages\screens a user views, this can be 5 or more pages per session have been loaded
      - Events which can be an action that happens when the event is triggered, examples include video plays or ad clicks         
  - Demo of how to setup custom goals in GA
    - https://www.google-analytics.ie/blog/how-to-set-up-goals-in-google-analytics-with-pictures/          
  - Consideration
    - There are a couple of considerations that are necessary to keep in mind when setting goals
    - Goals are not retroactive so there will not be data from before you set them up in reports
    - Google Tag Manager can also be used to setup goals
      - Some will prefer to use GA for setting up goals as it might be more intuitive   
          
Download Your Template
  -
  - A template for formulating an NSG
          
Task Assignment 1 : Formulate An NSG And Get Stakeholders To Agree
  -
  - Create a North Star for your own site
  - Start of with a very small set of keywords and add more later on
          
<br /> <br /> <br />

<h1><p align=center>Creating Your Monthly Report </h1><br/>
          
How To Report On SEO Performance
  -
  - Why SEO Reporting Is Important
    - It allows for demonstration of SEO value to wider stakeholders
    - Helps monitoring the impact of SEO on revenue
    - It ensures that data drives decisions in the future
    - It highlights both some future opportunities and current strengths or weaknesses
    - There is an opportunity to zoom out and see progress over a longer time period
  - The Purpose Of An SEO Report
    - It allows for sharing of progress on a series of KPI's
      - This can help inspire the wider team and boost motivation         
    - There can be an overview on how things are progressing towards NSG's
      - It can be a constant reminder of goals and how to work toward these over a longer term rather than day to day        
    - It should be easy for decision makers and other wider stakeholders to understand
      - Sometimes information can be too technical for some parts of the audience but summaries can help them understand and get on board with plans          
    - They provide insights that shape future SEO strategy
    - The reports demonstrate whether SEO efforts are bringing a positive return on the investment (ROI)          
  - How To Build A Report
    - There are lots of tools available to build reports
    - In the course Google Data Studio is used as it is a free tool
      - https://datastudio.withgoogle.com/          
    - Data Studio allows for integrating data from multiple sources
      - Both Google Analytics (GA) and Search Console (GSC) can be connected easily          
    - Filters can be added to track key metrics such as page or keyword performance       
    - Data Studio also allows for styling which can help make reports visually appealing as well as engaging to stakeholders          
  - What Metrics Should Be Tracked   
    - There are a multitude of metrics that can be tracked in Data Studio
    - It is important to be consistent, keeptrack of metrics that matter to you and stakeholders
    - Once connected to GA metrics such as organic entrances, audience analytics and conversions can be monitored
    - Connection to GSC allows for monitoring metrics such as clicks, impressions, CTR and average positions  
    - Other data sources such as Chrome UX report, ecommerce data, keyword tracking tools can be leveraged
      - https://developer.chrome.com/en/blog/chrome-ux-report-looker-studio-dashboard/          
  - Things To Avoid When Reporting
    - Avoid the temptation of assuming that strong keyword rankings mean success
      - Generating lot of clicks is good but not much use if they do not lead to conversions   
      - This will likely mean that key stakeholders will not be too interested          
    - A failure to compare performance with business goals
      - The monthly report should always refer to top level business goals are if possible and NSG's 
      - Key decision makers are then able to see how the SEO team is performing          
    - It is very important not to become obsessed with minor fluctuations
      - SEO can change quickly so if some keywords drop slightly then it is important to stick with the strategy  
      - If there are quick wins available then they might be taken but avoid spur of the moment actions which may negatively affect SEO strategy in the long run          
    - Avoid filling out the report with information that is unnecessary
      - Think of the audience and the value that they will get from each section  
      - Depending on the organisation it is probably have the report in sections that will appeal to each part of the audience          
    - Try to avoid reporting either too frequently or the opposite
      - Ask relevant stakeholders how of they would like to be updated           
      - Do not allow people to stay out of the loop for too long 
      - A reasonable benchmark is to report to the manager weekly and the department head monthly
  - Tracking Using Organic Entrances
    - In reports sometimes the key metric that is used is Organic Sessions           
    - Organic Entrances maybe a better metric than Organic Sessions
    - Organic Entrances are the number of times that visitors entered a site through a page or set of pages
    - Organic Sessions are a group of user interactions with a website within a given timeframe
      - A single session can contain multiple page views, events, social interactions and ecommerce transactions          
    - Why use Organic Entrances
      - A visitor has gone to a site and arrived at a page then gone to another page and exited the site
      - This is what is considered as a page hit and the entrance attributed to the first page
      - The session also began when the visitor arrived at the first page and is also attributed there
      - Both pages got a view so pageview counter is incremented to reflect this
      - The second page only gets a pageview counter in the scenario
      - Pixels or parameters can be hosted on different pages and this is an event hit    
      - When using an event hit with the same scenario the second page is credited with a session increment
      - If tracking the second page, this would show that second page holding more value than the first page
      - Page 1 will have done all the work but Page 2 will get the credit
    - How to view entrances in GA
      - UA -> Behavior -> Site Content -> Landing Pages  
      - GA4 -> Explore -> Blank Template -> Add 'Page Path + Query String' to dimensions and then move to rows   
          -> Add 'Entrances' to metrics and then move to values -> Increase 'Show Rows' from 10 to whichever you want
          
What To Measure, Finding Your KPI
  -
  - The Art Of Reporting
    - Properly reporting on the SEO performance of a site is key
    - If the proper Key Performance Indicator is not tracked then wrong decisions will be taken
    - There should be 2 types of reports
      - A report that is tailored to the stakeholders within an organisation
      - A report that is designed for the SEO team
      - These reports should contain the proper information that is pertinent to it's audience
  - Measuring For Reporting vs Measuring your Success
    - One pitfall of measuring the performance of an SEO campaign is to measure only for the sake of reporting
    - Sometimes there can be too many irrelevant KPI's tracked
    - This can make for visually impressive reports but they may not be looked at
    - Make the report as simple as possible and focus on business goals to measure your success
  - Key Performance Indicators (KPI's)
    - What a KPI really means can be overlooked when reporting
    - This is because there are so many measureable components
    - It is easy to be pulled in to reporting on all of them
    - It is very important to remember the difference between a key performance indicator and a performance indicator
    - To differentiate check to see if the indicator directly influences the NSG, if the answer is an unqualified yes then it is a KPI          
  - Reverse Engineering Your NSG
    - To ensure that reports contain the proper KPI then start by working backwards
    - Then start with the NSG and follow a few steps
      - Break the NSG into separate elements
      - As previously stated these elements should be organic entrances
      - Then for each element find what to measure and check for any dependencies
      - Revisit items that have a dependency and list any sub-elements that affect performance
      - These new sub-elements should be put into 2 categories depending on whether they can be measured or assessed
      - Then for each sub-element find what to measure and check for any dependencies
      - Once this has been completed to a point where all elements are dependency free the KPI should be 100% aligned with business goals          

Integrating The NSG Into Your Report
  -
  - The Importance Of Formulating An NSG
    - All SEO efforts should be revolving around the NSG
      - This helps with structure as well as being able report back to key stakeholders     
    - Ensure that the NSG captures business goals  
      - One way is to read it back, does it capture tha goals so that your teams feels that it can achieve these goals but also exceed them           
    - Has the NSG gotten approval from key stakeholders 
      - If they haven't then it is a good idea to get approval and feedback from them          
    - Have the right expectations been set
      - Does your team understand what they need to do in order to achieve the goals 
      - Are there any rewards promised if successful          
    - Are the NSG's not only ambitious but also achievable        
  - Translating An NSG Into Measurable Elements
    - Using a pre-made example to demonstrate an exiting NSG  
    - Best Practices
      - NSG's should always be time bound
      - The report should reflect the timeline
      - There should be a report on current performance and any gap that there maybe between it and goals
      - The NSG should be both visual as well as prominent in the report
    - Tools
      - Where is the data and what are the tools that are needed
      - Most of the data that will be needed such as events, sales or clicks etc can be found in either GA or GSC
          
RegEx Resources
  -
  - Downloadable pdf listing some RegEx resources
  - https://regexr.com/
  - https://regexone.com/
  - https://www.regular-expressions.info/tutorial.html
  - https://spreadsheetpoint.com/regexmatch-function-google-sheets/          
          
Using Search Console With Data Studio
  -
  - Demo showing how to connect GSC with Data Studio
  -  

Using Google Analytics With Data Studio
  -
  - Demo showing how to connect GA with Data Studio

Creating Reports With Data Studio
  -
  - Default reports are set to 1 page
  - There are also default style settings to be adjusted  
  - Simply click Add Page button on the ribbon
  - Pages can be renamed via the dropdown list on the side menu of each page
  - The same procedure can be used to delete a page          
          
Report For Your Team & Management
  -
  - Reporting for managers and stakeholders
    - This report needs to be business orientated
      - This type of stakeholder is likely to be busy so a summary view of progress needs to be shown          
    - Include information about the NSG and how the current performance is versus the gap        
    - Keep the report short and add some short analysis around factors influencing performance
    - A summary of actions that have been completed and some information on scheduled actions should be included
    - Any major challenges such as those to technical or budget should be flagged
    - What is referred to as R.A.G status can be used to indicate whether things are stopped, moving on schedule or moving but with some issues
      - https://www.gatekeeperhq.com/blog/how-to-use-rag-status-in-contract-management          
    - A slide deck is a recommended format as this can be presented by the SEO or even their manager  
    - Reports should be quarterly or monthly depending on the specific needs of the organisation          
  - Reporting To Your Team
    - These reports are usually geared more towards the technical side as well as strategy
    - Include information about the NSG and how the current performance is versus the gap  
    - All of the KPI's that are being used should be included   
    - A fully in-depth analysis of current performance should be included  
    - List all actions that have been completed or scheduled and their owners     
    - Share any impact on the SEO roadmap          
  - Reports should have the following
    - A clear header section
    - The NSG progress should both be clear as well as easy to find
    - There should be a report on any gap
    - Progress should be visualised granularly (either daily or monthly)
    - Show an overview of query performance which can be done using filters and the custom data sources
    - An overview of page performance should be shown by splitting into sections matching website sections     
    - Use other data sources to showcase other metrics such as page speed if possible          
  - How To Present A Report
    - This can be shown in a table with a wins column, considerations column and actions column          

Task Assignment 1 - Create A Monthly Report For You & Team
  -
  - Create a report for the SEO team utilising the methodology shown

Task Assignment 2 - Create A Monthly Report For Your Management
  -
  - Create another report, this time for management, this report should be business orientated rather than technical


<br /> <br /> <br />

<h1><p align=center>Site Structure & Internal Linking </h1><br/>
          
Why Internal Links Are Important 
  -
  - All Technical SEO Material including Internal Linking can be found at
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md          
  - What Internal Links Are
    - These are hyperlinks that are used to link between pages of the same domain    
    - These links are often found in some areas of the site
      - Within the main navigation usually with links to core services and resources on a site  
      - Embedded within the page itself which are often found with source content      
      - Within the footer of a page often leading through to a contact page
  - Their Use Within A Website
    - Internal Links can offer various uses to both users and search engines
      - Allowing users to navigate a site easily
      - Establishing a hierarchy of pages within a website
      - Spreading Link Equity & PageRank around a site
      - Reducing the crawl depth of key pages on a site
      - Providing users with related content or services          
  - Best Practices
    - When used to lead users through to relevant content
      - This can help keep users within the site
      - It offers users additional information such as relevant article or news etc          
    - Used to develop a clear site architecture
      - This helps guide search engines through a site
      - It allows the ability to follow links for key areas of the site to discover additional pages   
    - Including links from the homepage through to sub-pages increases the ability to spread linking out further  
      - This tends to be viewed as a pyramid type layout   
      - Additional pages will get discovered the further down it gets          
  - Common Pitfalls
    - There are some common pitfalls found when using internal linking
    - Overuse which is when there is a large number of links used within a single page
      - This will not have an effect on a search engine's ability to discover pages 
      - It will instead likely move away from the page prior to having full information          
    - Forgetting key links or pages that are not linked
      - These pages are regarded as orphaned content and the search engines have no way of reaching them or knowing they exist           
    - Hiding Main Navigation 
      - Some sites accidentally design their main navigation in a way that stops search engines accessing their links          
  - Types of Anchor Text
    - There are some different types of anchor text and can be easily identified
      - Exact Match which is anchor text that exactly matches the page being linked to
      - Partial Match which includes a variation of the main keyword of the target page
      - Branded Match which anchor text that purely uses the brand name
    - Try to avoid using generic terms like 'Click Here' as they are too generic
      - They provide search engines with no indication of what the target page contains             
          
Crawling A Site Using Screaming Frog
  -
  - A demonstration of crawling a site using the Screaming Frog Tool
          
Understanding Link Equity And Page Rank
  -
  - All Technical SEO Material including Link Equity And Page Rank can be found at
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md   
  - What Is Page Rank
    - It is an algorithm used by Google to rank web pages within search results
    - It is used to measure the importance and also relevance of pages
    - This is one of many different Google algorithms, this one focuses on link analysis
    - Pages are given a numerical value by Google which determines it's page rank          
  - How Is It Calculated
    - It works by tracking how many links point to a page and the quality of those links
    - The more good quality links that there are pointing to a page the higher the rank is likely to be
    - The Page ranks to each page on a site is determined by Google analysing the PageRank of the previous page it followed
      - This number will act as an estimated value
      - As a high number of pages end up bringing Google in a loop, each time it comes through it can calculate a more precise figure       
  - How To Boost Your PageRank Score
    - Backlinks
      - These are links pointing to a site from external sites
      - Quality backlinks come from high authority pages on a trustworthy domain and carry link equity 
      - Quality is decided on Trust Flow and Citation Flow          
    - Internal Linking
      - Efficient internal linking is needed to ensure the flow of Page Rank
        - Ensure that crawl depth is reasonable and pages are kept as close as possible to the homepage
        - Ensure that there are no pages that are orphaned   
        - Anchor text should have the correct keywords  
        - The number of links on a page should be kept to a reasonable level
  - What is Link Equity
    - It is commonly known as Link Juice and describes the value that a link passes from a page to another
    - There are a number of factors considered including page authority, topic relevance etc
    - Those pages who have more links with good equity pointng to it will usually rank higher          
  - How Is Link Equity Calculated
    - Link Relevance which covers how relevant the links is to both original and target pages
    - Site Authority which is about how trustwrthy a site is, this means that established site will perform better than new ones
    - Link Location, links hidden in footers and sidebars do not carry that same weight as those in body content          
  - Best Practices
    - Duplicate content, sites with clean architectures and no page duplication ensure that link equity is not damaged
    - Page Titles because pages that have clear, unique and distrinct titles distribute link equity better
    - Low quality pages pass little or no link equity
    - Use rel='nofollow' to stop crawlers following links, this link will not carry link equity
    - Ensure pages are crawlable, if the page is set to block crawlers then links wont pass value
    - Pages should return either 200/301/302 status codes to continue carrying their link equity
  - Common Pitfalls
    - Only certain pages within the site are getting all external liks pointing to them
    - All external links pointing only to the homepage
    - There are too many links on a page leading to a dilution of link equity    
    - There are too many redirect hops once again leading to dilution
  - Undersatnding Link Equity And Page Rank
    - PageRank will measure both the relevance and importance of a site & pages
    - Link Equity is the value and authority that is passed between pages    
    - Pages that are unique and of good quality pass the most link equity          
          
URL Structure And Crawl Depth
  -
  - What are URL's
    - A URL specifies the location of a resource on the internet
    - It also shows how to get these resources through using a protocol
      - This is usually either HTTP or HTTPS but can be FTP and others          
    - They are in human readable text to make it easier for users to understand the structure of a site
    - The standard format for a URL is
          protocol -> hostname domain name -> top level domain -> files/folder - resource
          https://    example.                com               /phones/apple
  - Why Do They Matter
    - Users Experience
      - Having clear structure to URL's will help users including search engines understand site and destination page structure
        - There is now a dedicated schema available to have this as breadcrumbs          
      - Rankings as URL's are a minor ranking factor
        - This is based on the structure and relevance to help the overall authority of the domain          
      - Site Architecture because the URL should accurately show the architecture of a site
        - This should show the way that folders or pages are laid out
        - Conceptually this can be thought of as breadcrumbs that users will follow to reach a destination page
  - Best Practices
    - Keep it simple as clear and relevant URL structures are key for users and search engines
    - Make sure that they are concise as you should get some understanding on page content just by the URL
    - Make use but not over-use of keywords within the URL
    - Consistency is important so when using a format, implement it across the site
    - Make sure that the URL is readable so use hyphens when separating words not underscores          
  - Common URL Structural Issues
    - Using too few keywords in the URL can lead to reduced relevance and weighting being applied to the page
    - The opposite can lead to problems as well as the URL may seem spammy
    - Avoid duplication issues by ensuring that no 2 URL's are the same
    - Ensure that the site is secure by using HTTP instead of HTTPS
    - When using category id's avoid using too many numbers or lettering          
  - Crawl Depth
    - This refers to how deep a search engine must crawl to find a page
    - When crawlers enter a page they analyse that content and follow links to deeper pages
    - Search engine crawlers go deeper into  a site when following it's hierarchy
    - Going deeper makes a crawler less likely to find a page          
  - Crawl Depth Best Practices
    - Internal Linking
      - Ensure that the correct internal linking is being used across the site as it
        - Reduces the number of clicks that are needed
        - It provides an increase to the user experience
        - It will lower a site bounce rate          
    - Breadcrumbs
      - Using breadcrumbs within a site will help users backtrack while staying on the site     
    - Key page focus ensures that pages that should be crawled as as close to the homepage as possible
      - These pages should try to avoid not being crawled          
  - Crawl Depth Best Practices
    - Orphaned Pages which are pages that have no links to or from them
      - This means that search engines have no way to crawl and index them          
    - Hidden navigation which is when main navigation has been designed so that search engines cannot access the links 
    - Poor internal linking can cause problems for key pages
      - They can appear deep in the site architecture or even not be crawled          
    - Navigation filters can create a high volume of pages
      - This is because of the large number of possible filter combinations          
    - Pagination should be managed to avoid errors 
      - There can be errors within long lists
      - Use low numbers of items per page or a scheme that does not allow clicking further than 3 - 5 pages at a time          
   
Crawl Budget
  -
  - What Is Crawl Budget
    - This is the number of pages search engine bots crawl and index on a single site       
    - It is based only around a certain timeframe, it also takes into account the number of pages
      - This is also affect by different factors          
  - How Is It Calculated
    - There are 3 factors that go into calculating crawl budget, 2 of them are core
    - Crawl Rate Limit which helps stop search engines from overwhelming a site when crawling
      - If there is too much crawling too fast then it will cause server problems          
    - Crawl Demand which is how much search engines want to crawl a site's pages
      - This based on how popular a site's content is within the index          
    - Page Rank which is based off of a site's Citation Flow and Trust Flow
      - This is the reason that bigger sites and more established
    - This then defines the crawl budget that a site has
      - Google defines a crawl budget as the number of URL's that Googlebot can and wants to crawl          
  - Why It Matters
    - Optimising crawl budget increases how often a crawler can visit each page
      - This collects and then returns the data          
    - It is unlikely that pages will be missed from the index due to crawl budget
      - Only if a site has millions of URL's will this be a worry          
    - It should still be a focus as the quicker that search engines can crawl a site the better
      - Sites that are quicker are viewed as cleaner and easier to access which can help rankings          
  - How To Optimise For Crawl Budget
    - Internal Linking
      - Clear and frequent use of internal linking through a site ensures that the crawlers can get the whole site easily            
    - Site Speed
      - The speed that a site's pages loads has an effect on crawl budget as Google spends time waiting
    - Clear Site Architecture
      - If possible have a relatively clear and flat site architecture
    - Important Pages
      - It is important to not block important pages from being crawled
      - Key content should be reachable and indexable as quick as possible          
    - Do Not Overcomplicate
      - Googlebot has imporved with handling rich media files ie JS, Flash etc
      - Clear HTML is much faster for the crawlers to handle and index          
    - Robots.txt
      - Instruct crawlers which areas of a site to ignore        
    - HTTP Status Errors
      - Even though a page has a 404 or 410 code it still uses crawl budget
      - The best way is to fix issues behind them or redirect properly          
    - Sitemap
      - Keeps sitemap(s) as updated as regularly as is possible
      - Crawleers can then view and crawl a wite smoothly that way          
    - Use rel='canonical'
      - This should be used when there are 2 similar pages
      - This tells the crawlers which is the main version          
  - Common Crawl Budget Pitfalls
    - Crawl depth
      - Pages deep within a site are not reached quickly so will be affected in the rankings           
    - Redirect Chains
      - Every time that a crawler has to follow a hop it will use some crawl budget          
    - Orphaned Pages
      - Pages that are not linked have no way of being indexed so will not use up crawl budget          
    - Page Speed
      - Slow pages take up more time to crawl
          
Robot.txt
  -
  - What Is A Robots.txt
    - This is a file to tell search engines that certain pages can or cannot be requested from a site
    - It is mianly used so that a site is not overloaded with requests
    - It is not a method of keeping a page out of the SERP's
      - This would have to be done with a noindex directive, these pages can still be reached through links on the site          
  - How It Works
    - It simply is a text file that tells search bots which pages not to crawl on a website
    - It should be remembered that it is a suggestion which can still be ignored          
    - There are some core elements to the file
      - User Agent which is often seen as below to say that the instructions apply to all search engines
      ```
      User-agent:*    
      ```          
      - Disallow which tells the crawlers which pages to ignore the example below tells crawlers not to visit any pages on the site
      ```
      Disallow:/ 
      ```          
  - How To Create One 
    - There are 4 steps to creating a robots.txt file
    - The first is to create a simple text file
    - Then add the values such as disallow that need to be included in the file       
    - Ensure that the file is saved as a .txt file and called robots (lowercase is necessary)
    - Upload to the root directory of the site          
  - How It Works
    - The format for instructing the crawler to avoid certain areas is
    ```
    User-agent:*
    Disallow:/tag/   
    Disallow/wp-admin/
    Allow:/wp-admin/admin-ajax.php          
    ```          
  - How To Best Utilise For A Site Structure
    - Crawl Budget can be saved by avoiding areas that are not a good use of it
      - This means that the pages you want crawled get the budget          
    - Traffic Management which in this case is about crawlers ignoring certain areas
      - Ignored areas means that core pages can rank in SERP's and more desired traffic can be gotten
    - Media Files can be troublesome if they are appearing in search results
      - robots.txt can be used to stop video, audio etc appearing in search results but the files can still be linked to
    - Purposeful duplicate content which is content that needs to be duplicate
      - Crawlers can be instructed to ignore this content          
    - Thank you pages for when people register or sign up can also be blocked
  - Common Pitfalls
    - Duplicate content can wotk in certain situations but there are better options
      - Use meta robots tags to hide duplicate content from the SERP's          
    - Case sensitivity is important when ensuring that urls are correct for crawlers
      - Rules that are added to robots.txt must be case sensitive          
    - Trailing slashes can be a very difficult issue at time due to it being hard to notice
      - This is a common mistake and is simply adding a slash after a url path which alters it ie /url vs /url/          
    - Forgetting to include slashes is related to the above
      - When urls are being excluded they must start with a /
          
Sitemap
  -
  - What Are Sitemaps
    - These are files where information is provided about the content on a site such as vides, pages etc
    - They are used to inform search engines about the important files, page that are considered important
      - There is also further information provided about these files          
    - The search engines can still crawl the site, this is just to guide them to the important pages
  - Why Might You Need A Sitemap   
    - Sitemaps can help improve the system that allows crawlers to go through a site
      - Large sites can have pages the are being overlooked
      - Large archives of content can have pages overlooked if they are not crawlable
      - Sites that are new without many external links can be helped by ensuring that the site can be found and indexed by a crawler
      - There is rich media content which could be more visible in Google          
    - There are circumstances though where a sitemap is not needed  
      - A site with less than 500 pages does not really need a sitemap
        - This is as long as there is good internal linking structure
      - As above a good internal linking structure on a site ensures that pages are found          
  - The Different Types Of Sitemap
    - There are 5 different types of sitemaps that were found
    - Only those which are applicable to a site should be used          
    - HTML Sitemap
      - This type of sitemap is created to help users navigate a site more easily
      - It will allow for accessing key site areas or topics without searching          
    - XML Sitemap 
      - This type of sitemap will tell search engines of the important pages that should be crawled          
    - Image Sitemap
      - As expected this type of sitemap is created for images and content
      - This can help image rankings within the SERP's          
    - Video Sitemap
      - This type of sitemap allow for enhancing video content with additional information such as adding categories          
    - Mobile Sitemap
      - These are rare types of sitemaps, they are only used for feature phones and not smartphones           
  - Sitemap Best Practices
    - When creating a site map the priority is to have key pages rank higher in SERP's
      - This is so that users as well as search engines doscover them sooner          
    - Always categorise site content to ensure that the hierarchy is understood as well as followed
    - Use tools such as Yoast in the Wordpress ecosystem to create sitemaps
    - HTML sitemaps should be placed in key locations on the site to help users navigate
      - This should be at the bottom of the homepage and root directory          
    - If needed organise categories and create separate sitemaps
      - This stops 2 issues, confusing for crawlers or being viewed as a link farm          
    - Large dynamic sitemaps will automatically update when pages are added or renewed
      - This means that the sitemap will always be up to date          
  - Pitfalls
    - Pages can still be reached through internal links even when not included in an XML sitemap
    - Remove noindexed URL's from sitemaps, they use and waste crawl budget as well as being contradictory
    - When pages are removed make sure that they are removed from the sitemap          

Faceted Navigation
  -
  - What Is Faceted Navigation
    - These define the search options available within ecommerce sites
    - They are made up of facets and filters
      - Filters will mean categories, facets will be options within the categories          
  - How To Utilize It For Site Structure   
    - URL Structure
      - Ecommerce site structure & hierarchy are influenced by faceted navigation
        - This means that there are a high number of varied possibilities for URL's that could be generated
      - The URL's should be layed out as orderly as possible to make them clear to search engines    
    - Robots.txt
      - Can be used to block crawlers for accessing all filtered pages
        - There can be some use for specific filters to be blocked
        - There is still a chance that search engines will crawl and find them (Albeit through internal linking)
        - Link Equity will not be spread
        - It also has a good effect in reducing crawl budget          
    - Canonicalisation
      - This should be used on filter pages and focused on the main product
        - This will mean that the core porduct or service will be indexed
        - This won't have any site pages competing with it
        - There will also not be any filtered pages ranking          
    - Nofollow which is used to stop search engines following links through to filter pages
      - This can result in link equity being trapped
      - If there are external links then the page can still be indexed
      - It can help send search engine bots the correct way on sites          
  - Common Pitfalls
    - Having all fiter pages ranking will create duplicate content
    - Crawl budget can be eaten up if crawlers are sent to low value pages instead of important pages
      - This can be worse if the pages are created dynamically          
    - Equity can be diluted from being passed to pages that will not even be indexed
    - Extreme filtering can result in thin content pages where there may only be a couple of results          
          
Canonicals
  -
  - What Are Canonicals 
    - They are simply a method of telling search engines that a version of a page is the master version
    - Use the rel=canonical on the non master version
    - This will prevent problems with indentical or duplicate pages
    - It will inform search engines the correct version of the pages to have in the SERP's          
  - How Do Canonicals Work
    - They are put on pages which are duplicates and point toward the main version
    - This informs and guides search engines and works especially well for e-commerce pages   
    - There are a few different benefits to using canonicals
      - They help guide the search engines
      - They work very well for filter pages on ecommerce websites
      - They can also be self-referencing when needed          
  - Best Practices
    - Self-referential Canonicals
      - To avoid any confusion which may happen
        - Ensure that any duplicate version poins to the main version
        - Also the main version must point to itself
        - Remember that search engines can still ignore them          
      - There is an international benefit to using country specific versions
        - If it is on the same domain alongside hreflang           
    - Faceted Navigation
      - This can be used for pages with facets and filters
        - This stops these filter pages from targeting the same terms that the main page is targeting
        - It helps divert the serach engine bots away from multiple similar pages
        - There is a hierarchy on the site and with filter pages and it helps the search engines understand this
    - Search Intent
      - This comes into play when 2 pages are targeting 
      - Always try to make sure that the better perfroming page is used
      - Stops search engines switching pages for pages
      - This helps consolidate traffic          
  - Common Pitfalls
    - Mixed signals are a common pitfall
      - Ensure that links are pointing from one location to another
      - Avoid canonicals on top of canonicals
        - Page A -> Page B -> Page C          
    - Canonicals and Redirects
      - Do not canonicalise to a redirect page, use one or the other
      - This type of practice will waste a lot of crawl budget
      - Search engines may not be sure which mpage is actually the master version and so may choose their own
      - This can also lead to canonicalising to a noindexed page which would be very poor practice
    - Canonicalise pages in sitemaps
      - Again this will provide mixed signals to search engines
      - Only pages that are to be indexed should be in sitemaps          
    - Pagination
      - There is a theory that each page in a pagination series should canonicalise to the first page, this is wrong
      - Each page should self-canonicalise so that search engines can understand the sequence and discover the content

Information Architecture Template
  - 
  - Downloadable template for mapping out a site structure          
          
Task Assignment 1 -           
  -
  - Create a visual crawl map using Screaming Frog
  - Use it to understand the key pages and how deep they are in the site          
          
Task Assignment 2 -            
  -
  - Identify any internal linking opportunities that may be available
          
Task Assignment 3 -
  -
  - Create a IA that functions, using the provided template
          

<br /> <br /> <br />

<h1><p align=center>Creating A Content Strategy </h1><br/> 
 
Understanding Your Audience
  -
  - Create A Content Strategy
    - Prior to putting together a content strategy understand who is being written for
      - Who is the audience that will engage with the content 
      - Understand what this audience is interested in
      - What are their needs and values so that content can be customised          
  - Look At Competitors
    - Looking at competing brands can bring great insights into how they reach their target audience
    - This is especially useful if the competitor is targeting the same audience
    - Examine the tone of voice that they use in addition to the messaging
    - Look for subtlties in the language used and the phrasing
    - Do they use images and how do they use them
    - Don't just limit this to organic content, look at social media and paid search also to get a fuller feel          
  - Create Personas
    - Personas have been used by marketers for decades, this is simply because they work
    - A persona represents the targeted users and offers a more realistic target audience to keep marketing efforst consistent
    - These personas help you better understand how people search for or buy something ie a product or service  
    - This gives the ability to focus energy on improving offerings to real life customers
    - There are 3 main questions that are asked by personas
      - Who are they
      - What is their main goal
      - What is the main barrier preventing achieving that goal
    - One of the best ways to put together personas is to use data from users gained through interviews
    - Some budgets may not allow this so analytics tools can be used
      - Common themes or trends that overlap between users can fall into one persona
    - There are templates available to create personas such as
      - https://blog.hubspot.com/marketing/buyer-persona-research           
  - Speak To Existing Customers
    - Where possible engage with existing customers to get their opinion
      - Ask what type of content interested them most
      - What can your brand do to more align with theirs
    - Try finding out what the customers greatest challenges are
      - Where does your product fit in with helping them with these challenges
      - What appeals or excites them
    - The more customers that are spoken to the better
      - This will give a much clearer picture of who is consuming the content in real life          
    - There are a variety of tools available to help with reaching out to customers
      - SurveyMonkey is an easy to use survey tool
      - Survey completion rates tend to be on the low side due to them not incentivising completion          
  - Review Engagement Metrics
    - This is where attention is paid to how many are responding to content
    - It also covers how they are responding and how often they are responding
    - It is recommended to take a view of engagement across all channels
      - Likes, shares and comments from social media and on-site comments if the facility is available          
    - There are also some organic metrics to check out
      - Organic entrances, time on page and site bounce rate to name a couple          
    - Once the engagement metric data has been collected it is time for examination
      - Check out the content and topics that are faring the best and that the audience values most
      - There is also the opposite effect too where content that is poorly valued is shown up
    - After this it is time to consider how to reach customers throughout their journey
      - This can only be done once the user intent is understood          

Understanding Visitor Intent
  -
  - Search Intent
    - Search Intent aka user or query intent is the categorisation of a search term depending on what the user is looking for
    - Matching content to user needs is a fundamental part of a content strategy
    - Search engines have moved much closer to delivering search results that reflect user intent          
  - Types Of Search Intent   
    - There are usually 3 types of search intent informational, navigational and transactional aka Do, Know, Go
      - https://www.searchenginejournal.com/seo/how-people-search/          
    - When a user performs a 'do' search they are looking to perform a certain action eg purchase an item
      - Search results will usually return ecommerce sites offering a chence to purchase or sites that match a brand
    - When a user perform a 'know' search which is an informational type of search
      - Users perform this type of search to find answers to questions or try to learn more about a subject
      - Building content on the basis of informational intent is a little more challenging
      - Informational searches tend to go from broad to in-depth specific
      - The issue with these complex queries is that they often do not have a clear answer
      - A larger uses of informational type queries is people preparing for trips and\or experiences who wish to find some prior information
      - 
    - The 'Go' type queries typically occur when people know the brand or location of something
      - A user is usually looking for a specific brand which they have used in the past
    - Searchers will often use a combination of different queries depending on the stage of the customers journey that they are at
      - https://blog.hubspot.com/service/customer-journey-map
      - https://thecxlead.com/topics/website-customer-journey/          
    - Some very broad queries will fall into more than 1 of these types of search intent
  - User Intent And The Search Landscape
    - Each of the intent types will result in a different result page from the search
    - More advert heavy pages are found when doing navigational queries
    - Knowledge panels are found more on navigational and informational type SERP's
    - The SERP landscape should not overly influence content creation but be mindful when assessing content performance
    - Content should be optimised to use SERP fetures like featured snippets
    - As is to be expected when trying for organic clicks in competitive areas, content should be very authoritative and highly relevant
  - What To Do If Intent Is Not Understood 
    - If there is any doubt on search intent the best place to start is by looking at the SERP for the chosen phrase
    - Are the results being returned related to the products or services or are they more inofrmational
    - There are applications such as SEMRush, BuzzSumo and AnswerThePublic which help show what keyword modifiers are being used
      - https://buzzsumo.com/
      - https://answerthepublic.com/
    - Keyword modifiers are usually a good indication of search intent
          
Auditing Existing Content
  -
  - Why Audit Content
    - Prior to rolling out new content to a site there are some things that should be understood
    - What content is already available on the site
    - How is that content performing, good or bad
    - Are there any gaps in the content that is currently offered          
  - Where To Start
    - If a site is larger than a few pages then simplification of URL list compiling is needed
    - Some CMS such as Wordpress will allow for downloading a list of site URL's
      - https://www.elegantthemes.com/blog/tips-tricks/how-to-export-wordpress-urls-using-two-different-methods          
    - SEMRush can be used to crawl all pages on the site
    - Pages that return a 200 status code are to be filtered for
    - Data can be supplemented by information form sitemaps and Google Analytics      
  - Extract Metadata And On-Page Metrics
    - There is data that will need to be extracted
    - Title Tag from the page, H1 and other headings, Meta Descriptions, The word count from articles and the categories of the topics
  - Extract Organic Traffic Metrics
    - Average organic traffic that has been gained for each URL
    - What are the click through rates (CTR) for each URL
    - What is the average time on a page for organic search traffic
    - The bounce rate of organic search traffic from the site          
  - Extract Backlink Metrics
    - How many backlinks to the page are there
    - What is the overall authority of both the overall domain and the specific URL
    - Who are the competitors and what is the authority of their domain and pages          
  - Other Important Metrics To Consider
    - There are other metric available which according to situational needs should be looked at
    - What are the current ranking for the target keywords for the page
      - What are the search volumes for associated keywords          
    - Are there any engagement metrics available such as social shares          
  - Bringing The Data Together
    - The first thing when putting data together is to create a spreadsheet to track all of the collected metrics   
    - Compare content that is performing well to content that is performing poorly
    - Check which topics are working well for the audience and engaging well with them
    - What are the content types which have the most backlinks
    - Which content click through rates need to be improvement
    - Look for quick win topics such as metadata improvement          
  - Next Steps
    - Add an action column to the spreadsheet to define actions to be taken
    - These actions are create new content, leave existing content, optimise existing content, merge 2 or more pages of content          
          
Keyword Research For Content
  -
  - Keyword Research For Content
    - Keyword research is the first part of the content writing process
    - Doing this ensures that the writing is framed around the terms and topics of the target market
    - Keyword research also also ensures that content matches the search intent of users
  - Using The Gap Analysis
    - Ensure that an examination of keywords gaps to competitors has been performed
    - When beginning to create content a list of the most important gap topics should be put together
    - These should be based on relevance, search volume and competitiveness
    - Once the topics have been created then there are a few things that need to be done
      - Check for existing pages on the site which target keywords from the gap cluster
      - If there are then a decision needs to be made whether to optimise existing content or create additional pages or articles around the same topic
      - The keyword gap cluster should be supplemented with newer keyworsd from further research        
  - SERP Based Keyword Research
    - Using Google itself can be fertile ground for keyword ideas
    - Auto Suggest where suggestions are based on real searches
    - Related Searches which are available at the bottom of a SERP page
    - People Also Ask which is a great way of understanding search intent and related questions          
  - Keyword Modifier Research
    - A modifier is a word that when used in conjunction with the main keyword makes the search query more specific and have greater context
    - There are some common modifier types 
      - General words which are intended to narrow searches such as 'new' or 'best' etc
      - Money related terms such as 'Cheap' or 'Affordable' or 'Deals' etc
      - Time-bound modifiers such as 'Latest' or 'Updated' or 'Quickest' etc       
      - Niche or industry specific modifiers which include 'Services', 'Solutions' or 'Forecast' etc
      - Location specific such as 'London based' or 'North-East' etc    
    - There are a multitude of tools available for this task
      - GSC, Google Keyword Planner, Google Auto Suggest, Keywordtool.io etc   
      - SEMRush also has the keyword magic tool available as part of it's product
        - This algorithm will suggest keywords that are semantically related to initial searches and sorts them by topic
          - https://www.semrush.com/blog/semantic-keywords/
        - The tool can be uses to sort keyword by a few criteria 
          - Search volume, keyword difficulty, competitive density and Cost per click data to name a few

Keyword Competitiveness
  -
  - What Is Keyword Competitiveness
    - If content is being optimised or created it needs to be understood how difficult it will be to be seen using organic searches
      - This is afterall why the content is being created          
    - Keyword Competitiveness is defined as the level of difficulty in acheiving visibility for a keyword
      - There will be a variety of factors including SERP competition and site authority
  - Keyword Competitiveness Metrics
    - Keyword Competitiveness can be and often is used as a metric 
    - There is a keyword difficulty tool available within the SEMRush application
    - It estimates how difficult it would be for a new webpage to possibly outrank pages that are on the first 2 pages of SERP's
    - The score is between 0 and 100 and the higher the percentage the more difficult SEMRush reckons it will be to rank for the search term           
  - High Difficulty
    - There are in general 3 levels of difficulty scores
    - Above 80% Difficulty level
      - This level requires significant investment in technical, content and link building efforts
      - Brand new domains are highly unlikely to achieve a high level of organic visibility
  - Medium Difficulty
    - This is from 60 - 80% difficulty
    - It is a very challenging but more realistic option for sites investing in SEO initiatives
    - There is still a need for considerable investment to drive traffic is organic visibility is still below expectations
  - Low Difficulty
    - This is below 60% difficulty
    - Keywords can be easier to rank for but search volumes are lower too  
    - There is a sweet spot which would be low difficulty but high search volume which is relevant          
  - Caveats Of Keyword Competitiveness Metrics
    - These metrics often use average data which means there may be issues with precision
    - SEMRush uses the difficulty of a new site achieving visibility against competitors
      - It does not consider if there is existing content being reoptimised          
    - The database is not exhaustive and there are factors that can interfere with calculations
      - Low search volumes and long-tail keywrods which can often return 0 results          
  - Using Backlinks To Determine Competitiveness
    - Backlink profiles will offer clues to how difficult it will to be to rank for a term
    - Take note of competitors in the SERP's and assess their backlink profiles in a tool like Ahrefs
    - If either Domain Rating or Trust Flow are considerably higher than yours then overtaking them is likely to be extremely challenging
      - It maybe better to spend time focusing on long-tail lower competition keywords           
  - Reviewing The SERP's
    - It might not be just the usual competition that a site is up against such as organic competitors or paid search results
    - There might be unforseen verticals of universal search such as
      - https://www.searchenginejournal.com/search-engines/universal-search/          
      - Featured Snippets, Local Packs and Knowledge Graphs          
    - Be aware of this when considering which keywords to target          
          
Using SEMrush To Estimate Traffic Cost
  -
  - Why Look At Paid Keyword Data
    - Understanding paid keyword data is useful for paid search marketers seeking to build budgets
      - They are able to get to know the likely competition on the paid side
    - SEO's can also get some help from this data as the Cost Per Click (CPC) data will allow for some conclusions to be drawn
      - The main one will be the approximate value of the keywords when trying to gain traffic through SEO      
    - Then there will be a proper examination of how cheap or otherwise paying for keyword visibility is versus organic visibility           
  - When exporting data from SEMRush there is a column in the export called CPC 
    - This shows the average cost per click for each keyword        
  - How To Calculate Keyword Value
    - Download non-branded keyword data from SEMRush
    - Remove any of the CPC values that are equal to 0.00
      - This will distort the average CPC if left in          
    - Calculate the average CPC for selected keywords, this can be either specific topics or the entire keyword group     
    - Then multiply the average CPC by the total monthly search volume
    - This gives the approximate monthly cost of achieving visibility for keywords if paying          
  - Caveats Of Using This Method Of Calculation
    - Tools such as SEMRush can often underestimate the true cost of keywords
      - Actual keyword spend may turn out to be a lot higher          
    - There are 3 main reasons for this fact
      - 15% of all searches are new which means that no database will contain every query
      - Paid SERP's change much faster than organic ones
      - Ads are only usually analysed at national level, they may not reflect the true picture in different locales
    - There are other reasons for this method to be used as a guideline rather than an absolutely true method
      - Not all keywords have equal value
      - Not all keywords will be relevant
      - There will be keywords with no CPC data available          

The Relationship Between Paid And Organic Search
  -
  - How SEO can benefit from PPC
    - As a rule paid search results do not directly affect organic rankings
    - Advertising though does help unearth opportunities and insights for SEO          
  - Indirect Benefits of PPC For SEO
    - Searchers who see an advert maybe more likely to click on an organic result
    - Searchers who have already been exposed to a particular brand may in the future be more likely to convert via organic results
    - Paid ads can result in increased traffic for a site which increases exposure ie social media signals etc 
    - Google research shows that users are more likely to click on an organic link when a paid ad is shown 
    - Google has also shown that when ads are removed that their clicks are not replaced by organic clicks
      - https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37161.pdf
  - Supporting The SEO Strategy With PPC
    - There are multiple ways that PPC can supplement the SEO strategy
    - Together they can maximise real eastate in the SERP's
      - https://searchengineland.com/seo-ppc-serp-real-estate-content-glue-roi-194073          
    - There can be immediate visibility for hard to rank for terms
    - It will allow for immediate results to be seen
      - Keywords and content that have the highest conversion rates can be seen easily
      - Low performing keywords can be eliminated
      - It is ideal for running quick tests  
      - It will help when creating brand awareness
      - It can help negate any negativity that may exist around the site
      - PPC landing pages can be indexed to drive traffic from organic
      - Paid social campaign insights can help understanding an audience better          
          
Keyword Mapping And Content Hubs
  -
  - What Is Keyword Mapping
    - Keyword mapping is simply that assigning of keywords to pages on a site
    - This is done to ensure good Information Architecture so users and search engines find the most important content
  - Why Map Keywords
    - Mapping keywords has several benefits
    - It helps ensure that key topics have dedicated and relevant content
    - It allows for primary pages to be tracked
    - It will reduces the chances of keyword crossover or cannibalisation
    - It can for the basis of on page meta optimisation          
  - Keyword Crossover\Cannibalisation
    - Keyword crossover or cannibalisation occurs when two or more pages on a site attempt to rank for the same keyword
    - There are a few reasons why this is a bad thing
      - Google may not rank the page that you want it to rank
      - There is a potential splitting of clicks and PageRank between 2 or more pages
      - Content writing and maintenance are split across multiple areas which may consume a lot of resources 
    - There are things that can be done to stop this happening
      - It should be avoided as a practice right from the beginning so that it does not become a habit 
      - Every article should have a targeted keyword
      - High volume search keywords should only be used on 1 page as a primary focus keyword  
        - https://mangools.com/blog/focus-keyword/          
    - There are also some things that can be done if it has already been happening
      - Change the keywords being targeted by the page by removing them from title and h1 tags
      - Merge the content involved and remove pages where necessary
      - Make a page the canonical version
      - If the content is related but not duplicate create a hub page for the pages to link to, this signals which pages is for ranking       
        - https://ahrefs.com/blog/content-hub/          
  - What To Map
    - There are a few ways to keep track of keyword mapping, the simplest option is to use a spreadsheet
    - Each page will need columns for URL, Page Title, H1, Meta Descriptions, Primary Keywords, Supporting keywords, If it is hub or spoke page, Anchor text          
  - What Is Next
    - Once keywords are mapped to a page usually there will be several related pages around the same theme 
    - In order to signal to users which of these is the most important internal linking will need to be used  
    - This is where content hubs become useful          
  - What Are Content Hubs
    - Content hub strategies are internal linking strategies
      - It involves linking several pages of content back to a central hub page          
    - Conceptually it can be thought of as a wheel with the central hub and supporting content via spokes
  - How Do Content Hubs Help SEO
    - Content hubs work by driving link authority and topical relevance from the spoke pages into the hub page
    - By concentrating internal linking into a single page the page's ability to rank has improved
  - How To Create Create Hubs
    - There are 2 primary ways of linking to create a hub
    - Inline Linking or Links From Navigation
    - The structure of the site will dictate which will be the choice
    - Avoid overcrowding navigation bars if at all possible
    - Important pages should be kept within a maximum of 3 clicks from the homepage          
  - Succeeding With Content Hubs
    - Ensure that the content in the hub is free from crawling and indexing issues
    - Keep all linking consistent
    - Use descriptive and keyword focused anchor text in the links
    - Ensure that backlinks are on spoke pages as well as hub pages          
          
Meta, Title, H-Tags Optimisation and Best Practices
  -
  - Identifying Areas For Metadata Optimisation
    - The first step in optimising metadata is to find pages that require the most optimisation
    - Identify and then download the lowest CTR content using GSC
    - Use a spreadsheet to create a graph measuring the average position of organic search rankings vs the CTR
    - Then apply an exponential average curve to the graph
      - https://www.investopedia.com/terms/e/ema.asp
      - https://www.dallasseogeek.com/math/exponential/          
  - Deciding Which Area To Focus On
    - Areas that fall well below the average curve line should be focused on
    - This is because these areas have minimal risk but the prospect of large rewards          
  - The Role Of The Meta Title
    - The page title aka title tag is shown on SERP's as a blue hyperlink one line page summary
    - They help search engines understand page content and can help with rankings          
  - Meta Title Optimisation
    - Titles should be between 50 and 60 characters long at most
    - They should be unique for each page on the site
    - Titles should be written for the target audience with search intent kept in mind    
      - This means that they need to be relatable as well as emotive, sometimes numbers can also help          
    - Titles can be divided using things like hyphens and pipes
      - https://webmasters.stackexchange.com/questions/126994/vs-vs-which-title-separator-is-suggested-for-the-best-google-seo-practic          
    - Include the brand name within the title
    - Try out different combinations of punctuation and capitalisation
    - If there is any data from PPC sources than try and use it for insights          
  - The Role Of The Meta Description
    - This is the short description found in the SERP's beneath a url
    - They don't influence rankings but can influence whether a user clicks on the url by helping show what the website is about          
  - Meta Description Optimisation
    - There should be no more than 160 characters which includes spaces
    - These should also be unique for every page
    - Ask questions of users and make use of action words such as 'explore' or 'read' and 'listen' etc
    - Also make use of power words such as 'secret' or 'premium'          
  - The role of the H1 tag
    - This tag is a main heading tag
    - It is usually the same as the page title but it is not essential that they match exactly
    - The primary keyowrd should occur once within the h1
    - It should outline the topic of the page and should aim to entice readers to consume the page content
    - There are similar tags for subheadings such as h2, h3, h4 etc
    - The subheadings are useful for segmenting content and braking the content into easier to read pieces          
          
Search Quality Evaluation Guidelines
  -
  - Search Quality Evaluation Guidelines          
    - Google's Search Quality Evaluation is available at the following URL
      - https://static.googleusercontent.com/media/guidelines.raterhub.com/en//searchqualityevaluatorguidelines.pdf
    - There are approximately 10,000 people who are employed as search quality raters
    - The goal is to assess websites and ensure that they offer high quality content that meets vistors needs
    - These guidelines are used by people to judge both quality and trustworthiness of sites and SERP's
    - These raters are not able to directly affect rankings
    - On the other hand though their checks are designed to evaluate the effectiveness of the Google algorithm
      - Any findings then will feed into algorithm tweaks and changes          
    - The guides are large pdfs and are easily found online 
    - A lot of search engine and SEO related sites will publish material on any changes that are made
    - Google places a lot of importance on impartiality and diversity in reviewers and site reviews          
  - Purpose
    - All webpages that are created on the internet have a purpose
    - It is important for search quality raters to determine the purpose of a page
    - This is so that they can evaluate how well the page in question fulfills that purpose
    - Sometimes a site's purpose can be helpful to users by for example educating them, other times not by misinforming or lying to them
    - If sites are to be rated based on their purpose then according to Google that they should be created to help users
    - A page whose sole purpose is to make money would be considered as a page of the lowest quality          
  - Expertise Authority Trust (EAT)
    - E-A-T is a criteria that is applied by Google to assess content
      - Content must be reliable, accurate, good quality and publishers must be reputable          
    - In order order to meet this content demand, it must demonstrate
    - Expertise which means being highly knowledgeable about the topics in question
      - There are a few different ways such as using thorough in-depth content on a page
      - Another is demonstraing the experience and qualifications of the content author          
    - Authority, backlinks from sites which are both relevant and autoritative are a strong factor in rankings
      - This in turn determines the E-A-T of a site
      - There are other ways of signalling Google of authority such as brand mentions on authoritative sites and social media shares           
    - Trustworthiness which helps maintain those hard won Google rankings
    - There are many different ways that you could use to demonstrate a site's quality to a rater
      - Implementing HTTPS on a site and showing that a site is secure
      - Provide proper and up to date contact details for site owners
      - Ensure that terms and conditions are easy to access
      - If goods are sold then refund and return policies should be clear (the same is true for deliveries)
      - If appropriate show proper safety advice on products to customers
  - Your Money Your Life
    - EAT is not only about judging a site it looks at content authors too
    - YMYL sites are of particular importance for EAT
      - https://searchengineland.com/google-ymyl-eat-seo-380569          
    - YMYL sites will always be more scrutinised because of their sensitive nature
    - This type of site includes medical, governmental, financial and legal orientated sites
      - It also covers shopping, banking and financial transactions          
    - Usually sites that have even a portion of YMYL focus will have to endure higher scrutiny
      - This is because search engines seek to protect users from potentially harmful information or products          
    - Page quality's nature shows the need for any brand to grow a strong and authoritative brand online
      - This is not something that can be achieved overnight but will have to be done incrementally
          
How To Spot Key Opportunities
  -
  - Looking At Competitors
    - Taking note of all competitors including direct market and also search competitors is essential to spotting key opportunities
    - Opportunities can be seen from examining keyword gaps as well as manually reviewing competitor websites          
  - Competitor On-Page Opportunities
    - Start by reviewing both the top and sub level navigation
    - Are there any topics which are missing
    - How is the content on the site structured
    - It is worth examining where content is kept and how it is structured on sites that get a lot of traffic through content          
  - Creating A Knowledge Centre
    - A knowledge centre offers a chance to write content that supports the main products or services being offered
    - There are a variety of ways to do this and it will depend on what suits the website
      - Informational guides and resources
      - Tutorials and 'howto' guides
      - Industry news and any updates available
      - Case studies and related inspirational content          
  - On Page 'Serendipitous' Content Opportunities
    - Serendipitous keywords are those which are outside the typical sales funnel for the target audience
    - They cover topis related to the main products a site offers but are not as focused on conversions
    - This type of content will help widen the potential reach to a broader audience
    - This can attract potential new customers through a variety of insightful content on wider industry trends and topics
    - This type of content can be created for almost all types of sites from B2B throught to local sites          
  - SERP Opportunities People Always Ask
    - People Also Ask boxes appear in the SERP related to the original query
    - They are sets of question similar to the one asked by the original search
    - https://www.semrush.com/blog/how-to-maximize-people-also-ask-seo-opportunities-study/          
  - How To Get Content In PAA Boxes
    - Getting content in PAA's is similar to using featured snippets
    - Create clear good quality content that fully answers questions about the service or product that you are selling
    - Ensure that all the questions have been answered fully
    - Avoid jargon and answer the question in simple language
    - Sometimes using bullet point lists helps or maybe short sentences
    - Add a Q&A or FAQ schema where it is appropriate          
  - Link Opportunities
    - There are things that must be remebered when reviewing key opportunities for links
    - Unlinked brand mentions, this can be handled using Google Alert to monitor mentions
    - Legacy URL's and their redirects should be looked at again
    - 404s which are happening on the site should be properly examined          
  - Structured Data (Schema) Opportunities
    - Implementing a schema correctly can lead to richer information being shown on SERP's which imporves visibility and may improve click through rate
    - It can also lead to a better understanding of the site's content by search engine crawlers
    - Use the Schema Markup Testing Tool from Google to check both current implementation as well as more suitable opportunities        
    - There is a structured data codelab available from Google to guide the process
      - https://codelabs.developers.google.com/codelabs/structured-data#0          
          
Google Discover
  -
  - What is Google Discover
    - This is a mobile tool which uses AI to recommend to users, content based on interests and web activity
      - https://developers.google.com/search/docs/appearance/google-discover          
    - It is available via the Google search app or homepage on mobile
    - It is not available in all countries yet          
    - There are approximately 800 million users of Google discover according to its own research
    - There are huge opportunities available to engage with users prior to them even starting to search          
  - How Discover Differs From Traditional Search
    - Google for the last few years has been continuously shifting to more personalised search
    - Discover enables users to curate pre-search content based on their interests
      - https://www.321webmarketing.com/what-is-google-discover-why-it-matters-for-seo/          
    - No keywords need to be entered into the search bar
      - This means a mindset shift from keywords to audience interests and relevant topics          
  - How Does Google Rank Content On Discover
    - Google has been as always fairly quiet on how to rank for content on Discover
    - The best guess is that it ranks algorithmically on what it believes a user would find most interesting
    - This means that there is a close match between content and a user's interests
    - Content therefore needs to be timely and relevant
    - E-A-T principles would be of even greater importance          
  - Creating Content For Discover        
    - Discover is designed to show users content that they will interact with
    - This will mean that is usually trending news stories, evergreen articles or a mixture of the two          
  - Things To Note
    - Covering both evergreen as well as fresh topics is the basis of a good strategy for discover
    - There should be a plan for regular fresh content such as new product reviews or similar
    - Always review the evergreen side of things to ensure that it stays fresh and be prepared to renew if it is needed
    - There does seem to be a slightly heavier weighting by Google towards new content so publish regularly
    - It is also improtant to remember that user needs and preferences will evolve over time so evolve with them          
  - Optimising For Click Through Rates (CTR)
    - Make any headlines present concise
    - Ensure that the headlines are outlines of the article content
    - If using images then make sure that the capture viewer attention
    - Make sure that the articles are very closely aligned to user interests and what they find important          
  - Optimising For Click Through Rates (CTR) Things To Avoid
    - Do not publish fake news or misinformation
    - Avoid potentially offensive content which may alienate or upset users
    - Do not use sensationalist or misleading headlines
      - https://www.semrush.com/blog/what-is-clickbait/          
    - Headlines should be concise but not too much as well as not too verbose          
  - Optimising Images For Google Discover
    - Bigger is better where images are concerned, images should be at least 1200x1200 pixels
    - Use large images over tumbnails
    - To show large images there are a couple of options available, the first is to be using AMP
    - The second option is to allow Google to use the hi-res images via an opt-in form         
  - Large Images vs Thumbnails
    - Large images are reported by Google to be better than thumbnail
    - They can boost CTR rates by up to 5%
    - Time spent on pages with large images increases by 3%
    - Overall user satisfaction is also increased by 3% when using large images instead of thumbnails          
  - Technical Requirements For Discover
    - There are several technical requirements that need to be met when starting with Discover
    - Pages must be mobile friendly
    - The pages are indexed by Google
    - There is no requirement for accelerated mobile pages
    - There is also no requirements for structured data
    - Allow Google to use the hi-res images via an opt-in form           
  - Tracking Traffic From Discover
    - Once a decent level of visibility has been achieved then a new tab called performance will appear in GSC
    - This will allow for comparisons between the regular organic traffic and Discover traffic
    - Content that is performing the best can be seen in the Discover tab
    - Total amounts of traffic that is arriving through Discover can be seen
    - How frequently the site appears in Discover can be seen          
          
How To Give Recommendations To Content Writers
  -
  - Writing For Users And Search Engines
    - Google for a long time has strongly recommended content writers to write for users and not search engines
    - Content writers can often become to focused on metric such as keyword density
      - This is a mistake as it does not indicate whether a piece of content will perform well organically or not.          
    - Content should offer real value to a user such as solving a problem or answering a question
      - It doesn't really matter if the content is human written or auto generated          
  - Creating An SEO Checklist
    - There can be some confusion between writing for users and writing for search engines
    - If using others to write content then they should have a checklist to follow
    - This should mean that all key SEO elements are included in the content 
    - This checklist should cover everything that someone creating a new page on the site would need to know
    - There are a few metadata best practices that should be followed as well as the following
      - URL structuring conventions should be adhered to
      - Image names and their ALT text best practices should be used
      - Internal linking and anchor text should follow guidelines           
  - Using Content Templates
    - Content templates are structured briefs that are given to content producers
    - There are a few elements of a content template
      - Title tag (Page Title)
      - Meta descriptions
      - Main heading (Using h1)
      - Any subheadings used (h2 or h3)
      - The main target keyword and their monthly search volume 
      - Secondary or supporting keywords should also be provided
      - The content topic(s) or category and whether it is time bound or evergreen
      - The recommended length of the text which is based on the top ranking content
      - There may also be a link to benchmarked content to show the writer what successful content is          
          
Quick SEO Checklist For Writers
  -
  - Downloadable checklist for writers
          
Recommendation Template For Writers
  -
  - Downloadable template for giving writers recommendations 
          
Task Assignment 1 : Audit An Existing Piece Of Content
  -
  - Choose an existing piece of content on your site and perform an audit on it
  - Then use a tool like SEMRush to look for keyword optimisation opportunities
          
Task Assignment 2 : Create An SEO Content Template For Writers
  -
  - Use the downloaded SEO checklist template to create recommendations for writers
  - It may be worth writing specific recommendations for specific topics


<br /> <br /> <br />

<h1><p align=center>Technical On-Page Optimisation </h1><br/> 

Checking Canonicals
  -
  - What is a canonical
    - It is a piece of code which can be added to the HTML of a page
    - This tag allows for setting a master version of a page
    - It tells the search engines which version of a page should be indexed
  - Why Check Canonical Tags
    - If there is similar content on more than a single URL Google might deem that to be duplicate content
    - If a canonical is not specified then Google might pick one
    - This can be a risk as Google might not pick the preferred page and displays this in the SERP's
    - Making sure that canonicals are set properly can help avoid the above issues
  - Checking The Canonical Of A Page
    -   
  -

JavaScript And The DOM
  -
  -

Image SEO
  -
  - 

Video SEO
  -
  -  
