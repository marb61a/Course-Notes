<b><p align=center>                    
  Blue Array Academy SEO Manager </br>
  Course Notes  
https://www.bluearrayacademy.com/courses/seo-manager-certification

<br />
<h1><p align=center>Welcome </h1><br/>
  
Course Update History
  -
  - A list of updates to the course is shown
  - The course is updated regularly in order to keep it relevant

About this course
  -
  - A quick intro to the course
  - The course shows the way Blue Array do SEO for clients <br /><br /><br />
  

<h1><p align=center>How Does Search Work </h1><br/>  
  
Market Share of Search Engines
  -
  - There are 5 dominant search engines
    - Google - https://www.google.com/
    - Bing - https://www.bing.com/
    - Yahoo - https://ie.yahoo.com/
    - Yandex - https://yandex.com/
    - Baidu - https://www.baidu.com/
  - Duck Duck Go is another search engine available but the course concentrates on the 5 above
    - https://duckduckgo.com/
  - Google is by the market leader with about 93% share
  - Bing and Yahoo largely return the same result as Bing powers a lot of Yahoo
  - Yandex is Russian and it's audience is mostly Russian speaking, Baidu is the same but for China

SERP features & Google Horizontals  
  -
  - Google's results have evolved considerably but not affected their dominance
  - Google SERP's continue to change and new features are added and removed
  - Google adverts will have a box saying Ad beside the URL
    - This kind of result will not be covered in the course as they are not organic search results
  - Structure data and Rich results will have a large impact on rankings
    - More information on the kind of technical factor is available in notes at
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md
  - Depending on what is being searched for there will be a knowledge card on the right of the search results
    - This can contain aggregated information from different sources
    - It is hard for a single site to fully own this
    - It is easier for companies to own this if they have filled out Google MyBusiness of Google Brand Account
    - Using MyBusiness and Brand Account can help getting location specific maps appearing on the main SERP page
  - Horizontals refer to the list of search tabs such as News, Videos, Images etc
  - Many of the listings found on the Shopping horizontal are paid for listings

Crawling, Rendering and Indexing
  - More information on the kind of technical factor is available in notes at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - Crawling, rendering and indexing is how a page appears in the SERP's
    - These are independent of a page's rankings in the SERP's
  - https://developers.google.com/search/docs/crawling-indexing/javascript/javascript-seo-basics
  - If Google encounters a noindex prior to executing JavaScript then the page will not render
  - Orphan pages are pages that a page lacks links from other pages on a site
  - https://www.danielmorell.com/guides/htaccess-seo

The Notion of Crawl Budget
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - Most sites do not need to worry abount crawl budget as it is only sites over 1000 pages that start having issues

Basics of HTML DOM, What it is and why it's important
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 

JavaScript and modern Web frameworks
  -
  - Again more information can be found at the following URL
    - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md 
  - JavaScript is a very technical topic for anyone new to SEO
  - Search engines find it easier to process pure HTML versus JavaScript
  - JavaScript is great for aesthetics and features but not so good for search engines
  - https://www.linkedin.com/pulse/ultimate-guide-javascript-seo-syed-faraz-abbas-rizvi
  - There is no reason to use JS for aesthetics if they can be created without it
  - Search engines cannot do onClick() events
  - Not using JS properly can result in Google thinking that a site is trying cloaking
    - https://en.wikipedia.org/wiki/Cloaking
    - Using cloaking will have consequences for a site's SERP rankings

Black Hat SEO vs White Hat SEO
  -
  - Black Hat SEO describes optimisation techniques that go against guidelines
    - Techniques that follow guidelines are called WhiteHat
  - Avoid Black hat techniques like the plague
    - They are usually only successful for a short period of time
    - The penalties that a site will receive often far outweigh any benefits
  - Google updates the search algorithm several times a year
    - Each time there are winners and losers but not every loss is a penalty
  - Greyhat is a term which describes techniques which are not against search guidelines but do contradict ethics
    - Link building can be an example of Grayhat activities
    - https://developers.google.com/search/docs/essentials/spam-policies?hl=en&visit_id=638114020697901476-3303849116&rd=1#link-spam
    - Technically anybody reaching out for links can be seen as greyhat but as long as honest efforts are done there is little risk
<br /> <br /> <br />
  
<h1><p align=center>SEO Strategies </h1><br/>

How to identify your SEO strategy
  -
  - Identifying an SEO strategy early is important so that you are aware what needs to be done with a site
    - This should take into account any challenges that there are within the business
  - 4 common pillars of strategy to maximise results
    - How your site currently performs, what are its strengths and weaknesses
      - This shows where is performing and where performance is not meeting expectations
      - Without knowing where a site is at the moment performancewise it will be very hard to know where to target improvements
    - Who your real competitors are
      - When working with businesses they should know who their competitors are
      - In this case the real means competitors in the SERP's rather than shops etc
    - What are the goals of the business
      - This is something that often be overlooked by businesses
      - The purpose of SEO is not just to rank high for certain queries
      - If this cannot be measured against business goals then calculating returns on any investment is going to be tough
      - Everything that will be done should always start with understanding what purpose it serves
    - Identify who the key enablers are
      - SEO is not something that is easy because there are many different elements
      - Find the things that will drive results as fast as is possible
      - An SEO strategy is usually a 6 to 12 month exercise
  - Start thinking as early as possible about SEO
    - This should include yourself and how you will approach things
  - The one size fits all formula does not exist
    - There are multiple factors that will affect your strategy
    - The strength of a site such as its size, authority, reputation and others
      - Is the site a new one or has it been online for several years
      - There should be as much understanding of these as possible as they will affect SEO strategy and it they are wrong so then will be the strategy
    - The industry that a size is involved in
      - Different niches will have different strategies as what works in another industry will not work in yours
    - The Competition Landscape
      - Some niches will have low volumes of queries
      - If competition is low this can still work well
      - Again if there are low volumes for searches in a niche this will have an effect on SEO strategy 
    - The site market (Is it local or international)
      - Is the main part of where a site is selling to located locally or will there be shipments abroad
      - Understand where your market is today as well as how it is likely to evolve otherwise opportunities may be missed
      - This covers a period of the next 6 to 12 months
    - The type of site that a site is such as whether its an e-commerce site, blog, forum etc
      - Each of these type of sites comes with their own unique challenges
      - E-commerce sites for example can be very challenging to put together an SEO strategy for
      - Blogs that have lots of comments, guest posts etc and other type of content need to be fully understood
    - What are the business goals
      - How much brand awareness or how much revenue is expected from organic search will affect SEO strategy
      - Not having an initial expectation or initial goals may not be a bad thing as it will give time for proper early discussions
    - Budget and Resources available
      - Simply put the more resources available then the more options are available
      - One example is having access to developers or specialist SEO experts eg Backlink specialists
      - Being in a smaller business without access to some of these is not necessarily going to prevent success
      - SEO is not tied to how much money can be spent on an SEO strategy
  - The course structure
    - Improving SEO skills
      - The course will drastically improve SEO skills as well as confidence
    - Answering what the 4 pillars of SEO are
    - Maximise the chances of success and the ROI
  
SEO Roadmap Template
  -
  - Downloadable template

Creating your mid-term SEO strategy
  -
  - Quick review of the Roadmap Template which will be updated by the student as the course progresses
    - The 6 - 12 month timeframe is envisaged
  - Examples of Situations
    - Where a site is new or has low visibility
      - There are not any magic formulas but only guidelines and pointers
      - Focus on content as if there is little content it will be very tough to bring traffic
        - Look within the niche to see what the audience is interested in or talking about
        - If running an ecommerce site it maybe worth starting a blog around the niche your site focuses on
        - Make sure that the content is properly optimised and uses keywords that will convert
      - Go after low volume and low competition keywords
        - New sites with low visibility and low authority will not be able to go after competitive keywords
        - Newer sites will probably be better targeting low volume long tail keywords
      - Structure the site properly
        - Being low visibility gives an opportunity for information architecture to be implemented correctly
        - Make sure that everything on site flows well for users as well as search engines
        - While a site is still new it means that changes needed can be made much more easily 
      - Quality over Quantity
        - Its better to have less than 50 pages that are well organized than 1000's that are not
        - Make sure that the small number of pages are keyword optimised and offer great value to users
        - Remember that thin content does not bring any value
    - A site is up against fierce competition
      - Most industries are very competitive today, any that aren't usually will be within a couple of years
      - Focus on sub-niches when it is possible
        - If there is a specific part of the niche which has good demand then that should be focused on
      - Identify the weak spot of competition
        - Are there things that competitors are not doing well that you could focus on and do better
        - Breaking dow things may show pockets of opportunities when you can get ahead despite the fierce competition 
      - Set the right expectations internally
        - Stakeholders may not appreciate how much effort is involved when competition is very high
        - It is the responsibility of the SEO to set those expectations correctly
        - Demonstrate that you have taken into account the high competition to give confidence to stakeholders and avoid disappointment
      - Go after highly targeted keywords
        - These will convert well because the volume is low
        - Even with fierce competition there will be room for you if using the correct long tail keywords
    - A site that does not have a strong backlink profile
      - Backlink profiles are always difficult because it will be an area where a site is not in control
      - A high quality keyword can be better for a site than lots of low quality backlinks
      - Identify the gap in the backlinks profile
        - Find out how big this gap is, look at competition and how many backlinks that they have
        - See which are genuine backlinks and how quickly these backlinks were acquired
        - Is there a process that the competition uses to acquire backlinks
      - Outsource digital PR (If possible)
        - If resources are available then it maybe possible to work with an agency
      - Improve and optimise internal linking
        - It acquiring backlinks is proving very challenging then one thing to do is improve internal linking
        - Although they are not backlinks internal links can be very powerful as they help search engine crawlers
          - Optimizing with anchor text will help to improve SERP ranking
        - This approach can bring very good results so might be a good place to focus some of an SEO strategy on
      - Have a process in place which makes you consistent
        - Many businesses are not consistent and tend to work in waves
        - Instead of not doing anything for a period and then trying to rush instead try and do some work regularly even if it is small
        - SEO can be a lot about consistency
    - A site has technical issues
      - Fix in priority can be usually quick wins
        - Sites for example that are fast on desktop but slow on mobile can be first for fixes
        - Google rewards sites that are quick on mobile devices so getting issues fixed quickly can be rewarded quickly  
      - Migrate the site
        - Changing the website hosting can be a fix for several issues 
        - This can be a stressful time but it is often something that is better to get done rather than leave it until later on
        - If there are too many issues that are not being fixed on present hosters then it may be the only answer
      - Outsource if it is needed
        - If the knowledge is not available in-house to fix things then contracting out maybe the best option
        - This can in some circumstances a one time expense
      - Prioritise the issues
        - Not all issues can or will need to be fixed all at once 
        - The main thing is that issues are identified so that they are at least recognised
      - Monitor any fixes
        - Have processes in place that monitor fixes to ensure that the same issues do not reoccur 

Producing Your SEO Roadmap
  - A run through of the different elements of the SEO roadmap template
  - Tips and Best Practices
    - All sites evolve as do users and search engines
      - User behaviour and search engines are changing significantly 
      - Google for example releases core and minor updates on a regular basis
    - The competitors of today may not be the ones of tomorrow
      - Newer competitors may enter the market so there is a need to be proactive in understanding these   
    - SEO is an ongoing processes
      - What maybe successful for a site now may not be in the future
      - Doing nothing to prepare for these changes will see a site's rank fall over time 
  - Key Takeaways
    - SEO strategy should have answers to the 4 pillars
    - Update the SEO roadmap as the course progresses
    - Be consistent
    - Focus on getting the quick wins first
  
<br /> <br /> <br />

<h1><p align=center>Site Audit : Checklist & Requirements </h1><br/>

Why Audit your site and how often should you do it?
  -
  - Downloadable resource
  - The document covers in some braod terms some of the things needed in an SEO technical audit as well as what to look out for
 
Audit Requirements
  -
  - What is a new client checklist
    - This is a document provided to clients with checklists to fill out and questions to answer
    - It allows for understanding of a client's industry as well as asking for certain things which allow for sites to be audited
    - There will be multiple things asked for in a client checklist
  - What should the new client checklist contain
    - There should be access to a number of tools such as GSC, Google Analytics, Bing Webmaster Tools, Google Tag Manager
    - Information about the site such as Internationalisation, Domain History and any major competitors
    - Detailed technical information about the site such as crawling permissions and their speed, user agents to whitelist as well as any pain points
      - User agents may be provided to the client so that they can be whitelisted to avoid crawling being blocked
      - Pain points should also be disclosed in case the client knows and already has fixes being processed 
    - Log Files to determine which pages Googlebot is visiting vs user vising
      - These can be used with other technical findings 
    - Some wider business questions such as company objectives within the next couple of years
      - What is the Unique Selling Proposition (USP)
      - What is the core company mission and what are it's core values  
    - Marketing such as what campaigns are currently ongoing
      - What campaigns have been ran previously
      - Has there been an SEO agency been used before and if so then what did they do 
    - Having all of the above answered in not completely necessary but it is a good thing to have
  - What we need to perform a technical audit of a site
    - Access to the site and permission to crawl
      - This is needed to understand the site and the technical setup   
    - Google Search Console (GSC) and Bing Webmaster Tools
      - This is Google's interface with webmasters, Bing tools should also be used if the client has them
      - Google Search Console will be sufficient if Bing is not available
    - Google Analytics
      - This will make available in-depth data for the client's site 
      - If there are other packages available such as Adobe Analytics that would be good but not absolutely necessary
    - The specific site pain points that the site owners are aware of
    - Domain History such as if the domain was owned by someone else
      - If the domain was recently bought or the client has other domains is valuable information to know 
    - Log Files have an abundance of information for performing a technical audit
  
Enabling & Configuring Google Search Console
  -
  - Search Google for 'Search Console' and this is the first result that will be found
    - https://search.google.com/search-console/about 
  - It is an interface between you and Google for reporting on your website
  - It can really help improve site performance within the SERP's
  - GSC offers reports on multiple areas
  - Users will need to add a domain manually
    - There are also instructions for adding GSC to your DNS provider
    - The whole domain does not have to be tracked, the URL prefix option allows for adding GSC to a certain part of the site
    - It is important when using the URL Prefix option that the canonical version is use
  - After the site has been added then it must be verified so that GSC can start receiving data
    - One option is to download a html file and add it to your website
    - Another option is to use Google Tag manager and add a tag to your site 
  - In order to keep GSC working it is important not to delete either the tag or html file which ever was added to the site
  - If GSC has already been added to a site then you will need to be added as an admin
    - Users can then be added with specific permissions 
  - Getting added by the site owner as a contibutor is much easier than verifying a site
  - Fetching a url for the first time may take a couple of minutes
  
Enabling & Configuring Google Analytics (GA)
  -
  - Search for Google Analytics and the first result will be the analytics site
    - https://analytics.google.com
    - Users will have to be signed in to their Google account 
    - Google Analytics is a free tool
    - It provides a lot of analytics in one place which allows for organisations to make smarter decisions
  - Why you should use Google Analytics over a different provider
    - Google Analytics has a lot of room in the free tier, approximately 10 million hits per month
    - In some instances Analytics 360 which is paid might be a better option
    - If you go over the limits of Google Analytics then you might start getting sampled data
    - Unless your site is getting an enormous amount of traffic then GA is the perfect tool
  - Enabling Google Analytics
    - Simply click the start measuring button
    - This takes you to a create account window where account details and other things will need to be entered
    - Try making the account name something that makes sense
    - You can track websites, mobile apps or both together
    - Once successfully set up, you should be presented with a gtag.js tag
      - This is the tracking code for the site   
      - This tag has to be in the head section of every page that you wish to track
      - Different platforms such as WordPress will have their own ways to allow this tag to be added eg the tracking id can be added in WP
    - GA has multiple customisable filters and settings to allow for fine grained data analysis

Technical Audit Checklist
  - 
  - Downloadable text resource
  - This is an Excel spreadsheet allowing for checking for site issues

Working with your Audit Checklist
  -
  - What is Technical Auditing
    - Technical auditing is the process of looking at a site from a technical perspective
    - We will be looking for any technical issues or problems
    - There will not be any looking at backlinks or keywords
    - If any issues are found they should be documented
    - There can be 3 stages within the documentation process
      - What is the issue that is occuring
      - Why this issue is an issue in the first place
      - How can this issue be fixed 
  - Why do we audit
    - Audting is a vital part of any SEO initiative
    - It not only highlights issues but guides strategy over the next 3 - 12 months 
    - There is little point in working on other SEO items if a site is not technically sound
    - Sites with a solid technical foundation help search engines understand a site better
    - Sites that are better understood by tsearch engines tend to rank better
    - Auditing helps identify issues that a search engine may see and provides recommendations for fixes
  - A quick run through of the checklist provided

Task Assignment 1 - Configuring and Checking GSC
  - 
  - A practical assignment to get your site working with GSC

Task Assignment 2 - Configuring and Checking Google Analytics
  -
  - A practical assignment to get your site working with GA
  
<br /> <br /> <br />

<h1><p align=center>Technical Site Audit Part 1 </h1><br/>

All Technical SEO Material including Auditing can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md

Understanding HTTP Codes
  -

Robots.txt
  -

Checking Your Site Speed
  -

Crawling Your Site with Screaming Frog
  -

Checking Your Internal Linking
  -

XML Sitemap
  -
  
Task Assignment 1 : Perform a Technical Audit of Your Site (Part 1)
  - This is a practical assignment where students will perform a technical audit of their own site
  - Use the provided Technical Audit Checklist
  - Note any areas that are being flagged as issues
  
<br /> <br /> <br />

<h1><p align=center>Technical Site Audit Part 2 </h1><br/>

Again all Technical SEO Material including Auditing can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md

Checking Legacy URL's with Majestic
  -
  - https://majestic.com/
  - This tool scrapes the web and creates a database of encountered URL's
  - The historic index contains a 5 year backlog of any URL's encountered by their crawlers during that time
  - Checking URL's
    - Ideally these URL's should respond with a 200 status code
    - Or 301 redirects which redirect to a new location within 1 hop
    - Large amounts of 4xx errors can be problematic
    - The same applies for long redirect chains
      - Follow redirect chains until the end of the chain is reached
      - Alternatively only follow for as many hops as is of interest
      - These long chains should be flagged as an issue
      - The same will apply to any errors that are encountered
      - Any URL's with backlinks should be of particular interest 

Canonicals
  -
  - Using Screaming Frog to perform checks on canonical links
    - Also use the coverage reports section of GSC
    - Searching for excluded URL's can yield benefits 
  - Simple checks can be done using devtools and searching the DOM for the phrase 'Canonical'
    - If it is not present in the DOM or source code then it may need further investigation
  - Paramaterised URL's that canonicalise elsewhere are fairly standard practice
  - Paginated pages that canonicalise elsewhere are not good practice and warrants further investigation
  - Site canonical hygene is important and should not be left to google to decide
    - Google can on occasion can select the wrong canonical tag 
  
Pagination
  -
  - Rel=next/prev used to be the recommended way to handle pagination
    - It was used to denote logical pagination structure 
    - In 2019 Google anounced that it had not been using this method for a while 
    - Although not necessary any longer, it can be harmless if implemented correctly if left in HTML
    - When incorrectly implemented rel=prev/next can cause erros such as bot traps
      - https://www.contentkingapp.com/academy/crawler-traps/ 
    - Bot traps are often seen when crawlers are sent down a non-existent chain of URL's linked through rel=prev\next markup
  - To search for any instances of rel=prev\next
    - Open up dev tools on a page and search for next which will find any occurences of the word 'next' in the DOM including tags 
  - Canonical Tags
    - Check that the canonical tag is implemented properly when using pagination
    - Each page in a paginated series should have a self referential canonical
    - Often seen is page 2+ of a series canonicalising back to page 1 which is bad practice
  - Nonindex Tags
    - Pages in a series are supposed to be indexable
    - It used to be considered good practice to noindex pages 2+ of a paginated series
    - A noindex on a page for extended time can result in the page being no longer crawled 
  - Load More
    - Often 'Load More' buttons are used instead of numbered pagination
    - If the button does not contain a real link it will not be properly crawlable
    - Check the DOM to identify whether it contains any <a href> links to further content sections 
    - Inspect the button in the DOM
  - Infinite Scroll
    - A similar issue exists with infinite scroll
    - JS events which are often uncrawlable are used to load content after a certain point in the page
    - Pages with infinite scroll should support paginated loading with unique links to each section
    - The DOM can be searched for what would be the next logical URL in the series
  
JavaScript
  -
  - JavaScript heavy sites
    - When dealing with JS heavy site it is important to identify whether links are buried in content that relies on JS
    - One way to do this is to use a Chrome extension to disable JS and compare by observing how the page looks
    - Also take the time to compare the DOM to the page source
      - The DOM is the rendered version of the page, page source is the HTML
  - Crawling JS heavy sites
    - A JS heavy site will affect how the site is crawled
    - Using a tool such as screaming frog, run 2 crawls, 1 for text only and the other using JS rendering
      - This will show the impact that using JS has had on a site 
  - SPA or Single Page Applications
    - These are JS web apps which use JS to dynamically update instead of loading new pages from the server
    - When they are done correctly they will be indistinguishable to users
    - Under the hood SPA's are very different to normal web pages and can cause SEO issues
    - SPA's have become reknowned as being bad for SEO
    - Since google switched to Evergreen Chromium in 2019 the issue has lessened
      - https://searchengineland.com/evergreen-googlebot-chromium-rendering-engine-316652
      - This is because from this point Googlebot could render JS
    - Other search engines may not be able to render JS
    - Relying on Google to render a large amount of JS pages is not efficient
      - JS heavy pages can take longer to index due to having to render a large amount of content
  - Pre-rendering
    - Pre-rendering is a strategy for ensuring that JS heavy sites can be crawled by search engines
    - Pages are rendered and then cached server side
    - The cached version is then delivered to search engine user agents
      - Users will see the live version of the page
    - This means that bots will see a slightly different page to users
      - This should not be a problem if the cache is updated regularly
    - Again Screaming Frog can be used to check if the cached pages and user page differences are an issue
  
What is AMP and how to test AMP pages
  -
  - AMP is an acronym standing for Accelerated Mobile Pages
    - https://instapage.com/blog/amp
    - These are pages that use a special stripped down HTML framework
    - It was created to make things faster on mobile
    - It strips non-essential components and sometimes replaces JS with AMP library components
  - AMP Setup
    - All AMP pages must contain some boilerplate code in the head tag
      - https://amp.dev/documentation/guides-and-tutorials/learn/spec/amp-boilerplate
      ```
      <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;
        -ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start
        {from{visibility:hidden}to{visibility:visible}} @-moz-keyframes -amp- start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes 
        -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}} @keyframes 
        -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript>
      <style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
      ```
    - Normal HTML tags can be used on AMP pages, there are some AMP specific alternatives though
        ```
        <img> is replaced by <amp-img> 
        <iframe> is replaced by <amp-iframe>
        ```
    - A number of HTML tags are prohibited
       ```
          <embed>
          <frameset>
       ```
    - AMP pages must be setup as counterparts to non-AMP pages
    - AMP pages need a rel=canonical tag pointing to the non-AMP version
  - AMP Restrictions Javascript
    - Because AMP needs to be streamlined there are restrctions on what developers can use
    - Third party JS is banned other than in iframes
      - It can be loaded in sandboxed iframes using the following tag
            ```
            <amp-iframe>
            ```
      - This is done so that the critical rendering path is not blocked
    - Custom JS must be loaded asynchronously, again to stop the page rendering from being delayed
    - There is access to a library of AMP components
    - AMP components use JS under the hood, it is not editable however
    - The following tag captures analytic data from an AMP document
      ```
      <amp-analytics>              
      ```              
    - These components are to be used where there are gaps due to JS restrictions
  - AMP Restrictions Images
    - Images must have a predetermined size and position which is stated in the HTML
      - This is called static layouting
    - The layout attribute can be used to make images responsive
    - The following tag must be used with images
      ```
        <amp-img>
      ```
    - This tag allows the AMP renderer to understand the page layout prior to assets being loaded
      - It also relies on JS so a fallback img tag should be provided within a noscript tag
      - Only using the img tag will results in images being rendered in the browser but not validated as AMP
  - AMP Restrictions CSS
    - Stylesheets in AMP are limited
    - External stylesheets are prohibited
    - Styles can be either inline or internal which is created in the document head section
    - Only one internal stylesheet can be included on each page
    - There are some disallowed styles such as the use of a reference to the !important qualifier
    - Stylesheet size must remain below 50kb
  - When to use AMP
    - AMP is most often seen on contentful pages
    - There is a case to be made for use on sites with high mobile traffic      
    - AMP may also benefit certain types of pages such as news articles       
    - Only AMP pages can be used in Google News Carousel          
  - AMP Benefits 
    - AMP reduces the number of requests so reduces page load time
      - The critical rendering path does not get blocked by resource downloads
    - To speed up AMP pages, Google crawls and caches them         
    - Pre-rendered AMP pages are served in mobile search results          
  
Manual action checks using GSC
  -
  - How to check for manual actions in GSC
  - Manual Actions
    - These occur when Google guidelines are breached
    - These breaches can be either intentional or unintentional          
    - These are submitted after a human person has deemed a site to be breaking guidelines 
    - They are found in Google Search Console        
    - Once issues are resolved a resubmission request can be submitted to GSC
    - The resubmission request should contain some items
      - Exact details of the issue and the steps taken to amend it
      - Actions that have been take to stop the issue happening again
  
Index Coverage, Sitemap, Speed, Links using GSC
  -
  - An examination of various reports in GSC
    - Index Coverage, Sitemap, Speed, Links reports are covered
  
Task Assignment 1 : Perform a Technical Audit of Your Site (Part 2)
  -
  - This is a practical assignment where students will perform a technical audit of their own site
  - Use the provided Technical Audit Checklist
  - Note any areas that are being flagged as issues
  
<br /> <br /> <br />

<h1><p align=center>Producing a technical backlog </h1><br/>

Interpreting your crawl data
  -
  - How can we investigate crawl data
    - Google Sheets is used in the example and a number of different reports can be filtered and viewed   
    - Data can be sliced as much as is needed to extract as many trends as possible 
    - The same data used in the technical audit module is used as part of the demonstration          
  - Quick Wins
    - Using the example dataset there are some quick wins available, these should be taken where possible
    - Redirection of trailing and not-trailing slash versions of websites is relatively is easy and could be a quick win
    - Removing title duplication could be seen as another quick win
    - There would have to be enough of these to be seen as a priority          
  - How to present the data
    - Data should be presented in a way that means something to stakeholders
    - This can include things like graphs or summary sheets
    - Summary points within audits and backlogs can help summarise issues          
  - How to add issues to the backlog
    - Issues should be added to the backlog with areas given particular attention
    - The title of the issue
    - The type of the task and whether it is a technical focused task
    - An issue description with as much detail as required for understanding 
    - A brief solution to the issue, it should have as much information as necessary to understand the solution
      - There can be a link to further documentation if it is needed            

Prioritising your tasks
  -
  - How can tasks be prioritised
    - Tasks that are added to a backlog or provided to a client or internal stakeholder should be prioritised
    - This approach ensures that the most important tasks which could cause damage are tackled first
    - Minor tasks can be identified and completed at a later time
  - Critical, High, Medium & Low level Tagging
    - When a technical backlog is being produced eask task should be assigned a priority
      - Critical which is when an issue needs to be resolved as soon as possible
        - If this issue is not resolved there will be a hit to SEO performance
        - One example is when there would be a noindex tag on a site homepage
      - High for when an issue is not 100% urgent but will have a positive effect when fixed
      - Medium is when an issue is not as critical as high, resolution may provide some benefits
        - Some issues maybe tricky to fix and may not be that beneficial to fix
      - Low is for when a task would be a positive to fix but offers no immediate effect on SEO performance
  - Impact vs Ease of Change
    - When tasks are being prioritised there are a number of factors to look at
    - 2 of these factors are Impact and Ease of Change
      - There will need to be identification of what sort of impact will happen if a task is completed
      - As to be identified is the ease of completion of a task
    - Understanding the scale of issues is very important
  - Uplift
    - This is usually in the form of a high, medium or low estimate in the uplift in traffic
    - This can usually be done by experience or working with large datasets
    - What usually improves technical performance will be classed as high, otherwise is will be classed as low
  
Tips to produce a backlog using a template
  -
  - Text based hints on producing a backlog
    - Give tasks a concise title
      - This makes it easier to refer to in communcations
    - Issues and Solutions
      - Backlogs should be usually be in the format of a spreadsheet
      - This will allow for columns for issues, explanations and solutions
      - Issues and solutions should be concise
    - Explanation
      - Explanations are usually included as teams using the backlog may not have been present during creation
    - Prioritisation
      - People who may not know SEO might need to see the pirority of tasks
      - Flags should be used to indicate priority levels as well as being put into order
      - Quick win tasks should be marked out as such
    - Task Allocation
      - Tasks should be appropriately assigned as multiple teams may be using the backlog
      - Clarity is very important here as it stops confusion or more than 1 person working on the same thing
      - Accountability is also improved here as people will know what is expected of them and where to raise any issues
    - Audit Resources
      - Any resources collated during the audit process should be linked within the backlog
      - It can be things like exports from software which are linked to the pages that are needed to be fixed so that time is not wasted searching for things
    - Task Status
      - Everybody involved should be aware of where the issues in the backlog are in terms of progress
      - There are 3 categories, Not Started, In Progress and Completed
      - The actual titles are not that important as long as the meaning is clear
    - Templating the Backlog
      - There will likely a need for multiple audits so backlog reuse is a maybe
      - Create a backlog template that can be used each time will ensure consistency in deliverables
      - Explanations for expected issues should also be included
      - Auditing may at first create best practices but that may not last so hold on to any meterial on recurring issues
          
Technical Backlog Template
  -
  - Downloadable spreadsheet file provided    
  
Task Assignment 1 - Create Your technical backlog
  -
  - Use the template provided to help create a detailed technical backlog 
          
Task Assignment 2 - Work with developers to understand the effort needed on each task
  -
  - Work with people who are responsible for different tasks

<br /> <br /> <br />

<h1><p align=center>Benchmarking Against Competitors </h1><br/>

Finding Your Competitors
  -
  - Why look at competitors
    - Looking at competitors and what works for them can help find opportunities for any business and it's website
    - Research should look at
      - What keywords they are using and how it is affecting their ranking
      - What type of content works for them and how you could replicate it for yuor website's needs
      - Where backlinks are coming from and where those backlinks are sourced          
  - Industry Competitors
    - These type of competitors are usually in the same industry as you
    - If your potential customers have you in mind then they will likely have your competitors too          
  - Search Competitors
    - Looking at organic search to see which sites are ranking well for your target keywords will show search competitors
    - Search competitors will not always fit into a single market or niche
    - These can be found by using a few methods
      - Use tools like SEMRush to see which domains are ranking for those target keywords
      - Use te SERPs to see which sites are ranking well for keywords
      - Look at who is bidding for paid space for the target keywords
      - Use social media, forums and customer feedback to see what brands are getting mentioned for you keywords
      - Use YouTube to check your keywords, those channels at the top are likely to be in SERPs too          
  - Aspirational Competitors
    - This would be competitor is usually a dominant player in the market you are in and in popular with your target audience
    - This competitor will usually be in the industry competitors list based on it's brand and trust level
    - You will probably not be able to compete with them at first but it would be foolish to ignore them completely
    - They should be used more as a benchmark of where you would like to be
    - Using some of their strategies may be possible and it is important to remember that they will have had wins and losses previously          
  
Domain Authority, Trust Flow and Citation Flow
  -
  - The concept of backlinks
    - Backlinks play a significant part in SEO along with technical optimisation and keyword targeting
    - Backlink strategy is catgeorised as off-page SEO
      - This is because it is something that cannot be controlled or influence through code on pages          
    - A backlink is a link from someone else's website to our own
      - It is more aligned with PR and general marketing or using other things like Social Media to amplify content         
    - Backlinks are a signal to Google that others trust our content 
    - The more good quality backlinks in a backlink profile the stronger a site would be          
    - This will manifest itself through SERP ranking inprovements and maybe overtaking competitors
  - Why are these metrics important
    - It is important to look at competitors and see which have the strongest and highest quality links
    - This should also show the type of content attracting strong backlinks
    - A strong backlink profile should balance both quality and quantity links as these metrics are both indicators
  - Domain Authority (DA)
    - There are 2 accepted and often used definitions of domain authority
      - It is a score between 1 and 100 by Moz to esitmate how likely a website is to rank within organic search results
        - High scores indicate a better chance to rank     
        - This is not a SERP ranking factor, it is not considered by search engines
        - Domain authority is also a catch-all term used for the perceived general strength or quality of a site 
        - It is based on how many backlinks it has and how authoritative these links are 
      - Trust Flow is a metric that is used in the application Majestic
        - Majestic is a tool which can help determine how strong or authoritative a site is
          - https://majestic.com/
        - It uses a similar scale to Moz with 0 - 1 being the least trustworthy and 100 being the most
        - It measures the authority of the websites that are linking to your site
        - Trustworthy sites are the sites that are good to get backlinks from
      - Citation Flow is also a metric that is used by Majestic
        - This is used to measure how many inbound links a particular domain has
        - It is used in addition to Trust Flow
        - It is based on how the quality of inbound backlinks that a site has acquired
        - It is also measured 0 - 100 with 0 - 1 indicating the lowest amount of backlinks and 100 the heighest
    - Spam Score
      - Trust Flow and Citation Flow scores can be used to calculate a site Spam Score
      - This metric balances the number of a backlinks a site has as well as well as how trustworthy the linking domains are
      - Spam scores is the trust score divided by the citation score
      - Spam score should be 1 or higher
      - Spam scores below 1 could indicate an unbalanced link profile
        - This could be from too many poor quality links
        - https://searcharoo.com/why-low-quality-links-will-tank-your-website/   
      - Use spam scores with competitor sites too
  
Analysing Backlink Profile using Majestic
  -
  - Demo of the Majestic Tool to find citation scores, flow scores and spam scores of sites
          
Competitor backlink profile template
  -
  - Downloadable text file template
          
Performing a keyword gap analysis using SEMRush
  - 
  - Demo using the SEMRush tool
  - A keyword gap occurs when competitors rank for keywords that we don't
  - It can also be when competitors rank higher for certain keyword than we do       
  - https://back2marketingschool.com/semrush-keyword-gap-analysis/          
          
Keyword gap analysis template          
  -
  - Downloadable text file template          
  
How to spot key opportunities
  -
  - Analysing the data - what do we have
    - This reuses the example data from the previous lesson          
    - We have a single and comprehensive set of keywords that we and competitors are ranking for
    - A list of gap keywords that we would aim to rank for
    - The URL's and the positions for each of these keywords  
    - A rank order of ours and our competitors domains in terms of quantity and quality of backlinks
    - A collated list of all backlinks to our website and competitors websites          
  - Gap keywords - Irrelevant
    - It is important to remember that not all keywords will be relevant to a company
      - Discounted keywords will need to be manually removed from consideration
  - Backlinks
    - Any data collected will form the basis of a future backlink strategy 
    - Based on a spam score you will be able to see who are the main players and also the least competitive ones
    - Focus should only be on the strongest competitors and their domains
      - Look at the domains that their links are coming from
      - What is the content type that has attracted the link
    - Disavow poor quality links on your website where needed
  - Calculating the size of the prize
    - The size of the prize refers to the amount of search volume opportunity available
      - Content would need to be optimised poperly and a ranking of number 5 for each relevant query in this calculation           
    - This is based on numerous large scale studies
    - Ranking in position 5 would translate approximately to a 5% CTR
    - This means that 5% of the average monthly search volume would be for gap keywords          
  - Calculating the size of the prize - Caution
    - It must be kept in mind that CTR can vary immensely based on a number of factors and are indicative rather than guaranteed
    - Rankings and visibility with SERPs fluctute contstantly
    - SERP features such as paid ads and featured snippets can influence CTR by pushing pages further down the SERPs
    - Some industries and therefore their keywords have higher CTR than others (the same is true of lower)
  - Organic vs Paid Opportunity
    - A good way to get people invested in SEO initiative is to work out the cost for traffic grabbed by paid ads
    - What would be the cose of getting the traffic highlighted by GAP if it was being paid for
    - Again use the 5% of the search volume opportunity and multiply it by Cost Per Click (CPC)
    - The same cautions must be applied to CPC as to CTR          
  - Prioritisation
    - Quick wins here would be keywords in middle positions that coould do better   
    - Also create a content strategy to rank for these gap keywords          

Task Assignment 1 - Perform a backlink profile analysis
  -
  - Take 5 competitor website and download their historic data from Majestic
  - Use the template provided and create a table filled with trust, citation and spam scores 
          
Task Assignment 2 - Create a keyword gap analysis 
  -
  - Using the same competitors as the above exercise, create a full Keyword Gap Analysis
  - Again make use of the template provided          
          
Task Assignment 3 - Estimate the size of the 'Prize'
  -
  - Work out the size of the prize for all Gap keywords

          
<br /> <br /> <br />

<h1><p align=center>SEO Common Pitfalls </h1><br/>
          
Manual Actions
  -
  - These occur when there are breaches of Google guidelines
    - They do not occur from algorithms but are the result of people reviewing the site
    - The same goes for penalties as there is a difference between manual actions and SERP ranking penalties
      - Algorithmic penalties do not directly punish a site but instead favour competitors who better meet the algorithm targets
      - Manual Actions on the other hand are punishments for specific breaches of guidelines that have a specific cause          
    - There are several different types of Manual Actions and they have varying consequences
    - Manual Actions are rarely found by accident and are usually the result of trying to manipulate or harm users and search engines 
      - They can occasionally be from icompetence where warnings have been ignored          
  - Submitting Manual Action Fixes
    - Once any issues identified have been fixed then it is time to submit a request to Google to review fixes
    - This request should contain details of the exact issue, steps taken to amend the issue and steps taken to prevent recurrence as well as outcomes of changes           
  - Types of Manual Actions
    - User Generated Spam
      - This is guideline breaking content posted by external people posting to a site
      - Although this content is not under your control the ability to manage it and moderate it is and failure to do so will cause problems
      - If it is possible Google will take action on the affected pages
      - If the issue is present on large parts of the site it could be entirely deindexed by Google
      - A lot of User Generated Spam can be handled by improving moderation methods
      - Some methods include limiting post numbers within a certain timeframe or reviewing certain user's posts
      - Spammers often post links in comments or profiles and they should be nofollowed
      - Using nofollow ensures that there is no link equity passed on to these links          
    - Spammy Free Host
      - Web hosting services should be aware of the sites that use this service
      - Google will punish if there are things like spam or even worse malicious malware
      - Moderation will again take care of a lot of this issue
      - Make sure to use captcha to stop automated account creation
      - Log files should be monitored for any increase in redirects which may indicate cloaking techniques being used
      - Use the Google Safe Browsing API on a regular basis to test URL's using the web service
        - https://developers.google.com/safe-browsing
    - Structured Data Issues
      - If Structured Data has been use to manipulate Google Rich Results then there will be a penalty
      - Usually it is intentional manipulation that is punished but continued failure to meet Structured Data guidlines can be punished if there are ignored warnings
      - If there is a manual action for this a site can face deindexing but usually the site will be prevented from generating Rich Results
      - The simple fix here is to either amend or remove the structured data that is causing the issue         
    - Unnatural Links to a site
      - These type of links usually come from buying as part of a scheme from a spammy domain
      - They can be either a site owner or agency acting on their behalf
      - There are cses where it was competitors trying to cause issues for another site
      - Google will always penalise these if they are trying to manipulate PageRank
      - If there are any on a site it is recommended to remove them even contcting webmasters if necessary
      - Disavowing those demains may also be needed          
    - Unnatural Links from a site
      - Google will hit a site with penalties if it thinks that site links out are trying to manipulate rankings
      - Penalties are especially likely if there are signs that links were sold as part of a linking scheme
      - Identify any links on a site that break Google guidelines
        - This will included any links which may not be malicious but would be unnatural
      - Remove any links from any manipulative scheme
      - Where a manual action comes because of an unintentional spammy linking it is recommended to change them so they will no longer pass PageRank
        - This can be done with a nofollow attribute or else make them inaccessible to bots
    - Thin content with little or no added value
      - The Panda algorithm introduced penalising for thin content
        - Google will devaulue an entire site if they feel the issue is widespread enough         
      - Spammy like practices such as scraping or designing pages to rank for terms can be problematic
        - Especially if they draw in users and funnel them to unrelated parts of the site, these are called doorway pages
        - https://searchengineland.com/doorway-pages-seo-deep-dive-389786
        - https://developers.google.com/search/docs/essentials/spam-policies#doorways
      - Removing scraped content and doorway pages is recommended
      - Review content as often as possible and ensure that it offers value to users
    - Cloaking and Sneaky Redirects
      - Cloaking is where a site shows different content to users than to bots
        - Sneaky Redirects take users to pages hidden from bots    
      - Both are against google guidelines as the intention is to deceive the search bots
        - They both are methods of preventing appropriately assessing page content
      - To avoid cloaking issues assess how the site treats bots
      - Use Screaming Frog to spoof bots and check for on-page differences
      - Live tesing an inspected URL in GSC can shpw hpw Google sees the page          
    - Hidden Text and Keyword Stuffing
      - Keyword stuffing is the practice of using keywords so often in text that it seems unnatural
        - https://contentwriters.com/blog/keyword-stuffing-avoid/          
      - This is done to explicitly manipulate search engines and improve SERP rankings
      - One way to do this is to set the colour of text to match the background
        - This renders the text almost invisible upon viewing    
      - It is also possible to hide text by using CSS or set set font-size to 0
        - This is known as hidden text          
      - Another example of hidden text is hiding links on a small character without context
      - Keyword stuffing will also occur in unnatural prose
      - Removing keyword blocks from unnatural prose is recommended as search no longer works this way
    - AMP Content Mismatch
      - AMP or Accelerated Mobile pages are specifically designed to function better on mobile devices
      - Content should be similar to non-mobile pages but the source code is going to be different
      - If there is a large difference in the content served on both sites then Google will penalise
      - If there is a penalisation then review AMP pages and the non AMP counterparts
        - AMP pages should be amended to better reflect content
    - Sneaky Mobile Redirects
      - Mobile redirect are occasionally used to redirect users to a more appropriate service such as the mobile site version
        - This can be using a domain beginning with m. to indicate a mobile subdomain
      - There can be issues though with sneaky redirects which redirect users to possibly dangerous content
        - These specifically target mobile users and are often caused by malicious code
      - Use the security issues section in GSC to check for any problems
        - Another way is to use SEO Spider with user-agent set to Googlebot
        - This crawl will pick up and resources used and any redirects present which allow for assessment          
      - Manual spot checks should be done using mobile device emulation in Devtools
      - Any sneaky redirects should be removed, removal of third party code in it's entirety is recommended          
    - Pure Spam
      - Pure Spam is any intentional combination of the above techniques
      - Usually a site will receive a manual action when there is not doubt that it is deliberately engaging in practices against Google guidelines
      - If hit with a Pure Spam manual action the it is best to audit all of the above techniques and resolve quickly          
  - Impact of Manual Actions
    - Impacts of manual actions will depend on how severe the guideline breaches are
    - If it is only a few pages then it is likely only those pages that will be penalised
    - If however the breach is across an entire site then the entire domain will be penalised and maybe deindexed
    - A manual action may not be a big deal in some cases eg Schema Manipulation which results in Rich Results being hidden rather than deindexing
          
Google Core Updates
  -
  - Panda
    - This algorithm targets thin or low quality content
    - It is a common pitfall for sites to have content that is targeted by this algorithm
      - Sites with no results pages, this occurs when searching on a site and nothing appears but the page is indexed
      - Low word count articles which are usually sub 100 words
      -          
    -           
  -          
          
Using IFrames and their impact on SEO
  -
  -
          
Canonical Tags
  -
  - Again all Technical SEO Material including Canonical Tags can be found at
  - https://github.com/marb61a/Course-Notes/blob/master/Misc/SEO/Technical SEO.md

Keyword Cannibalisation
  -
  -
          
