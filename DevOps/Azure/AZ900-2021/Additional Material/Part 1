                    AZ900 Microsoft Azure Cloud Fundamentals 2021
                    Additional Material Course Notes Part 1


Azure Storage Account Overview
  - Azure storage accounts provide cloud storage, whether you interface directly with the storage account using the GUI
    - Or command-line tools to put content into the storage account, so files or to retrieve them
    - But you might also have other services like a web application that talk to the storage account in the background
    - Maybe to read or write data, including perhaps even writing to logs stored in the storage account
    - Storage accounts can accommodate blobs, binary large objects, whether these are office productivity type of files
    - Like spreadsheets, or Word processing documents or whether they're virtual machine hard disk files
    - Storage accounts can also accommodate storage queues
  - This is going to be of interest primarily to software developers because they could use the queue 
    - As an intermediate messaging mechanism between software components
    - So that the software components don't all have to be running at the same time to exchange messages 
    - Because we've got a queue that can store it until a component becomes available
    - We can also configure Azure file, Azure files are file shares, they're SMB type of file shares
    - For example, that you might connect to or map to from Windows, from the UNIX and Linux perspective
    - When you mount that remote file system locally, it would happen over NFS, we can also work with table storage
    - In storage account, Azure table storage is NoSQL type of storage where we have key attribute stores to store data
    - It's considered to be a less rigid schema than you would find with a standard SQL type of database 
    - Where every single field within a table is defined with the data type and a length and so on
  - Azure storage accounts support many features, one of which is replication for high availability of the data
    - For example, LRS or locally-redundant storage means that within a locality, such as even within an Azure data center in some cases
    - You will have three copies of the data. But, if there is a regional problem or outage that data is inaccessible, it doesn't exist anywhere else
    - But it would if you use one of the variants of geo-replication. So when we talk about geo-redundancy, GR, we have GRS verses RA, read-access GRS
    - The difference is that with read-access GRS, you don't have to initiate a failover to make the secondary copy usable
    - It is automatic, but with GRS, either Microsoft or you, the cloud tenant, have to initiate a failover to use that secondary copy of the data.
  - You can also host static web sites through an Azure storage account. What this really means is that you've got a static site where the content isn't changing often. If you need a lot of more complex and intricate support for dynamically generated server side pages and so on, then you should look at actually deploying an Azure app service, which is designed for that sort of thing. You can also integrate Azure CDN endpoints, that's content delivery network endpoints, into your storage account.
A content delivery network is interesting in that the purpose is to copy content. So think of web site content for example, PDF documents, sound files, video files, graphic files, whatever it is that's on the site, you can copy that to alternate locations around the world to put that content close to users that will access them, so it improves the user experience. So that can be configured within the properties of a storage account as well. Just like Azure Search can, if you want to enable search indexing for storage account contents. You can also configure blob lifecycle management. So for example, after a certain number of days, you might determine that some blob files should be automatically archived. Hardening a storage account means applying security mechanisms to essentially reduce the attack surface.
One way to do this is using our back roles for other Azure administrators to limit their access to storage accounts. For example, you might want an Azure administrator to perhaps be able to upload content into an Azure storage account but not manage the account itself. And so there are different roles, role-based access controls or RBAC roles available for that. Every storage account has two storage account keys. And you can, at any point in time, regenerate either of those two keys.
Now there are two keys because first of all, the key gives access to everything in the storage account. So you want to safeguard those keys. And you would use them for programmatic access or for command-line access, maybe if you've scripted something. Now the idea is that you can refresh keys periodically, which is a security measure. Instead of keeping the same key, you change it every now and then. You've got two keys, so while one key continues to be used programmatically or by scripts, the other one can be refreshed and then after period of time, you can do that for the first key, you can refresh it.
Bear in mind, you'd have to change any references to the keys when they both become refreshed. Instead of giving access to the whole Azure storage account through a storage account key, you could also generate a shared access signature for the whole account or for an individual blob in the account. This provides limited access to content in the storage account. And it can even have an activation date and time and an expiration date and time. So it can be controlled in that manner as well. You can also limit which networks that have services or users perhaps issuing commands, you can limit from which networks access to the storage account is allowed.
You can configure storage account access policies to limit exactly what can be done, such as perhaps only having the ability to read messages from a storage queue in the account as opposed to doing anything else with those messages. You can designate custom encryption keys under your control to encrypt the storage account contents. The default is that Microsoft-managed keys are used. So encryption is enabled by default, it's just that the keys aren't entirely under your control, but they can be.
  - Now, other than using graphical tools like the Azure portal and Storage Explorer to interface with storage accounts, you can also use PowerShell cmdlets
    - Here we have an example of creating a storage account using the New-AZStorageaccount cmdlet
    - Where we specify a resource group name to deploy the account into, a name for the storage account, a location
    - And the SkuName which specifies the type of replication. In this case, standard, locally-redundant storage or LRS
    - You can get a list of storage accounts with Get-AzStorageAccount.
    - You can also use the CLI to manage Azure storage accounts such as creating them.
Here that's been done with az storage account create where the name of the storage account is specified, the resource group affiliation, the location, and the sku.
  - We can also list storage accounts with az storage account list. Here I'm adding the --query parameter and telling it I want to query on the .name property
    - The open and closed square bracket identify that we have an array of storage accounts, more than one
    - And that we want to gather the .name property from that array.
  - We can also show detailed information about a specific storage account with the az storage account show
    - We give it the name of the storage account, and the resource group association, notice in our last example, I'm using --resource-group.
  - Alternatively, so instead of that, you can also choose to use -g as in our first example to refer to the resource group
    - Really doesn't make a difference which one you use.


Storage Accounts and the Portal
  - Using the portal to create an Azure storage account.  
  - As the name implies, an Azure storage account is used for cloud-based storage
    - Whether you want to store files used by a web app or files that users upload from their desktops
    - Or whether you want to set up shared folders or whether developers want to create message queues to allow communication between software components
    - There are so many uses for Azure storage accounts, first of all in the portal, click Create a resource
  - Type in storage account, and I'll choose Storage account from the resultant list, and I'll click Create. There's a lot of details you have to consider when you deploy a new storage accounts, you have to have planned for this ahead of time.
So for starters, I need to deploy this into a Resource group. So I'm going to choose an existing one called Rg1, although I could choose to create a new one.
Remember that resource groups are used to keep items that are related under one management unit, and that would be the resource group, whether it's for billing purposes or tagging purposes, that type of thing. I will scroll down, and I'm going to give the storage account a name. Now, if I just put in a generic type of name like storacct, it says that name is already taken, not by me but by another Microsoft Azure tenant. The name needs to be unique. So as long as I adhere to organizational naming standards, I'll be able to specify another name. So I've specified a name that is unique. We see this because the red text is no longer visible and we have a green check mark to the far right end of the field.
So what next? We have to specify the geographical location where we want this storage account to reside. I'm going to leave it on Canada Central. You ideally want to place this storage account as close geographically as possible to where you think it will be most accessed from, if that's possible. They might be accessed from many different worldwide locations.
In which case, we might then start thinking about using a Content Delivery Network or a CDN. But in any case, I'm going to put this in Canada Central. For the Performance option, it's set by default to Standard. And then down below, we have Account kind, storage v2 or storage v1. That's an older version of a storage account. You'll only use that if you absolutely had to, for backwards compatibility reasons. You could also specify that it's to be used for BlobStorage, specifically binary large objects. So these are essentially files that would be stored in this cloud storage account, kind of as a cloud-based repository.
And then down below, we've got Replication options. So Locally-redundant storage (LRS), means that you would have three copies of your data but not spread out across geographical regions within one locality. Geo-redundant storage (GRS), takes that a step further, so beyond your three LRS, local copies of data, you would get an additional copy replicated to a secondary region. So, if there's a problem the primary region, failover can be initiated so that the data is accessible in the secondary. If you don't have to worry about initiating a failover, you could use Read-access geo-redundant storage. However, if I were to switch up at the top here, the performance from Standard to Premium, notice that from the account kind drop-down list, things changed a bit. So we have BlockBlobStorage.
So, if we're going to be dealing with Blob storage items of files and we want to optimize it for the best possible performance for read and write, we could choose BlockBlobStorage. If we're going to instead going to be using this cloud storage account primarily to host shared folders, so Azure file shares, then we could choose to optimize it for FileStorage. I'm going to leave it on StorageV2.
Now, since we switched it from Standard to Premium, you'll also notice that down to Replication, the options are gone. Where they go? They don't apply when you've got the Premium performance selection, only Standard.
So all we have is Locally-redundant storage. Having done that, I'm going to click next for the networking options. 
The networking options for the storage account determines how accessible it is from a variety of different types of networks such as public based networks, so all networks with selected networks, or only privately. I'm just going to turn on Private endpoint and click next. Then I've got a number of options such as whether secure transfer is required to and from the storage account.
That's Enabled by default. Whether I want soft delete for blobs, so they can be recovered. That's Disabled by default, but it could enable it. And these options are changeable after the fact, as well. So I'm just going to accept those and click next for tags.
If this is going to be tied to a project, let's say Project ABC, then maybe I'll tag it with a Name of Project and a Value of ABC. You don't have to do this.
You can add multiple tags, you might tag it for billing purposes, or by department, it doesn't matter. But in this case, Project ABC, I'll click next to review and create the storage account. So the validation is passed based on my selections throughout the wizard. This is good.
I'm going to go ahead and click Create to create the storage account in resource group 1. After a moment, it says our deployment is complete, so we can click Go to resource.
We can also navigate, for example, in our left-hand navigation bar to view our Azure storage accounts from there. Of course, we could also go to the all resources view and see them. And there's our storage account.
Now, if I click on it to open it up, we'll see it opens up the navigation bar, and by default, I'm placed in the Overview blade. So I can see the Location of this storage account, I can see the subscription it's tied to.
There's the tag I added, Project : ABC. And as I scroll down in the navigation bar, I can see I have Settings such as Encryption settings for the specific storage account.
It's defaulted to using Microsoft Managed Keys to encrypt content stored here. I could click on Configuration, for example, if I started to maybe decide I want to change something such as Secure transfer required, and so on.
As I scroll down, I also see a section for blobs. So Blob service, including Azure Content Delivery Network, CDN. And finally, then I see my Monitoring options and so on.
Now, depending on the type of account that you create, how you create it, the choices you make when you're creating them, will determine some of the items that you see here under the navigation bar. So for example, notice all I see here is Blob service. Well, what about Azure files, tables, and queues? Well, let's create another account to examine that further. So I'm just going to click on Home here, I'll click Create a resource, we're going to create another storage account. So let's create a second storage account here.
And this time we're going to make a few different selections, so I'm going to go ahead and create it. It's also going to be deployed in this example, in the same resource group, so, our first one is Rg1, so I'll select that from the drop-down list.
Now down below, I'm going to give this a name. Again, it's got to be unique. So I'll put in some characters to make that a unique name, Canada Central. Now, we could, instead of using Premium like we did last time, leave it on Standard.
Now, it's on StorageV2 (general purpose v2), Locally-redundant with Hot access tier. I'm going to leave that, I'm going to click next. Again, I don't want it publicly visible so I'll change it to Private endpoint, although I'm not going to configure one. And I'm not going to change any of these advanced options.
And for tagging, let's say I don't tag this one. We could always tag it after the fact if we wanted to. Validation is passed, let's go ahead and create this second storage account.
  - Once again, before you know it, Your deployment is complete is the message you'll see
    - This time click Go to resource, so we're in our storage account once again, and we're looking at the navigation bar on the left as we did with our first one
    - But as we scroll down, we see not only Blob service listed over here on the left as we did before
    - But look, as we scroll further down, File service for Azure File shares, Table service, Queue service
  - Depending on how you create your storage account, depending on your selections
    - Will determine exactly what you see when you go into the properties of that storage account after the fact


Storage Accounts and the CLI
  - You can use the Azure CLI to create as well as to manage storage accounts and even to manipulate objects in the storage accounts such as uploading and downloading blobs. But to do that we have to know the syntax.
  - And we can figure that out here in the CLI, which I've launched from the portal Cloud Shell by starting with az -h for help. Now by looking through the alphabetical listing of next level commands, I could gather that storage would be the next level command I would be interested in. So therefore I could do az storage -h.
And then depending on what it is I need to manage would be determined by me looking at this list and finding the most likely item for example, I want to create a storage account.
So the next level item would be account. Okay, so having done that, I'm going to run az storage account -h. If I want to create a storage account, create would be the next level command. So I could just go ahead and ask for help on that as well.
And when I do that I get quite a bit of information.
And one of the things I would probably want to know is, for example, specifying the --sku for the storage account, whether it's going to be standard locally-redundant storage, or read-ahead geo-redundant storage RAGRS. We have to know what these values are when we pass the --sku parameter on the command line. So let's get to it.
I am going to run az storage account create -n, so I can specify the name of the storage account. And I'm going to make sure that I adhere to naming standards within the organization. So I'm going to go ahead and specify a name. And I'm going to specify the resource group I want to deploy the storage account into with -g. In this case, I already have a resource group called Rg1 -l for the location. Here I want to deploy this in canadaeast --sku we looked at that a moment ago. So I'm going to specify in this case locally-redundant storage and I'm going to go ahead and press Enter to create this storage account. And so after a moment, we get the JSON output, which indicates that it was successfully created. So we can go ahead and check our work here.
Of course, in the portal since we're in the Cloud Shell. Let me just scroll back up here. So the account here is storacct445888221. And, if we take a look at our Storage accounts here in the portal, we'll see that that will have been created.
So there it is, storract445888221. Let's go back though, into the CLI where I'm going to start by just perhaps clearing the screen with cls. Now the next thing I can do here is also view a list of storage accounts from the CLI with az storage account list.
But this gives me all of the properties which may be important but if all I'm interested in seeing for example is the name. Then I could bring up that previous command and use --query, open and close square brackets since we have an array of items and ask for .name. Now I'm seeing only the names of the storage accounts and certainly we can see the one that we've just created.
We can also show details about a specific storage account using az storage account show storage account, show. And then I can specify --name, and I can specify the name of the storage account. I'm just going to go ahead and copy the one that we just created, copy the name, and paste it in here for the name parameter, and of course, the resource group. Now, I could just specify, instead of specifying everything, I could use shorter parameters, -n for name and -g for the resource group, Rg1. And I'll press Enter, and so it returns information about that storage account.
And there's a lot of configuration settings as you can see, that might be enabled. Just as in if we were in the portal, if we click to open up that storage account in the properties blade for all of the configuration. As you can see, there's quite a few options. And so that is the kind of thing that we can do with the CLI. We can create storage accounts and we can start to manage them.
Don't forget that if you go back into the CLI you can always get help with storage accounts by running az storage account -h. We really focus primarily on the creation and listing and showing, which can also, delete them, you can also generate a shared access signature or an SAS so there is a lot of things that we can do.


Storage Accounts and PowerShell
  - PowerShell cmdlets can be used to create and also to manage Azure storage accounts.
  - But the first order of business is to determine the names of some of those cmdlets. And we can figure that out by using get-command. I'll use an asterisk wildcard symbol. And I'm going to guess that maybe azstorageaccount is part of the nomenclature. Of course, I can always look it up online.
  - Sure enough, we see a number of cmadlets that make sense for what our needs are, so such as Get-AzStorageAccount, Get-AzStorageAccountKey, New-AzStorageAccount, and so on. So we're going to build a new Azure storage account new-azstorageaccount.
First thing I need to do is specify the resource group into which I want this deployed so Rg1, then the name for the storage account so -name. I'm going to call this something that's in line with my organizational naming standards for storage account. So I'll go ahead and put something in here. I also want to deploy this in a specific location, in this case, canadaeast. And I'm going to specify a skuname in "standard_lrs", locally-redundant storage. Now you might wonder, how would you know to use that? Well, if you use get dash help for the new dash AZ storage account cmdlet, you'll see all of these types of details including all the parameters that we're specifying here on the command line. So let me go ahead and press Enter to create that storage account. And we can see that the provisioning succeeded. So I'm going to clear the screen.
We can also run get-azstorageaccount. So we can see what we've got listed here. And we can see even the newly created one, storacct64845.
That's what we've just created in the canadaeast region with this SkuName of Standard_LRS for locally-redundant storage. So that worked fine.
Now the next thing that we can also do is get details about a particular storage account. So for example, get-azstorageaccount. Well, let's actually do this. Why don't we get the storage account keys? get-azstorageaccountkey, now it's key singular, not keys plural. And storage account keys of which there are two with every storage account by default are used for programmatic access to the storage account contents including API access. So I'm going to specify the resource group that we want to look into to find the account. So our -resourcegroupname, in this case, is Rg1. And the account name, -accountname, we know in this case is storacct, store account, and we had called it 64845. I'm going to go ahead and press Enter. We can see the both of the keys have been returned here. Now we can also retrieve specific information about a storage account.
Let's see, get-azstorageaccount, now, we can filter it out and ask for certain properties, get-azstorageaccount. And we're going to specify here that we want the one that's in the -resourcegroup called Rg1 and at the -accountname, storacct and it was 64845. Okay, so I'll put that in. Now the problem, if you consider it a problem, is that it returns a lot of information. So we can see all of the properties going across the screen.
All I want is, let's say, the AccessTier. If I'm not sure what that property is called because this is a column header, AccessTier, it doesn't necessarily mean that's what the property is called. But we can verify this by doing the following. Let's use the up arrow key to bring up that command. I'm going to pipe it to get-member. And I'm going to tell it I only want to see property so -type property, and let's see what we have. So as we scroll back up through the output, we can see indeed there is a property by the name of AccessTier.
Okay, well, that's interesting. Let's go back up to a couple of previous commands and pipe the result of retrieving that storage account. To select, install all I want to see is the accesstier. Perfect, so we can see the Hot AccessTier has been configured for this particular item.
Now what I want to do is I want to set that to be cool. I want to use the cool AccessTier for less frequently accessed data which results in less cost. So for that, I will use the Set-AzStorageAccount cmdlet. I'll specify the resource group, in this case Rg1, where that account was deployed. And I'll use -AccountName, specify the name of our storage account. And I'm going to use -AccessTier and specify a value of Cool.
Now it's asking me, are you sure you want to do this? Well, let's just Ctrl+C out of that because what we can also do if I bring up that command is add -force at the end of it. This way we are prompted with anything and this really lends itself nicely if you want to automate this perhaps in a PowerShell script.
So it looks like it succeeded. Now we can verify our work. Let's just use the up arrow key here to go back up a little bit. And we want to select the AccessTier. And we can now see that the AccessTier indeed has been set to the Cool AccessTier.
Now the last thing that we're going to do is remove that storage account. So for that, I'll call upon the remove-azstorageaccount cmdlet. I'll specify the -accountname and then the -resourcegroup it's been deployed into. And I'm going to add -force at the end so it doesn't ask me for anything. And after a moment, it will have been removed.


Storage Account Network Access
  - When you create a storage account, you can determine network access options.
  - In other words whether the storage account should be accessible with a public endpoint, or from all VNets in Azure, or certain select VNets, you can also change that after the fact. So here in Azure, I'm going to go to my list of Storage accounts. Now if I don't see it in my recently visited links at the top here in the portal, I can always click in the left hand navigator and go all the way down to Storage accounts.
  - Either way, I'm going to open up an existing storage account. Now when I do that, I'm interested in taking a look at firewalls and virtual networks. You're going to see that down under Settings, there is Firewalls and virtual networks.
So think about how this storage account will be used, and which other components might require access to it. Such as a front-end web application, just as an example. Currently, we can see that we are allowing access from all networks, which does include the Internet. Now of course, if we don't have containers like blob containers with anonymous access enabled, it doesn't automatically mean anyone can get into the storage account. They would have to authenticate with either Azure AD, that would need maybe a shared access signature instead, or a storage account key. But nonetheless, we should limit the visibility as an added layer of security where possible. So I'm going to choose Selected networks.
And what I want to do is specify only certain VNets from which requests to access the storage account can be made. Down below, there are no virtual networks currently selected, so I'm going to click Add existing virtual network. Notice you can also build a new one at this point in time, but I already have one. So I'm going to choose the Add existing virtual network link. Of course, I'm going to choose that from the list.
So I've got my Subscription, I've got my Virtual networks. And so I'll select Vnet1, and I can choose a subnet within it, let's say Subnet1.
And I'm going to click Add, and now we can see that that has been added up above in terms of being on a network that is allowed to access this particular storage account.
Now I can also add an IP range to allow access from the Internet. So maybe beside software components in Vnet1, Subnet1 being allowed to access through storage account, I need to be able to access it from my on-premises computer. And I can see it already knows my public IP address. I'm going to go ahead and turn on that check mark to add that. Of course, I could also specify an alternate IP address or CIDR range as well.
And finally, down below, we can see it's set to allow trusted Microsoft services to access the storage account. So any other Azure resources that should be able to access this will be able to. And then we've got some logging in metric options that we could also enable here for exceptions. So I'm going to go ahead and Save this configuration. And in so doing we're limiting the visibility and the access of this storage account to the specifically listed Vnet, Subnet, and IP address.


Uploading Blobs Using the Portal
  - In this demonstration, I'm going to use the portal to upload some content into an Azure storage account.
  - To get started, I'm in the portal, I'm in the Storage accounts view. So I'm going to click on an existing storage account. I'm interested in uploading some blob files, binary large objects.
  - So I'm going to scroll down in the navigation bar until I see Blob service and I'm going to choose Containers.
So a blob service container is just a folder, the same way you would organize files on a USB drive, for example, by organizing them in folders. Well, I do the same thing here with containers. So I'm going to click + Container, and I'm going to start by creating a container called budgets.
The default Public access level is Private, so no anonymous access. But depending on what you might be dumping into this container might require that you enable something different, like anonymous read access for individual blobs or anonymous read access for containers and blobs within them. You might do that if you've got some public product brochures or documentation that, for example should be made available to anybody on a static host of web site. But in this case, I'm going to leave it on Private (no anonymous access) and I'm going to click Create.
So we can now see that we have a budgets folder.
Now I can select that folder and click Change access level at any point in time, but I'm not going to do that, said, I'm going to click to open it up to go into it.
So I'm going to go ahead and click the Upload button to upload some content into this blob container. And over on the right, I need to select one or more files, which I will do.
So I can specify whether I want to overwrite the files if they're already there. And if I open up Advanced, I can determine if I want to authenticate using the storage Account key versus Azure AD. And I can determine the Blob type, the default here is set to a Block blob.
When you're uploading things like Excel spreadsheet files, Block blob would be appropriate. But if you're going to be dealing with storing things like virtual hard disk files, page blobs are better. Append blobs are useful if you've got files such as logs that keep getting added to.
So I'm going to leave it on Block blob, and I'm going to go ahead down below and specify in this case that I wanted to use the Cool Access tier, because it's not going to be accessed frequently.

[Video description begins] The blade also includes a drop-down list box labeled "Access tier" in which an option labeled "Hot (Inferred)" is selected by default. He selects an option labeled "Cool" in the Access tier drop-down list box. [Video description ends]

And I'm going to go ahead and click on the Upload button. So we can now see that our content has been uploaded into this container.

[Video description begins] The Upload blob blade closes and the budgets blade displays in which two rows add in the budgets table. The first row entries under the Name, Access tier, and Blob type column headers are HR_EmployeeList.xlsx, Hot (Inferred), and Block blob respectively. The second row entries under the Name, Access tier, and Blob type column headers are Regional_Spending_2016.csv, Hot (Inferred), and Block blob respectively. [Video description ends]

Now I can also select a specific blob here and then I have options at the top, such as changing tier, so I can go ahead and do that. And from the drop-down list I can determine if I want it to be in the Hot, Cool, or Archive tier, but I also can acquire a lease.

[Video description begins] He selects a checkbox adjacent to the Regional_Spending_2016.csv row entry. Then he clicks a button labeled "Change tier" and its corresponding blade opens. It contains a drop-down list box labeled "Access tier" and two buttons labeled "Save" and "Cancel". In the Access tier drop-down list box, the Hot (Inferred) option is selected by default. [Video description ends]

When you acquire a lease on a blob, what you're doing is locking it. It's kind of like a resource lock on an entire Azure resource, the difference here being it's a blob within a resource, within a storage account. I can also click right on the Name of that uploaded blob and get some details about it.

[Video description begins] He closes the Change tier blade and the budgets blade displays. Then he points to a button labeled "Aquire lease". Then he clicks the Regional_Spending_2016.csv row entry in the budgets table and its corresponding blade opens. [Video description ends]

So I can see the Properties, the URL to it, I can see the modification and creation date and timestamp. I can see the size of it, I have the option of downloading it, I can Delete it, and so on.

[Video description begins] He closes the budgets blade. [Video description ends]

Actually, let's go back in there for one second, because there's something else that's important. When you go into an individual blob, you can also specify a shared access signature just for that specific blob. So notice here we have Generate SAS.

[Video description begins] He opens the budgets blade. Then he clicks the HR_EmployeeList.xlsx row entry in the budgets table and its corresponding blade opens. It includes four tabs labeled "Overview", "Snapshots", "Edit", and "Generate SAS". [Video description ends]

A shared access signature allows you to set an expiration on certain permissions that should apply and in this case, it's for a specific blob.
Whereas, if I were to back out of my budgets container and go back to the Storage account level, I can also specify that I want to create a shared access signature at that level. So you can do it at the Storage account level, as well as at the individual blob level.
Shared access signatures of course provide limited access to items within the storage account.


Uploading Blobs Using the CLI
You can use the Azure CLI to not only create and manage storage accounts, but also to work with the contents within storage accounts, such as uploading blobs, which is going to be the focus of what we're going to do here. 
So in the Azure portal, I've launched a Cloud Shell, specifically it's Bash shell. And we're going to learn how to make a reference to a storage account here in the CLI and upload a file. First thing we need to do is get a file here that we want to upload. Now if I haven't already, I have the ability of clicking this Upload/Download files button. So I'm going to choose Upload to get a file here in my Cloud Shell environment.

[Video description begins] He clicks a button labeled "Upload file" in the menu bar of the command prompt. A flyout opens which contains three options labeled "Upload", "Download", and "Manage file share". He points to the Upload option. [Video description ends]

So I've just done this. So I'm going to go ahead and do a ls. And I can see I've got a file here called HR_EmployeeList.xlsx.

[Video description begins] He executes the ls command. The output displays a message which reads, clouddrive Create_Custom_Role.json HR_EmployeeList.xlsx. [Video description ends]

Now, if I just minimize this and if we just look at the storage account here in the portal, in the background, that we're going to be using, if I were to go all the way down, under Blob service, Containers, we'll see that there are none.

[Video description begins] He minimizes the Bash command prompt and the Microsoft Azure web page displays in which the storacct333325 blade is open. He clicks the Containers option in the navigation pane and its corresponding blade opens in the content pane. It includes a table with multiple columns and no rows. The column headers include Name and Last modified. [Video description ends]

So a container is just essentially a folder that we use to organize blobs that we upload here. Now, at the same time, while we're here, why don't we take a look at the Access keys. Every storage account in the Azure Cloud gets two keys assigned automatically.

[Video description begins] He clicks an option labeled "Access keys" under a section labeled "Settings" in the navigation pane and its corresponding blade opens in the content pane. It includes two sections labeled "Key1" and "Key2". Each section contains two text boxes labeled "Key" and "Connection string". [Video description ends]

Now, these are used for programmatic or API access to the storage account's contents. But you might ask, why two keys, why not just one? Well, the idea is that we might have scripts and programming code that refers to, let's say key1. Now from a security perspective, you should be regenerating keys to change them periodically. Although, in practice in my experience that happens rarely, but it's what should happen.

The idea is that we could regenerate a second key, and at some point flip over our code to reference the second key and then regenerate the first key. Because you don't want to just regenerate a key without understanding that it will break any scripts or command-line API calls that refer to that key. They will no longer work after you change the key or regenerate it. Okay, so having said that, let's go into the CLI and let's get to work. The first thing I'm going to want to do is create a variable. And I'm going to create a variable, let's say, called ACCOUNT and it will equal the name of our storage account. So storacct333325, there we go. So if I do echo $ACCOUNT, we can see the value of the variable.

[Video description begins] He executes the ACCOUNT="storacct333325" command. No output displays and the prompt remains the same. [Video description ends]

So, it's the name of our storage account, which is what we want.

[Video description begins] He executes the echo $ACCOUNT command. The output displays a storacct333325 storage account name. The prompt remains the same. [Video description ends]

Clear the screen and I'm going to do the same kind of thing for a storage account key. We can even view the keys from here. So, for example, az storage account keys list --account-name, actually I can use my variable here, $ACCOUNT. How handy is that? --resource-group. It's in a resource group called Rg1. And I can just press Enter to see the output of that and there's the two keys, key1 and key2. Let's say I want to use key1, it doesn't matter which one you use.

[Video description begins] He executes the clear command. The screen gets clear. The prompt remains the same. Then he executes the following command: az storage account keys list --account-name $ACCOUNT --resource-group Rg1. The output displays key1 and key2 properties which includes KeyName, permissions, and value. The prompt remains the same. [Video description ends]

So I'm going to go ahead and copy that first one and dump that in a variable I'm going to call key.
So KEY equals open quote, let's just paste that in there, close the quote, done.
So if we echo $KEY, there's the value. Okay.
So we have an account variable, we have a key variable. Let's get to it. So what I want to do is create a storage account container, essentially a folder in my storage account, in which files I upload will be stored. So, az storage container create --name. I want to create a storage container called eastdata --account-name. We can use our $ACCOUNT variable name here, which contains simply a text string that references our storage account name. And also we'll specify --account-key, and let's pass our key variable $KEY. Okay, so let's press Enter.
Looks like it was created. We can even check our work here in the portal. If we just go back here and scroll down in the navigation bar for the storage account down under Blob service, Containers, we should see eastdata, and there it is.
So let's go back into the CLI. Now to actually upload content we're going to use az storage blob upload. --container-name, we just created a container, it's called eastdata I want it to go in there. The other thing I have to specify is the name I want to use for this file, so I'm going to call it HR_EmployeeList.xlsx, that's what I wanted to be called in the storage account once it uploaded. --file, I have to specify the actual file I want to upload. And I already have it here in my Cloud Shell storage. So it's called HR, it's actually called the same thing, EmployeeList.xlsx. Doesn't have to be the same. We can use the name to be the same or make it the same as the actual file that we're uploading, but it can be changed.
Either way, we've got that part done. So now we have to specify --account-name, we've got our handy variable here, $ACCOUNT, and --account-key, you guessed it, we have a $KEY variable. And it looks like the syntax is correct, az storage blob upload, container name, name file. Yup, looks good. Well, it will tell us if there's a syntax error.
And it looks like it finished, just like that, 100.0000%. So let's minimize this, let's open up eastdata here. And sure enough, we're going to see the presence of our HR_EmployeeList.xlsx file.


Uploading Blobs Using PowerShell
  - In this example, I'm going to use PowerShell to upload a blob to a storage account container.
So to get started here, the first thing I want to do is figure out which storage account we're going to work with. So I'm already in the portal where I've launched a Cloud Shell running PowerShell, going to minimize that for a second. And we're just going to go back here and take a look at our list of Storage accounts.
There's one here called storacct333325 that we're going to go into and work with, from the PowerShell perspective. And so, let's just scroll down here in the GUI to Blob service, Containers. There's already a container here that we're going to see, and it's called eastdata.
I want to create an HR container and upload a file. So we can see currently there is no HR container, at least not yet. Let's go back into PowerShell. Let's make this happen. So to do that, I'm going to create a variable here that I'm going to call context, ctx. And this variable is going to essentially point us to the storage account. To do that, I'm going to put =get-azstorageaccount. Because I don't want to keep repeating the name of the storage account, the resource group, -r for resource group it's Rg1. And -name and the name of the storage accounts, storacct333325. Okay, so what we're going to do, is we're going to store the result of running that command in the context variable. So $ctx. And it looks like we have a reference to that storage accounts, returning other properties that we didn't specify must be working.

[Video description begins] He executes the following command: $ctx=get-azstorageaccount -r Rg1 -name storacct333325. No output displays. The prompt remains the same. Then he executes the $ctx command. The output displays a table with eight columns and a row. The column headers include StorageAccountName, ResourceGroupName, PrimaryLocation, and SkuName. He highlights a Standard_RAGRS row entry under the SkuName column header. [Video description ends]

Looks good, clear the screen. What's next? Well the next thing I want to do, is actually get the context property. What that means is if I run $ctx and pipe that to get-member to show me object properties and methods, although I'm going to say -type property, I only care about properties.

[Video description begins] He executes the cls command and the screen gets clear. The prompt remains the same. Then he executes the following command: $ctx | get-member -type property. The output displays a table with three columns and multiple rows. The prompt remains the same. [Video description ends]

One of the properties of our variable here is called Context, and I need this. So, what I'm going to do then is the following. How about we overwrite same variable, it doesn't matter. We're not going to need it again anyway. So $ctx is going to =$ctx., and the name of the property we just looked at is context. Okay, so now let's just take a look at our $ctx variable.

[Video description begins] He highlights context in the output table. Then he clears the screen. The prompt remains the same. Then he executes the $ctx=$ctx.context command. No output displays. The prompt remains the same. Then he executes $ctx command. The output displays a list of properties of the storacct333325 storage account. The prompt remains the same. [Video description ends]

Now what's got a reference pointer that points to that storage account. This looks different. This is good, and this is what we needed. So, I'm going to run new-azstoragecontainer, spell this correctly, storagecontainer -name. I want to make this call hr, human resources. And here's we're going to use my variable, I'm going to use -context. And, we have to do say, $ctx. So it knows everything about what I'm talking about. I don't have to tell it, at least not outside of this variable. The name of the storage account, the resource group and so on.

[Video description begins] He clears the screen. The prompt remains the same. Then he executes the following command: new-azstoragecontainer -name "hr" -context $ctx. An error message displays which state that Container name 'hr' is invalid. The prompt remains the same. [Video description ends]

Well, it doesn't like it because it really needs to be 3 through to 63 characters long, no problem. Up arrow key, we will write out humanresources. Okay, now when we press Enter, it's going to love it. So we've now got a new storage container called humanresources.

[Video description begins] He executes the following command: new-azstoragecontainer -name "humanresources" -context $ctx. The output displays a table with three columns and a row. The column headers are Name, PublicAccess, and LastModified. The prompt remains the same. [Video description ends]

Let's just go back here and refresh the GUI. There's humanresources. Now, we can populate it with stuff. So let's clear the screen. And I'm going to do that by, well first of all, let's see what we have here in the Cloud Shell, dir.
So we've got a file called HR_EmployeeList.xlsx. How about we upload that file? So I'll use set-azstorageblobcontent, quite a long cmdlet name, -file. Well the local file here is called HR_EmployeeList.xlsx. I just typed in hr and press tab. It's unique enough, so it found the name in the current directory, and it spelled it out for me. How convenient, -container. I want to put this in the humanresources container. So humanresources. That's the one we just created a moment ago. And I'm going to specify that the blob, let's say, will be called the same thing as the file. How about HR_EmployeeList.xlsx. Sometimes you want to change the name of what it will appear as in the storage account, versus what it actually is in the file system that you have. But I'm okay with using the same name. The last thing I'm going to do, is specify -context and pass it our $ctx variable.
Now that's not a large file, so it won't take very long for that to be pushed up into the cloud. Let's minimize this. Let's go take a peek. Let's open up humanresources. And after a moment, sure enough, we can see our file has been successfully uploaded.


Uploading Blobs Using AzCopy
  - The Microsoft AzCopy command line tool can be used when you need to transfer files into a storage account, or from it or even between storage accounts. 
  - So here in my web browser, I've navigated to Microsoft documentation where they discuss AzCopy. And if I scroll down little bit, they also provide the downloads for AzCopy. So I'm going to go ahead and download the 64-bit Windows AzCopy tool on my on-premises Windows 10 computer.
Now, I've chosen to extract that to a folder on my machine on drive D and I've changed to that directory and if I do a dir, I can see the azcopy.exe tool.

[Video description begins] He executes the dir command. The output includes an information about the azcopy.exe tool. The prompt remains the same. [Video description ends]

Now depending on how you plan on using this, you can add this to your path this subdirectory where you've got the tool. So that you can run it no matter which subdirectory you're in, in the command line. I'm not going to bother with that, I'm going to run it right from this current location.

[Video description begins] He highlights azcopy_windows_amd64_10.3.4 in the prompt. [Video description ends]

So if I just run azcopy, for example, -h for help, we should be able to tell if it's good to respond and it looks like it is. So we know that it's working and it's giving us some help on how to use the AzCopy command-line tool.

[Video description begins] He executes the cls command. The screen gets clear. The prompt remains the same. Then he executes the azcopy -h command. The output displays an example and supporting commands for the aforementioned command. The prompt remains the same. [Video description ends]

Now before you can actually use this to talk to as your storage accounts, you're going to have to log in, and you can do that with azcopy space login.

[Video description begins] He clears the screen. The prompt remains the same. Then he executes the azcopy login command. The output displays a message which reads, To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code E7QC8WSRS to authenticate. [Video description ends]

So what that does is it says, go to this URL and enter this code to authenticate. So I've entered that URL into my web browser, it asks me to enter a code which I've copied from that command-line output, so I'm going to click Next.

[Video description begins] He opens the https://microsoft.com/devicelogin URL in a browser and a page labeled "Microsoft Enter code" displays. He pastes the E7QC8WSRS code in a text box labeled "Code". Then he clicks a button labeled "Next" and a page labeled "Microsoft Sign in" displays. [Video description ends]

So I'm going to login with one of my Azure AD users that I know has the appropriate storage account and blob roles. They have the right permissions so that they can manage the account such as creating containers, and also have the ability to write blobs or upload blobs to storage account containers. So I've got that person's email address filled in, I'm going to click Next.

[Video description begins] He logins with the user name: lbrenner@quick24x7test.onmicrosoft.com in the Azure Storage AzCopy tool. [Video description ends]

Of course, I'll then specify the sign in password for that account and I'll click the Sign in button. And it then says, I've signed in to the Azure Storage AzCopy app on my device. I can close this window and get back to work at the command line using the AzCopy tool. Back here at the command line, we can see here that the Login succeeded so we're ready to go, so I'm going to clear the screen.

[Video description begins] He switches to the Command Prompt window. The prompt displays Login Succeeded message. The prompt remains the same. Then he clears the screen. The prompt remains the same. [Video description ends]

Now here in the portal I've navigated to a storage account that will be interacted with using AzCopy and I've gone to Access control (IAM). And on the right, I'm going to click View, Role assignments, I want to check out which permissions or indirectly through role assignments that Lucas Brenner or L Brenner has.

[Video description begins] He switches to the Microsoft Azure web page in which the storacct333325 | Access control (IAM) blade is open. It includes five tabs labeled "Check access", "Role assignments", "Deny assignments", "Classic administrators", and "Roles". The Check access tab is selected by default. He clicks a button labeled View in a subsection labeled "View role assignments" in the content pane. Its corresponding Role assignments tab opens. It includes a table with four columns and multiple rows. The column headers are Name, Type, Role, and Scope. [Video description ends]

So, let's take a look, and if we scroll down, we can see that L Brenner has the Storage Account Contributor. So we can manage the storage accounts such as working with containers, but he also has the Storage Blob Data Contributor, so he can contribute blobs burning on large objects, such as, uploading files. The next thing I just want to do is take a look at the properties of this storage account.

[Video description begins] He points to Storage Account Contributor and Storage Blob Data Controller row entries under the Name column header. [Video description ends]

I'm interested really in the primary blob service endpoint, because we're going to use that to make a connection, we have to specify basically the URL. So let me just kind of scroll over to the right here, and we could see the Blob service resource ID and below that the Primary Blob Service Endpoint.

[Video description begins] He clicks an option labeled "Properties" in the navigation pane and its corresponding blade opens. [Video description ends]

And really it's just the full DNS name of our storage account, so I'm just going to go ahead and copy that.

[Video description begins] He copies https://storacct333325.blob.core.windows.net/ in a text box labeled "Primary Blob Service Endpoint". [Video description ends]

So I'm going to use the azcopy command followed by make, because I want to make a container or in other words, a folder or directory in the storage account before I upload content. So this is where I've specified the primary blob service URL that we just copied. However, what I want to do is specify after that the name of the container that I want to make. In this case, I'm calling it, eastprojects, let's see what happens by pressing Enter. Now if you get an error message, it's probably because you don't have the appropriate permissions to create that.

[Video description begins] He switches to the Command Prompt window. Then he executes the following command: azcopy make "https://storacct333325.blob.core.windows.net/". The output displays a message which reads, Successfully created the resource. The prompt remains the same. [Video description ends]

And that's why it was important that this user that we authenticated with L Brenner, has the storage account contributor role. So it looks good successfully created the resource, excellent. Next thing we want to do is copy a file there. To do that, I'm going to run azcopy copy, and then in quotes, I'm going to specify a local path on my on-prem computer. And in this case, I've decided I want all of the txt files in the projects folder under samplefiles in drive d. So instead of specifying an individual file or blob, I'm specifying a bunch of them using a wild card asterisk. After which I am then specifying where the target is, where's that going?

So, after that the next parameter is my storage account blob service endpoint URL. Of course, with our newly created container eastprojects, that's where I want that stuff to go, so let's go ahead and press Enter. Now again, if this fails and you get some kind of a message about permissions or insufficient permissions. It's probably because the account you authenticated with, in our case, it was L Brenner, doesn't have the storage blob data contributor role. In this case it looks good, it looks like the number of transfers completed was 3 so that is indicative of the fact that there are 3 files.

[Video description begins] He executes the following command: azcopy copy "d:\samplefiles\projects\*.txt" "https://storacct333325.blob.core.windows.net/". The output includes a message which reads, Using OAuth token for authentication and a Job summary. The Job summary includes information about total number of transfers, number of transfers completed and skipped. The prompt remains the same. [Video description ends]

So why don't we check our work, well, we could do it in a number of ways including in the portal. Back here in the portal, I'm still looking at the navigation bar here for the storage account question.
So, why don't we just kind of scroll down until we see Blob service, Containers. Hopefully, we'll see eastprojects, which we do, and if I open that up, we should see whatever files should be there, in this case, Project A, B, and C.txt.
We asked for everything that had a txt file extension in that location on-premises to be uploaded here, and we did so using the AzCopy command.


Blob Lifecycle Management and the Portal
  - Lifecycle management is important when it comes to cloud stored data.
  - Sometimes it might be required for regulatory compliance that you configure how data is treated over time. Such as, it be archived and retained for a period of time, or for cost savings, maybe for infrequently accessed data you want Azure to automatically move it to the cool tier to save on costs. Either way we can do this with Blob lifecycle management. Here in the portal I'm in the navigation bar for an existing Storage account. I'm going to scroll down under the Blob service section where I'm going to click Lifecycle Management.
What I'm going to do here is add a lifecycle management rule. So I'm going to click Add rule. Now this rule is going to determine things like how many days we want to elapse before files stored here, Blobs, are moved to some other type of storage.
So, for example I'm going to call this first Rule name CoolStorage. And for blobs, I can choose to move blobs to cool storage a certain number of days after they were last modified. So maybe after 90 days, so that means they're not being written to. So I'll move it to CoolStorage, which is designed for infrequently accessed data.
And it really does this at a reduced charge. I can also Move blob to archive storage. Now you might be required to archive data and retain it for a period of time. So this would take care of the archiving portion at least in an automated fashion. We can also determine if we want to delete blobs after a period of time. So a certain number of days after they were last modified and same with Snapshots that might have been taken. We can delete blob snapshots, a certain days after the blob was created.

So we can go ahead and do that. But this is just the rule. The next thing we have to do is specify the filter set. In other words, to what in the storage account should this rule apply? Now perhaps it's the whole storage account, or maybe a specific container, or maybe only a certain type of file within this storage account. We're going to find out what to do by clicking next down below to add the Filter set. Now we can add a prefix here.
Now there's nothing here by default, which means that if we don't specify anything, we are looking at everything stored in this Azure storage account to determine when it was last modified, to determine, if it should be moved to cool storage. But we could specify containers or folders that we want to use as a prefix. So instead of typing that in, I'm going to click Browse, and from the drop-down list, I'm going to choose one of my containers.
In this case, let's say eastprojects, The entire container, and I'll choose Select. So it's been added down here to the list.
That's what I want this Filter set applied to, that is the filter set.
And I'm going to then click next down at the bottom for review and add, validation has passed, perfect. Let's click Add, so that we can create this lifecycle rule. And we can now see that we have our CoolStorage lifecycle rule and it's been Enabled.


Blob Container Access Levels
  - You create blob containers to organize the blobs that you might store within an Azure storage account. And each of those blob containers can have an access level, to determine what access is allowed to the container itself or the blobs within it.
To get started here in the portal, I'm looking at the navigation bar for an existing Storage account. So I'm going to scroll down and choose Blob service, Containers.
This way we'll see what's already here. For example, if I open up the eastdata folder, we'll see any files that might exist there.
And I'm going to upload a new file, so I'm going to click Upload, and I'm going to upload a JPEG logo file. So I'm going to go ahead and click Upload. And after a moment, we can see quite clearly that our logo file exists.
And if I click on that blob, then we can even see what it looks like. So when it opens up the properties, I'm going to click Edit which in this case shows me that we've got a fictitious logo for a fictitious organization.
Now the next thing we want to do is take a look at access to that item. So for example, if I go to the Overview for that blob or that file, I have a URL. And the URL, I'm going to copy because I'm going to paste that into a browser to see what kind of access we have to it. Now we can see that the URL consists of the name of the storage account, followed by the default suffix .blob.core.windows.net. Then after that we see the path /eastdata is the blob container and the file, of course, we just uploaded is called logo.jpg.
However, when I put that into a web browser it basically says can't find it, don't know what you're talking about. Let's find out why that message is showing up. To do that, we should go back and look at the folder. So I'm going to go back a couple of levels here, so we can jump in and find out what's going on.

[Video description begins] He switches to the Microsoft Azure web page and closes the eastdata blade. The storacct333325 | Containers blade displays. [Video description ends]

So it's eastdata, I can select that folder, and I can change the access level. Now notice, it's currently Private (no anonymous access).

[Video description begins] In the containers table, he selects a checkbox adjacent to the eastdata row entry under the Name column header. Then he clicks the Change access level button and the dialog box with the Public access level drop-down list box opens. The Private (no anonymous access) option is selected in the drop-down list box. [Video description ends]

That explains why we currently are unable to view that file or access it using the URL. But let's change it to, for example Blob (anonymous read access for blobs only). The third option is to allow the enumeration of the container so to list blobs, such as programmatically. Here I'm just going to choose Blob (anonymous read access for blobs only), so for individual blobs, and I'm going to click OK.

[Video description begins] He selects the Blob (anonymous read access for blobs only) option in the Public access level drop-down list box. A notification message displays which reads, Blobs within the container can be read by anonymous request, but container data is not available. Anonymous clients cannot enumerate the blobs within the container. [Video description ends]

So anonymous access, that means no authentication required.
Now there are times when that may be useful for public information posted on a web application and so forth. Let's go ahead and refresh the previous web browser session we had.
And this time, instead of saying resource not found, it's actually showing the blob to us. Well, naturally, that's because we've modified the blob container access level to allow access to individual blobs given that we have the URL for that individual blob.


Storage Account Queues
Within an Azure storage account, you can create queues. Let's scroll down here in the navigation bar for an existing Storage account.
I'll scroll down until I see Queue service and Queues and I'll click on Queues.
Now, what is the purpose of this? This is of great interest definitely to software developers, because it allows them to build modular code in a loosely coupled way. What that means is that multiple software components instead of requiring each to be online to transmit messages, they can instead drop messages into a storage queue where the other component can pick up that message when it's available or when it's up and running. So I'm going to go ahead and click the Add queue button.

[Video description begins] He clicks a button labeled "Queue" and a dialog box labeled "Add queue" opens. It contains a text box labeled "Queue name". [Video description ends]

And I'm going to call this queue1. Now notice, if I were to put in let's say Q in the name, it doesn't like it so lowercase letters only. Now this queue will be referred to programmatically by developers. We're going to go ahead and click OK.

[Video description begins] He types queue1 in the Queue name text box. Then he clicks a button labeled "OK" and the dialog box closes. A row adds in the queues table. The row entries under the Name and Url column headers are queue1 and https://storacct333325.queue.core.windows.net/queue1 respectively. He points to the queue1 row entry. [Video description ends]

So at this point, we've got a queue created called queue1. And I can see if I scroll over here, I can see the Url. It's using the storage account name, followed by the default DNS suffix of .queue.core.windows.net, and of course, then a reference at the end to the name of the queue. If I click to open the queue, one of the things I can do here is submit a sample test message.

[Video description begins] He clicks the queue1 row entry and its corresponding blade opens. It includes a button labeled "Add message" and a table with five columns and no rows. The column headers are Id, Message text, Insertion time, Expiration time, and Dequeue count. [Video description ends]

So I can click Add message, Hello world, and the default expiration is set for 7 days. I'm going to leave that, just click OK. And there's our Hello message being stored in the queue. You might do that if you're a developer and then you want to

[Video description begins] He clicks the Add message button and a dialog box opens. It includes a text box labeled "Message text". He types Hello world in the text box. Then he clicks a button labeled "OK" and the dialog box closes. A row adds in the table displayed in the queue1 blade. He points to Hello world row entry under the Message text column header. [Video description ends]

write some code to retrieve the message from it. Either way, we can put some sample messages in there. Now, we can also define an Access policy. If I click Access policy on the left, it is what you would expect it would be, determines what access is allowed to the queue. So for example, if I click Add policy, then the first thing we have to do is specify an Identifier.

[Video description begins] He clicks an option labeled "Access policy" under the Settings subsection in the navigation pane and its corresponding blade opens. It contains a section labeled "Stored access policies" which further contains a button labeled "Add policy" and a table with four columns and no rows. The column headers are Identifier, Start time, Expiry time, and Permissions. [Video description ends]

So I'm going to call this AccessQueue1. And I can specify the Permissions, let's say, the ability only to read messages stored in the queue. I can specify a start date and time of when that should apply in an expiration date and time of when that should stop. And that would, of course, be in accordance with the specified time zone information for both the start and explorations. Now why would I do this, you would create this access policy as it limited way for software components in this case to read messages in this queue. Now it's good to know it's tied queue1 because we're creating this access policy for queue1. So I'm going to go ahead and click OK. And then I'm going to Save that policy.

[Video description begins] He clicks the Add policy button and a dialog box opens. It includes a text box labeled "Identifier", a drop-down list box labeled "Permissions", and calendar text boxes labeled "Start time" and "Expiration time". He types AccountClosure1 in the Identifier text box. He clicks the Permissions drop-down list box and a drop-down list opens. It includes four checkboxes labeled "Read", "Add", "Update", and "Process". He selects the Read checkbox. Then he sets a Start time and Expiration time in their respective text boxes. Then he clicks a button labeled "OK" and the dialog box closes. A row adds in the table displayed in the Stored access policies section. [Video description ends]

Now what we can also do here, if we go back to the storage account level, is we can open up the Storage Explorer tool from the navigation bar that's in preview.

[Video description begins] He opens the storacct333325 | Queue blade. Then he clicks an option labeled "Storage Explorer (preview)" and its corresponding blade opens. [Video description ends]

 And what I can do is generate a shared access signature based on that access policy for the queue, which is kind of a cooler feature. We can do it all right in here. So for example, if I take a look at my QUEUES here, I can see we've got queue1, and if I right click on it, I can get a shared access signature.

[Video description begins] The Storage Explorer (preview) blade displays four folders labeled "BLOB CONTAINERS", "FILE SHARES", "QUEUE", and "TABLES". He expands the QUEUE folder. It contains a subfolder labeled "queue". Then he right-clicks on the queue subfolder and a flyout opens which contains two options labeled "Get Shared Access Signature" and "Delete". He clicks the Shared Access Signature option and its corresponding blade opens. [Video description ends]

Shared Access Signatures traditionally provide limited and in some case timed access where there's an expiration to resources within a storage account, as opposed to a storage account access key which gives access to the entire account. That's not the case here. It's only really Read access for a specific queue. Okay, so I'm going to choose my Access policy from the list here it's AccessQueue1. We can see the details for the start and expiration dates and times, and the Permissions which are Read.

[Video description begins] He clicks a drop-down list box labeled "Access policy" and its drop-down list opens. He selects an option labeled "AccessQueue1" from the drop-down list. [Video description ends]

So at this time when I created, I then can copy either the URI or the Query string depending on how, as a developer, I'm making a connection, to the Shared Access Signature to gain these privileges.


Azure AD Storage Account AD Authentication
There are times where you might want to grant storage account permissions to Azure AD users. And we can do this by assigning RBAC roles related to storage account usage.
So we get started here in the portal, I'm looking at the properties of an existing Storage account. And within that navigation bar, I'm going to click Access control (IAM), where I would normally go for RBAC role assignments. And I'm going to click Add and Add role assignment.
What I want to do is filter the list of roles for any that begin with the word storage, since, we're talking about a storage account. So from the Role drop-down list, I'll just give a click there and I'll type in storage. So the first thing I want to do is assign the Storage Account Contributor role because I want to allow a specific user in AD, Azure AD or a group perhaps, or even a service principal. I want to grant them the ability to manage the storage account. So such as creating storage account containers. So I'm going to select that role. And what I want to do is assign that to a group. So I'm going to type in the word east. And I've already got an Azure AD group I've defined previously called EastAdmins. I want the members of that group to have storage account contributor permissions. So I'm going to go ahead and select that. And I'll click Save.
If I also want, let's say that same group to be able to upload blobs, then I would work with a different role. So I'm going to click Add, Add role assignment. And the role I'm interested in here is going to be the storage blob data contributor role. Let's find that first. To make sure that's the correct name.

[Video description begins] He opens the Add role assignment blade. [Video description ends]

So I'm going to click on list roles. Once again, type in storage and it looks like we were right, Storage Blob Data Contributor role. And I'm going to assign that to the same group so they can also upload blob content to the storage account. Then I'll click Save.

[Video description begins] He selects an option labeled "Storage Blob Data Contributor" in the Role drop-down list box. He keeps the default value in the Assign access to drop-down list box. Then he selects the EastAdmins group. Then he clicks the Save button and the Add role assignment blade closes. [Video description ends]

So once that role has been assigned or those roles, we can click on View to view the role assignments.

[Video description begins] He clicks the View button under the View role assignments subsection in the content pane. The Role assignments tab gets selected. It displays the table with four columns and multiple rows. The column headers are Name, Type, Role, and Scope. [Video description ends]

And over on the right here, if I scroll down, we'll see in fact that EastAdmins has been given the Storage Account Contributor role at This resource, so wasn't inherited from a management group or subscription or resource group. And also the Storage Blob Data Contributor. So any member of the EastAdmins group, which by the way, why don't we just take a look at that specific group, so we can see who the members might be because it is the Members of the group that will have those permissions or those abilities at the storage account level.

[Video description begins] He clicks the EastAdmins row entry under the Name column header and its corresponding blade opens. It is divided into two sections: navigation pane and content pane. The navigation pane includes an option labeled "Overview". It is selected by default and its corresponding blade is open in the content pane. Then he clicks an option labeled "Member" under a subsection labeled "Manage" in the navigation pane. It displays a table with four columns and two rows. The column headers are Name, Type, Email, and User type.[Video description ends]

And here we can see its users Jen Hill and Marcia Lin. 


Storage Account Access Keys
  - Every Microsoft Azure storage account always has two storage account access keys.
  - Here in the properties of a Storage account, I can go down under Settings and click on Access keys to open the Access keys blade.
And when I do that, over on the right, I will see that there are two keys. Now there are also two connection strings, we're focusing primarily on the keys. These keys give access to the storage account, and it's access that would be granted for example, at the command-line level.

[Video description begins] He highlights the alphanumeric value of the key1. [Video description ends]

When using command-line tools, you could use the access key to gain access to the storage account. Even using GUI tools like Azure Storage Explorer, sometimes developers will be accessing Azure storage account APIs. And then we'll need to specify a key to gain programmatic access, either way, that's what it's for. But there are two of them, why two? Well, what you could do is you could refresh keys periodically. And as a matter of fact you should, from a security perspective, but if you're regenerating a key, what you're doing is changing it.

[Video description begins] He points to a button labeled "Regenerate" adjacent to the key1 text. [Video description ends]

So I've select a key1, the Regenerate icon, it says, the current key will become immediately invalid and can't be recovered. You want to regenerate access key1, I'm going to choose Yes. And when I do, it looks completely different, so if I had any code or scripts or command-line references to the old key1.

[Video description begins] He highlights the modified alphanumeric value of the key1. [Video description ends]

They will no longer work, they won't have access to the storage account any longer. So I would do this on a periodic basis and you'd have to make sure you change all references that use the old key. But because it could be disruptive in some cases, that's why you have two keys. Because you could have commands and scripts use key2 for a while, as you generate key1. Which you might use for new command line or programmatic access.

[Video description begins] He highlights the alphanumeric value of key2. Then he highlights the new alphanumeric value of key1. [Video description ends]

So you have the access of having a, or the ability rather, having a key that's still valid and useful while you refresh a different one.

[Video description begins] He highlights the alphanumeric value of key2. Then he points to the new alphanumeric value of key1. [Video description ends]

You can also do this at the command line, so let's go ahead and jump into Cloud Shell. And I'll just click Reconnect, since my session timed out. So here in PowerShell, I'm going to use the New-AzStorageAccountKey cmdlet to generate a new key.

[Video description begins] He opens the PowerShell command prompt. The PS /home/danlachance72> prompt displays with the following command: New-AzStorageAccountKey -ResourceGroupName Rg1 -Name "storacct333325" -KeyName "key1". [Video description ends]

I'll specify the resource group where that account was deployed, the name of that storage account. And the key I want to regenerate, in this case, key1 and I'll press Enter. Now we can actually retrieve information about that as well, using the Get-AzStorageAccountKey cmdlet.

[Video description begins] He executes the aforementioned command and its output displays a single column table with a single row. The column header is Keys and its respective row entry is {key1, key2}. The prompt remains the same. [Video description ends]

So that would be Get-AzStorageAccountKey, specify the resource group where the storage account was deployed. And the name of the storage account, and when I press Enter, it returns both key1 and key2.

[Video description begins] He executes the following command: Get-AzStorageAccountKey -ResourceGroupName Rg1 -Name storacct333325. The output displays a table with three columns and two rows. The column headers are KeyName, Value, and Permissions. [Video description ends]

Shared Access Signatures

[Video description begins] Topic title: Shared Access Signatures. The presenter is Dan Lachance. [Video description ends]

A shared access signature or SAS, S A S, which you'll see when you're looking at the navigation bar for a Storage account, you'll see it down here, Shared access signature. 

[Video description begins] The storacct333325 blade opens in the Microsoft Azure web page. [Video description ends]

This is a way to allow limited access to a storage account to particular services with perhaps only read access as opposed to read, list, and write. And even it can have an expiration date and time.

[Video description begins] He clicks an option labeled "Shared access Signature" under the Settings subsection in the navigation pane and its corresponding blade opens in the content pane. [Video description ends]

So the permissions are only good within a certain time frame. Now that's as opposed to Access keys, of which there are two for every storage account.

[Video description begins] He opens the Access keys blade. It includes key1 and key2 alphanumeric values and connection strings. [Video description ends]

If you have an access key, you have full access to everything in the storage account. Not quite so with the Shared access signature, depending on how it's configured. Let's take a look at how that works.

[Video description begins] He opens the Shared access signature blade. [Video description ends]

So I'm going to go into Shared access signature, and I only want to allow access, let's say, to blobs, not to Azure file shares, queues, or tables.

[Video description begins] The Shared access signature blade includes four checkboxes labeled "Blob", "File", "Queue", and "Table" for Allowed services. By default, all the checkboxes are selected. He removes checks from the File, Queue, and Table checkboxes. The Blob checkbox remain selected. [Video description ends]

Now, I only want to allow Read and List access. I don't want this Shared access signature to allow the creation of content at the blob level.

[Video description begins] The Shared access signature blade also includes eight checkboxes labeled "Read", "Write", "Delete", "List", "Add", "Create", "Update", and "Process" for Allowed permissions. The Update and Process checkboxes are grayed out and rest of them are selected by default. He removes checks from the Write, Add, Create, Update, and Process checkboxes. The Read, Delete, and List checkboxes remain selected. [Video description ends]

Now, if I scroll down further we'll also see the timing, the date and the time stuff. So the start and expiration date and time. So the starting date and time, the expiry date and time, which I'm going to leave. Notice here that on the same day, we have access starting at about now, 10:56 in the morning, and ending at about 6:56:15 PM. So we got 8 hours by default. Of course, you can change that, you can also specify an allowed IP address either individually or range, as implied with the faded or ghosted text in that field to help you along. It's going to be for HTTPS and I have to choose either key1 or key2 as the Signing key for this signature.

So I'm just going to leave it on key1, and I'm going to Generate SAS and connection string. And then down below, I'll see it's given me three items, a Connection string, a SAS token, which I would use for programmatic access, and also a Blob service SAS URL. So I'm going to use the Blob service SAS URL down at the bottom here. I'm going to copy it and I'm going to use that to make a connection to the storage account, using the Azure Storage Explorer GUI tool.

[Video description begins] He opens a window labeled "Microsoft Azure Storage Explorer". It is divided into three parts: menu bar, navigation pane and content pane. The navigation pane includes options labeled "Quick Access" and "Local & Attached". The Local & Attached option includes a suboption labeled "Storage Accounts" and its corresponding table with six columns and a row is displayed in the content pane. The column headers include ID, Message Text, and Insertion Time. [Video description ends]

Microsoft Azure Storage Explorer is a free downloadable tool, which I've already downloaded. What I want to do is create a new connection using the shared access signature. So in the left hand navigator here I'm going to right-click on Storage Accounts, and I'm going to choose Connect to Azure storage.

[Video description begins] He right-clicks on the Storage Accounts suboption and a flyout opens which includes an option labeled "Connect to Azure storage". He clicks that option and a wizard labeled "Microsoft Azure Storage Explorer - Connect" opens. It displays a page labeled "Connect to Azure Storage". [Video description ends]

What I want to do is, I want to make a connection using a SAS URI, shared access signature. So I'm going to go ahead and click Next. I'll paste in the URI.

[Video description begins] The Connect to Azure Storage page includes several radio buttons. A radio button labeled "Add an Azure Account" is selected by default. He selects a radio button labeled "Use a shared access signature (SAS) (URI)". [Video description ends]

Everything else is filled in, including the Display name. I can change that. I'm going to call this SASTest, just so we can distinguish it from anything else that's already there in the navigation panel.

[Video description begins] A page labeled "Attach with SAS URI" displays. It includes various text boxes. He pastes the copied Blob service SAS URL in a text box labeled "URI". The text boxes labeled "Display name", "Blob endpoint", "File endpoint", "Queue endpoints", and "Table endpoint" auto-populates with default values. Then he edits the Display name to storacct333325 -SASTest. [Video description ends]

I'll click Next and Connect. So in the left-hand navigator, we can now see our SASTest connection to the storage account. And if I expand it, I can see only Blob containers.

[Video description begins] A suboption labeled "storacct333325 -SASTest (SAS)" adds under the Storage Accounts suboption. He expands the storacct333325 -SASTest (SAS) suboption. It further includes a suboption labeled "Blob Containers". [Video description ends]

Now I have another example up here in the Azure Storage Explorer where I've connected to the storage account using an access key, which provides access to everything in that storage account. But here, we only have access to what the shared access signature says we have access to, which is Blob containers.

[Video description begins] He expands the StorAcct333325 (Key) suboption under the Storage Accounts suboption. It contains four suboptions labeled "Blob Containers", "File Shares", "Queues", and "Tables". Then he collapses the StorAcct333325 (Key) suboption. Then he expands the storacct333325 -SASTest (SAS) suboption. He points to the Blob Containers suboption. [Video description ends]

And if I take a look for example, under eastprojects, I can see some content there. But I will not be able to upload content because I don't have the ability to add items, only list and read. So I'm going to go ahead and try to upload a file here.

[Video description begins] He expands the Blob Containers suboption which further contains three suboptions labeled "eastdata", "eastprojects", and "humanresources". He clicks the eastprojects suboption and its corresponding table with nine columns and three rows displays in the content pane. The column headers include Name and Access Tier. The content pane also includes a quick access toolbar which includes buttons labeled "Upload" and "Download". [Video description ends]

But it's going to fail, says Insufficient credentials.

[Video description begins] He clicks the Upload button and a flyout opens with two options labeled "Upload Folder" and "Upload Files". He clicks the Upload Files option and its corresponding dialog box labeled "Microsoft Azure Storage Explorer - Upload Files" opens. It includes a text box labeled "Selected files" and a button labeled "Upload". The Selected files text box contains a file name labeled "CardData.txt". He clicks the Upload button and the dialog box closes, but no file uploads. [Video description ends]

It's not allowed because of the shared access signature, it doesn't allow the adding of content at the blob level.

Storage Account Replication

[Video description begins] Topic title: Storage Account Replication. The presenter is Dan Lachance. [Video description ends]

When you create an Azure storage account, at creation time you can determine whether or not you want a specific type of replication enabled for that account.

[Video description begins] The storacct333325 blade opens in the Microsoft Azure web page. [Video description ends]

Replication allows you to increase the data availability for whatever you have stored in the account. First of all, we can change this after the fact. And I'm going to do that here in the portal. So I've got the properties of an existing Storage account displayed. I've got the navigation bar. First thing I'll do is go down to Geo-replication. But when I click there, what I'm going to see is a map with my Primary location.

[Video description begins] He clicks an option labeled "Geo-replication" under the Settings subsection in the navigation pane and its corresponding blade opens. [Video description ends]

So where is the option to add additional locations for replication? Well, there's nothing, not even down at the bottom. All I see is that our current primary location is Canada Central. Well, that's because we have to enable replication, if it wasn't enabled when the account was created, by going to Configuration over on the left. So I'm going to go ahead and I'm going to do that. So we'll be able to select the type of replication that we're interested in.

[Video description begins] He clicks an option labeled "Configuration" under the Settings subsection in the navigation pane and its corresponding blade opens. [Video description ends]

If we take a look, currently it's set to Locally-redundant storage (LRS). So what this means is that we've got three copies of data within an Azure location. So, if we've got a problem such as fire in an Azure data center, we might lose all access to those three copies. And so for additional availability, you might select either Geo-redundant storage (GRS), or Read-access geo-redundant storage (RA-GRS).

[Video description begins] In the Configuration blade, he points to a drop-down box labeled "Replication". By default, it contains an option labeled "Locally-redundant storage (LRS)". [Video description ends]

The difference being that with geo-redundant storage if there's a failure in the primary region, you've got to initiate a failover to the secondary region where data was copied to in order to be able to access it. So I'm going to choose in this case, Geo-redundant storage (GRS).

[Video description begins] He clicks the Replication drop-down list box and a drop-down list opens with three options labeled "Locally-redundant storage (LRS)", "Geo-redundant storage (GRS)", and "Read-access geo-redundant storage (RA-GRS)". [Video description ends]

And I'm going to Save that config change here. So we're changing the configuration of an existing Storage account. It says it successfully updated it, perfect.

[Video description begins] He clicks a button labeled "Save" to save the changes in the Configuration blade. [Video description ends]

Let's go back and let's look at the Geo-replication screen. Previously, we had only the blue indicator on the map which according to the legend was our Primary location of Canada Central.

[Video description begins] He opens the Geo-replication blade. It displays a world map and a table with four columns and two rows. The column headers are Location, Data center type, Status, and Fallover. He points to Canada Central and Canada East row entries under the Location column header. [Video description ends]

Now it's automatically determined that geo-replication will be enabled for the green listed item on the map, which is a different location. It's another region but it is reasonably close to the primary, it's Canada East. So now, Canada Central will be replicated to Canada East. So at this point at the bottom, it says, well you can't even enable failover because replication is happening, you've just enabled this. And as you might imagine, depending on the amount of data in the account will determine how long it takes for this to complete. But at this point, we've increased the availability of the data by replicating it to an alternate or secondary Azure region.

Azure Storage Explorer Connectivity

[Video description begins] Topic title: Azure Storage Explorer Connectivity. The presenter is Dan Lachance. [Video description ends]

Microsoft Azure Storage Explorer is a free GUI tool that lets you connect to various aspects of an Azure storage account, such as, if you want to manage blobs, or queues, or tables, and so on.

[Video description begins] The Azure Storage Explorer web page opens. [Video description ends]

So in my browser, I've navigated to the Azure Storage Explorer download page. You can search this up quite easily using your favorite search engine. And I'm going to go ahead and download and install Azure Storage Explorer for the Windows platform on my on-premises Windows 10 station. You can then start Microsoft Azure Storage Explorer from your Start menu like you would with any app. Now, the first thing I need to do here is determine how I'm going to make a connection to a storage account in Azure.

[Video description begins] He opens the Microsoft Azure Storage Explorer window. [Video description ends]

So for example, I can expand Storage Accounts here and I can right-click and choose Connect to Azure storage.

[Video description begins] In the navigation pane, he expands the Storage Accounts suboption. Then he right-clicks on it and a flyout opens. He clicks the Connect to Azure storage option from the flyout and the Microsoft Azure Storage Explorer - Connect wizard opens. In the wizard, the Connect to Azure Storage page is open. It includes several radio buttons. A radio button labeled "Add an Azure Account" is selected by default. [Video description ends]

And I have a number of ways I can do that using my Azure account credentials, or via Azure AD credentials, using a connection string, a shared access signature, a storage account name and a key to it, or I can attach to a local emulator for testing purposes. In this case, let's say I want to connect to a storage account using its name and key. We'll go ahead and click Next.

[Video description begins] He selects a radio button labeled "User a storage account name and key". Then he clicks the Next button and a page labeled "Connect with Name and Key" displays. [Video description ends]

You'll want a Display name. Now, I'm going to give it the name of the storage account StorAcct333325. And I have to give it an Account name that would be for the storage account and also the Account key. So I'm going to pop in the actual name of it, storacct333325. But I also need an Account key.

[Video description begins] He types StorAcct333325 and storacct333325 in text boxes labeled "Display name" and "Account name" respectively. [Video description ends]

So here in the portal in my storage account, I'm going to scroll down to Access keys of which there are two. Now, it doesn't make a difference which one I copy. So I'll just take the first one and click the copy icon. And I'll go ahead and paste that in the Account key field.

[Video description begins] He switches to the storacct333325 blade opened in the Microsoft Azure web page. Then he opens the Access keys blade. Then he copies the alphanumeric value for the key1. Then he switches back to the Microsoft Azure Storage Explorer window in which the Microsoft Azure Storage Explorer - Connect wizard is open. He pastes the copied key in a text box labeled "Account key". [Video description ends]

And that's all I'm going to change here. So I'm going to go ahead and click Next and then Connect.

[Video description begins] A page labeled "Connection Summary" displays. He clicks a button labeled "Connect" and the wizard closes. A suboption labeled "StorAcct333325 (Key) adds under the Storage Accounts suboption in the navigation pane. [Video description ends]

So now we can see the storage account listed in the navigator and in parentheses the word Key, since that is how we gained access to the account. So now I can check out what's in the account. I can drill down into it, expand Blob Containers. Here's all of the different blob containers we have. I can click on each one of those and see files, download, upload, content.

[Video description begins] He expands the StorAcct333325 (Key) suboption. It includes suboptions labeled "Blob Containers", "File Shares", and "Queues". He expands the Blob containers suboption. It further includes a suboption labeled "eastprojects". He clicks the eastprojects suboption and its corresponding table with nine columns and three rows displays in the content pane. The column headers include Name and Access Tier. [Video description ends]

I can also see any Azure File Shares that might have been created, of course, the contents of those file shares.

[Video description begins] He expands the File Shares suboption. It contains a suboption labeled "projects". He selects the projects suboption and its corresponding table with two columns and three rows displays in the content pane. The column headers are Name and Size. [Video description ends]

I can see any Queues that might have been defined, and those items are shown here and so on.

[Video description begins] He expands the Queues suboption. It contains a suboption labeled "queue1". He selects the queue1 suboption and its corresponding table with multiple columns and no rows displays in the content pane. The column headers are ID and Message Test. [Video description ends]

So really, the Azure Storage Explorer is another way to make a connection to a storage account. In our case, with the storage account access key, which gives access to the whole storage account from which we can then work with the content in the storage account.

Storage Account Blob Soft Delete

[Video description begins] Topic title: Storage Account Blob Soft Delete. The presenter is Dan Lachance. [Video description ends]

One of the many options you have when you create a storage account in Azure is whether or not you want the blob soft delete option enabled.

[Video description begins] The storacct333325 blade opens in the Microsoft Azure web page. [Video description ends]

It's disabled by default. Now you can check this out on an existing Storage account in the portal by going into the navigation bar, scrolling down under Blob service, and choosing Data protection.

[Video description begins] He clicks an option labeled "Data protection" under the Blob service subsection in the navigation pane and its corresponding blade opens in the content pane. It includes a toggle button labeled "Blob soft delete" with two options labeled "Disabled" and "Enabled". It is Disabled by default. [Video description ends]

You will see that the current state is as it was by default Disabled. I'm going to choose Enabled. And I've got a retention policy set for a default value of 7 days. This means that any data, blob data, that is overwritten or deleted can be recovered for up to 7 days. Now this doesn't apply, if we're talking about a container, essentially a folder that is deleted. You can't get those contents back after 7 days. It also doesn't apply if any of the metadata, such as tagging and so on related to blob items is modified. So I'm going to go ahead and click Save to save that change. And let's navigate to our Blob service Containers, and I'm just going to open a Container called eastprojects.

[Video description begins] He opens the Containers blade. Then he clicks the eastprojects row entry under the Name column header of the containers table. The eastprojects blade opens. It includes a button labeled "Delete", which is grayed out and a table with multiple columns and three rows. The column headers include Name and Modified. [Video description ends]

In here, I have a file named Project_A.txt. So I'm going to select that file and I'm going to choose to Delete it. It also asks would you also like to delete any blob snapshots, if there are any. Well, I don't have any, so I'm going to just choose OK. And we can now see that our blob is deleted, the file is gone, Project_A.txt.

[Video description begins] He selects a checkbox adjacent to the Project_A.txt row entry under the Name column header. Then he clicks the Delete button and a dialog box labeled "Delete blob(s) opens with a message which reads, Are you sure you would like to delete the selected blobs?. He clicks the OK button and the dialog box closes. The first row entry deletes from the table. [Video description ends]

Now notice within this view here over on the far right I have the option to Show deleted blobs. If I turn that on, we can clearly see Project_A.txt was deleted because the Status says as much.

[Video description begins] He clicks a toggle button labeled "Show deleted blobs" and the deleted row entry displays in the table. The first row entries under the Name and Status column headers are Project_A.txt and Deleted respectively. [Video description ends]

Now, what I can do is click directly on that blob. And when I'm in the properties of it over on the right I can click Undelete. So it says it successfully undeleted it.

[Video description begins] He clicks the Project_A.txt row entry and its corresponding blade opens. It includes a button labeled "Undelete". He clicks the Undelete button and a notification message displays which reads, Successfully undeleted blob. [Video description ends]

Let's just take a quick little peek here.

[Video description begins] He closes the Project_A.txt blade. [Video description ends]

So, if we close that current property window or blade, so close that out, we can now see Project_A.txt, instead of having a Status of deleted is now back at being Active.

[Video description begins] The status of the Project_A.txt row entry under the Status column header changes to Active. [Video description ends]

And we have the ability then to do this for the retention period, which by default is 7 days.

Storage Account Encryption

[Video description begins] Topic title: Storage Account Encryption. The presenter is Dan Lachance. [Video description ends]

Encryption of data at rest has become more and more important over time, as we keep hearing media reports about compromised customer information for example, due to the fact that data was not stored in an encrypted format.

[Video description begins] The Home - Microsoft Azure web page opens. [Video description ends]

And really, in this day and age, that's completely unacceptable as there are so many freely available encryption tools out there. And in many cases, it's enabled by default, such as with an Azure storage account. Let's explore that a little bit. Here in the portal, I'm going to go into Storage accounts and I'm going to open an existing Azure Storage account. What we want to do is scroll down in the navigation bar and click Encryption.

[Video description begins] He opens the Storage accounts blade. Then he clicks the storacct333325 row entry under the Name column header of the storage accounts table and its corresponding blade opens. [Video description ends]

When we do this, we'll see that it's set currently to Microsoft Managed Keys.

[Video description begins] He clicks the Encryption option under the Setting subsection in the navigation pane and its corresponding blade opens. It includes radio buttons labeled "Microsoft Managed Keys" and "Customer Managed Keys". The Microsoft Managed Keys radio button is selected by default. [Video description ends]

So yes, it is server-side encryption. It is enabled but the keys are controlled by Microsoft. Depending upon your regulatory compliance requirements, that might not work. You might need to have control of them. So we do have the option, as we can see here, to enable Customer Managed Keys. So down below, I can select a Key vault.

[Video description begins] He selects the Customer Managed Keys radio button. It displays a toggle button labeled "Encryption key" with two options labeled "Enter key URI" and "Select from key vault" and a link labeled "Select a key value and key". The Enter key URI option is selected by default in the Encryption toggle button. Then he selects the Select from key vault option in the Encryption toggle button. [Video description ends]

Now a Key vault is a centralized Azure resource. I say centralized only meaning it serves as a central repository for you to store secrets like PKI certificates, passphrases, and of course, as in our case, keys. And so other Azure resources or software code that you might develop can refer to this centralized storage location for secrets and call upon things. And in this case, I'm going to go and select a key vault. Because I want to choose a key from it that will be used to encrypt the storage account and it will be under my control.

[Video description begins] He clicks the Select a key value and key link and is corresponding blade labeled "Select key from Azure Key Vault" opens. It contains three drop-down list boxes labeled "Subscription", "Key vault", and "Key". An option labeled "Pay-As-You-Go" is selected by default in the Subscription drop-down list box. [Video description ends]

So from the drop-down list, I have a key vault I've created called KV1East1. And within it, I've got a key called Key1, that's something I created, I generated.

[Video description begins] He selects an option labeled "KV1East1" in the Key vault drop-down list box. Then he selects an option labeled "Key1" in the Key drop-down list box. [Video description ends]

So I'm going to go ahead and choose Select. And at this point, we've now got the required information to enable Customer Managed Keys here for our storage account.

[Video description begins] The Key vault and key information displays in the Encryption blade. The Key vault is kv1 east1 and Key is Key1. [Video description ends]

Now that that's filled in, I'm just going to go up at the top and choose Save. And it says it's updating the server-side encryption for the storage account. So at this point, it's now been enabled. So from this point forward, newly added items to the storage account will be encrypted.

[Video description begins] He saves the changes in the Encryption blade. A text box labeled "Current Key" and "Automated Key Rotation" displays with "https://kv1east.vault.azure.net/Key1" and "Enabled - Using the latest key version" values respectively. [Video description ends]

And of course, any existing items will be retroactively encrypted with the background encryption process.

Blob SAS Token

[Video description begins] Topic title: Blob SAS Token. The presenter is Dan Lachance. [Video description ends]

In the Microsoft Azure environment, a shared access signature is usually associated with the storage account.

[Video description begins] The storacct333325 blade opens in the Microsoft Azure web page. [Video description ends]

Here in the portal, I'm looking at a Storage account, and if I scroll down and look under Settings, I'll see Shared access signature.

[Video description begins] He opens the Shared access signature blade. [Video description ends]

And the purpose of this is to allow limited, time-based access to some contents of the storage account, whereas the access keys provide full access to everything in the storage account. But we not only have the option of setting up a Shared access signature, for example, to allow access to blobs, but we can get even more specific to individual blobs. So for example, I'm going to scroll down further under Blob service and choose Containers. And I'm going to pick on a container called eastprojects, let's say, where I've got some sample files uploaded. One of which is called Project_A.txt.

[Video description begins] He opens the Containers blade in which the containers table is displayed. He clicks the eastprojects row entry under the Name column header and its corresponding blade opens. It displays a table with multiple columns and three rows. The column headers include Name, Modified, and Access tier. [Video description ends]

I want to set a Shared access signature for that specific blob as opposed to all blobs.

[Video description begins] He clicks the Project_A.txt row entry under the Name column header and its corresponding blade opens. [Video description ends]

So when I open up the properties on the right, I need to click on the Generate SAS option.

[Video description begins] He clicks the Generate SAS tab and its corresponding options display in the content pane. [Video description ends]

Now from here, I can choose the Permissions that would apply here. It's set to Read by default, which is it what I want, so I'm going to leave it at that. And I can see the Start date and time, and the time zone. And of course, the idea is that we can have an Expiry date and time as well for when this is good for, this Shared access signature. So I'm going to leave it as it is. I could also specify IP addresses from which access is allowed. And I'm just going to scroll down a little bit by scrolling over to the far right, and see everything.

And then finally, we have to specify some finalized details here, such as whether we're going to allow HTTPS or HTTP. Of course, HTTPS encryption makes sense. And Key 1 in our storage account will be used to generate the signature. So Generate SAS token and the URL. And I'm just going to scroll down here to reveal those items. So we have both a Blob SAS token and a Blob SAS URL.

[Video description begins] He clicks a button labeled "Generate SAS token and URL". The text boxes labeled "Blob SAS token" and "Blob SAS URL" displays. [Video description ends]

So I'm going to copy the Blob SAS URL by pointing to the copy icon to the far right of it and choosing the copy option, so click, and it says it was Copied. So now, when I paste that blob SAS URL on my web browser, it will allow me to access the file. Now because it's a no MIME type in the browser, it's just the text file, it actually shows us the contents of the file as opposed to prompting us to download it.
