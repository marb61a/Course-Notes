                    AZ900 Microsoft Azure Cloud Fundamentals 2021
                    Course Notes Part 36


Create an Azure SQL Data Warehouse (Cont)
  - So SQL data warehouse is similar to when you deploy Azure SQL database in that you've got to determine how you need to make a connection into the database
    - Such as from a non-premises environment perhaps where you're running SQL Server Management Studio
    - Again, it's gonna make a connection over standard SQL ports like 1433 and so you'd have to add a firewall exception to allow that to happen
    - Now, on the left, go to the SQL data warehouses view where we can see, we've got our SQL data warehouse database
  - Click on it and open it up, then we can see if we scroll down, in the properties blade, for example, if we go to Quick start
    - We can see that we have a number of tools that we can use so that we can work with data in SQL data warehouse
    - And that is available through the Microsoft Azure SDK, Azure PowerShell and also the Azure SQL Data Warehouse Migration Tool.
  - We also have information about Integration with our application
    - Because the idea is that we'll have some kind of an application that is going to be interested in 
    - Running these types of complex queries and gaining insights from data that is stored and managed by SQL data warehouse
    - We do have a Geo-backup policy which takes a snapshot on a daily basis
    - However, this is kind of unlike the standard Azure SQL database geo-replication
    - Bbecause that same type of geo-replication option is simply not available with Azure SQL data warehouse
  - The other thing to watch out for is if we scroll down as we were talking about and go to Firewalls and virtual networks
    - We can add allowances for which IPs are allowed to make a connection into SQL data warehouse.
    - Again, over port 1433, the standard SQL port, and we can even add our current client's IP address in and just save that if we really wanted to
  - Scroll down, notice here that we've got a preview feature here called Query editor.
    - If we put in the credentials that were specified when we configured the SQL server
    - Then we can go ahead and actually start to peruse and work with some of the data in a very simple way, at least here directly in the portal
    - Expand the tables and see that we've got, for example, a dbo.DimCustomer as a table, and we can even start working with queries
    - Maybe select all of the columns from dbo.dimcustomer, and then run that query, and we'll start to get the results listed here
  - This is just a quick way to look at this, of course, you're going to have an application 
    - Or some kind of a way to hook into this using other tools to actually work with this data properly for analysis.
    - Bear in mind, one of the reasons you might use Azure SQL data warehouse over just standard Azure SQL database is because when you run queries
    - This is not a complex query, so imagine a much more in depth, detailed complex query
    - But when you do run queries here, what's going to be happening is that the query is going to be handled by a specific back end node 
    - That's got its own compute resources, like CPU and memory, as opposed to standard Azure SQL database, which does not support multiple parallel processing
  - The other thing to keep in mind is, you actually have the option of pausing your Azure SQL database warehouse if you're not actually going to use it
    - For the storage portion, you would still be paying, but not for the compute portion
    - You can see up at the top here that we do have in the overview part of the properties blade, a pause button
    - Which we could use to do just that and then we could resume it when we want to continue processing


Azure HDInsight
  - Azure HDInsight is a Big data analytics solution that's hosted in the cloud, and so it's considered a Managed service
    - With managed services in the cloud we're talking about something that's easy to provision and configure 
    - Compared to if you had to set it up yourself manually on-premises
    - Azure HDInsight uses a number of underlying open source frameworks but it does allow for Node clusters working together to process large amounts of data
    - Whether that data is like a real time feed through a data pipeline or whether that data is coming from some kind of massive data storage warehouse.
  - HDInsight Underlying Technologies includes but it is not limited to Apache Hadoop
    - Apache Hadoop is an open source framework that's used for distributed processing clusters
    - Apache Spark is similar in that, it is distributed in parallel processing
    - But what makes it a little bit different is it uses in-memory caching to speed things up.
  - Apache Kafka is another open source component that allows for real time streaming data pipelines to feed HDInsight
    - Another aspect of working with HDInsight is Extract, Transform, and Load or ETL, you might be familiar with this term with other database solutions
    - It's not exclusive to HDInsight, it's more of a standard methodology more than anything else, where we can start by copying a data from source
    - Whether it's a data store of some kind in the database or whether it's real time streamed data.
  - In the next step, for transform, we can convert the data to a different format so it can easily be consumed by the target 
    - That might expect things in a different format, such as dates
    - Finally, we can load the data into some kind of a storage facility, whether it's a data warehouse 
    - Or whether it's going to be treated as a real time data feed that's gonna be fed into some other component
  - What do we use HDInsight for? Well, we know it's about big data analytics, but can we be a bit more specific than that? 
    - While using HDInsight, it can be related to Machine learning or ML
    - Where we can gain insights from vast amounts of data that are fed into it
    - You can run very large petabyte-scale types of queries against this type of information
    - Or it can be automated so the insights are gained based on code that's written, which can result in predictive analysis for future trends
  - On the IoT side of things, the Internet of Things, we can have a large amount of IoT device telemetry that is fed into the HDInsight solutions
    - We can draw conclusion from large datasets, whether those are related to the security of IoT devices or due to the nature of those IoT devices
    - We can draw conclusions, such as those related to monitoring industrial control networks and so on


Deploy an Azure Hadoop Cluster
  - Use the Azure portal to deploy an Apache Hadoop cluster
    - This is useful when you need to have multiple parallel processing for big data analytics
  - To get started here in the portal click Create a resource in the upper left
    - Then choose Analytics because what we are really talking about doing here is using the HDInsight as your offering.
    - Now, when I select HDInsight, I've got to give a name for the clusters.
    - In the example call it hdinsightcluster172, and it's going to use the .azurehdinsight.net DNS suffix by default
    - After a moment we'll have a check mark here that indicates that, that name is valid and unique, then we have to specify the cluster type
    - This is where from the Cluster type drop-down list, we can specify if we want to use a specific framework, in this case, Hadoop
    - It's going to use the Linux operating system.
  - Then we can choose from variations of the version of Hadoop
    - Depending on how we are going to interface with the cluster and what exactly we are going to do with that
    - Go ahead and just leave the default selection.
    - Specify a Cluster login username, which I'm going to do here, and password
    - This is what to can use, for instance to log into the website to view overall metrics and details related to my Apache Hadoop Cluster.
    - And if I plan on using SSH for cluster access, then I can use the SSH username.
  - Here it's set to use the cluster login password also for SSH
    - Tie this into an existing Resource group and specify the appropriate Azure location for my config, after which then click Next
  - Next, for the primary account storage type, choose Azure Storage as opposed to Data Lake variations
    - This is going to be for data that is used by HDInsight as well as for logs that get generated
    - Leave it on My subscriptions for access to that account, then go through and choose a Storage account, choose one of our storage accounts
    - And then it'll make a storage container with the name listed down below here, at this point click Next
  - Once the validation has passed, we'll be able to click Create to initiate my HDinsight Cluster
    - Which in this case, is configured in the back end to use Apache Hadoop, go ahead and click on Create to start the process
  - While that's happening, understand that the next couple of steps would really be for developers
    - Where they would use some kind of a tool to interface with Hadoop to work with the data and workloads related to that data
    - Tools like Microsoft Visual Studio, the Azure storage explorer
    - You could even, for example, use SSH to connect to the cluster and start actually issuing commands based on the Hadoop command syntax
    - For now click on the All resources view on the left, and filter it by hd as a prefix.
    - There's our HDInsight cluster, which we will click on
  - The first thing of interest, in the Overview part of the properties blade is the URL
    - Which we will copy to the clipboard, and go ahead and open that up in another tab here in the web browser
    - We are then prompted to specify the Username and Password that we configured when we set up this cluster in the first place
    - We're going to go ahead and pop in those credentials
    - That's going to give us the cluster website page where we can start viewing a bunch of details
    - For example, from here we can go to Hosts, where we can get a list of a lot of the hosts that are being used here within our cluster for Apache Hadoop
    - If there are any alerts, as we can see it listed up here at the top in red, and also by a specific host here
    - Then we can click to read any of those specific ideas.
  - We could see here that some of these alerts are related to connectivity issues because nothing has been actually done in this cluster at all thus far
    - If we go back to the Dashboard, we'll see how we can get an overall usage of the data nodes that are available.
    - The Hadoop distributed file system, or HDFS disk usage among the nodes in the cluster
  - Now you will use a variety of different tools to start loading data into Apache Hadoop as we mentioned
    - This is what we would do at the administrative level
    - From this point forward, it would be up to developers to interface with the Hadoop cluster to present data and workloads to be processed.
