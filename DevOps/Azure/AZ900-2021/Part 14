                    AZ900 Microsoft Azure Cloud Fundamentals 2021
                    Course Notes Part 14


Connect to Azure Virtual Machines
  - Once you've deployed a virtual machine into the Azure cloud, how do you connect to it?
  - There are a few ways to do it, and our perspective in this demo will be how to do it from outside of Azure. 
    - In other words, from over the Internet from our on-premises network. 
    - Here in the Azure portal, I've already clicked on the Virtual machines view on the left. 
    - There are two virtual machines here, one running Linux, one running Windows, one is in the midst of being created and one is running.
  - Our Windows virtual machine should currently running. 
    - First step to talk about is about how to connect to a Windows virtual machine deployed in the Azure cloud. 
    - Click on the link for the name of that Windows virtual machine.
  - What is of interest in here is its public IP address
    - Configure it with a public IP so I can reach into it from over the Internet. 
    - Hover over it, there should be the little two pieces of paper icon over on the right, which implies copying, like the tip tells me. 
    - Click to copy that to my Windows clipboard.
  - Next is to fire up the Remote Desktop Protocol, or RDP client, here on my Windows 10 station.
    - Paste that public IP address assigned to my Azure Windows virtual machine, and then I'm going to click Connect.

[Video description begins] A warning message displays in a window. He clicks the check box captioned Don't ask me again for connections to this computer, and clicks the Connect button in the window. [Video description ends]

I'm going to tell it, Don't ask me again for connections to this computer, about trusting it, and I'll click Connect again, Now, notice I'm getting a message that says, Remote Desktop can't connect to it. Well, we know it's running, and we know that that is a valid IP address, and obviously we are connected to the Internet to see this screen in the first place. So it's probably related to some kind of blockage, not allowing port 3389 traffic, which is what is used by Remote Desktop Connections. So I'm going to go ahead and click OK, and let's go back into Azure and take a look at this.

[Video description begins] A window showing the progress of the connection displays. The text: Initializing remote connection- displays there. [Video description ends]

[Video description begins] He closes all windows and returns to the Azure page. [Video description ends]

So I'm still in the portal, I'm still looking at the Properties blade for my Windows virtual machine. I'm interested in clicking on Network in the Properties blade.

[Video description begins] He clicks Networking in the Settings group in the navigator. [Video description ends]

On the right I can see that VNet, so virtual network inbound traffic, is allowed. It says allowed over on the right, so is load balancer traffic, but everything else is denied. Well, no wonder we can't make a connection. So I'm going to go ahead and click Add inbound port.

[Video description begins] The Networking window opens on the right. The Inbound port rules tab is active now and it shows a table with all the active ports. The table has columns titled PRIORITY, NAME, PORT, PROTOCOL, SOURCE, DESTINATION, and ACTION. There are 3 ports listed in the table. 2 ports have Allow in the ACTION column, and 1 port has Deny. [Video description ends]

[Video description begins] A new window titled Add inbound security rule displays. [Video description ends]

The source here can be configured as a specific IP address, or an Application security group, or I'll leave it here on Any. Then I can specify the source port.

[Video description begins] The first field is Source and it has 4 options in the drop-down menu. They are Any, IP Addresses, Service Tag, and Any. The default value is Any. [Video description ends]

Well that's fine, but the destination here, I can specify as being any or a specific IP address. I'm going to leave it here on any, but the destination port range is important.

[Video description begins] He leaves the default value of * in the Source port ranges field. [Video description ends]

[Video description begins] There are 4 options in the Destination field drop-down menu. They are Any, IP Addresses, VirtualNetwork, and Application security group. Any is the default value. [Video description ends]

Here I want to allow traffic to 3389.

[Video description begins] He clears the Destination port ranges field and types 3389. [Video description ends]

And that's going to be TCP based, or I could specify UDP, or just leave it on Any, and in this case I want to allow not deny it.

[Video description begins] He selects Any in the Protocol field and Allow in the Action field. [Video description ends]

So I have to also specify the priority. Notice the default priority here is 100. Let's also change the name here to reflect the new port number for Remote Desktop Protocol.

[Video description begins] He types Port_ 3389 in the Name field. [Video description ends]

Then I'm going to go ahead and click Add. Now, that's going to add it to the list of inbound rules that we see listed in the background.

[Video description begins] A window with the text- Creating security rule- pops up in the top-right corner of the window. [Video description ends]

It's important to note that the rule is at the top of the list, because these rules are checked from top down. And so we don't want this Port_3389 rule underneath the DenyAllInBound, because it would never get used. So let's go ahead and try to Remote Desktop into this virtual machine again. Well, this is much better.

[Video description begins] The newly-added rule is at the top of the list. [Video description ends]

[Video description begins] A window titled Windows Security displays. It has the text - Enter your credentials- and 2 fields. The first field has cirving and the second field has an encrypted password. [Video description ends]

I've specified my username and password that I defined when I deployed that virtual machine, and I'm going to go ahead and click OK. We can see here we're being asked to trust the identity of the computers. I'll say, Don't ask me again, I trust it, and I'll choose Yes.

[Video description begins] A window shows the progress of the remote desktop connection. Then another window titled Remote Desktop Connection displays. He clicks the check box in the window, and the Yes button. [Video description ends]

And after a moment, we can see that we are actually now being sent into a Remote Desktop session of that Windows Server 2016 virtual machine, which is running in the cloud.

[Video description begins] The 2 windows close and a new desktop screen appears. [Video description ends]

And before you know it, we're in. So it's just another Windows virtual machine. In this case, it happens to be running in the cloud.

[Video description begins] A panel titled Networks display on the right. It is asking for access permissions. He clicks the Yes button. [Video description ends]

Much quicker and easier to set up than we might do if we were trying to set this up ourselves manually on-premises. And if I go into the Start menu here, and let's say we go to a command prompt. So I'm going to run cmd. You can see in the background it's automatically launching the Server Manager GUI tool, which is normal for new installations of the Windows Server operating system.

[Video description begins] He clicks Run from the Start menu and types cmd. Then he clicks cmd from the best-matched results. A new window titled Select Administrator C:\Windows\System32\cmd.exe displays. [Video description ends]

But what I want to do here is simply type ipconfig. And notice here that we don't see a reference to the public IP address we connected to, because that's not configured within the virtual machine operating system, it's a separate Azure resource.

[Video description begins] At the command prompt, he types and executes the following command: ipconfig. The output displays. It is the Windows IP Configuration of the remote machine. He highlights the IP address 10.1.1.4. [Video description ends]

However, what we do see is an IP address that's been assigned based on the subnet address range that we deployed the virtual machine into. So that's it, we are now able to get into our Windows virtual machine. Well, that's fine for Windows, but what about Linux? Back here in the portal, I've gone back to my Virtual machines view and we can see our Linux virtual machine now has a status of Running.

[Video description begins] He closes the command prompt window and returns to the Azure portal. The Virtual machines window displays here. [Video description ends]

So I'm going to click on it so that we can see its public IP address.

[Video description begins] He clicks eastlinuxvm 1 from the list and its window opens. He points to the Public IP address 52.235.38.158. [Video description ends]

Now, what you want to do is use an SSH client of some kind. So if you're using a Linux machine already, you can use the SSH command line to connect to your Azure Linux virtual machine. Or, I'm using Windows, I could also use the free PuTTY tool. I've downloaded and installed the free PuTTY tool. And when I fire it up, I can paste in the public IP address from my Linux VM running in Azure, along with Port 22, which is used by SSH.

[Video description begins] He opens the PuTTY Configuration tool. It has a navigation pane on the left and Basic options for your PuTTY session on the right. The Linux IP address displays in the Host Name for IP address field, and the number 22 displays in the Port field. Connection type field has the SSH radio option clicked. [Video description ends]

I've already gone, made some other changes, like the window appearance to increase the font, and I've saved it into a setting called Azure Linux VM.

[Video description begins] In the navigation pane, he clicks Appearance in the Window menu group. New settings display on the right. [Video description ends]

So I can load that up at any point in time.

[Video description begins] Then he clicks Session in the navigation pane. Azure Linux VM displays in the Saved Sessions field. 3 buttons- Load, Save, and Delete are also present here. [Video description ends]

Now, to make the SSH connection, I would click Open.

[Video description begins] He clicks the Open button in the window and a new window with the IP address in the title bar opens. The window is blank. [Video description ends]

And it's trying to make the connection, but remember with the Windows virtual machine there was no default inbound port enabled? That's the problem here. Let's go back and let's explore that here in the Linux virtual machine in Azure.

[Video description begins] He closes the window and returns to Azure. The Networking window displays here. [Video description ends]

So I'm going to go to networking and again, we can see our inbound ports. There's no allowance here for port 22, so I'm going to click Add inbound port, and I'm going to specify a destination port of 22, so TCP 22 for SSH allow.

[Video description begins] The Add inbound security rule window opens and he types 22 in the Destination port ranges field, selects TCP in Protocol, and Allow in Action. He types Port_22 in the Name field. [Video description ends]

And I'm going to change the name here to reflect the port number, and I will go ahead and click Add. And we can now see the rules at the top. It's allow rule, so it should allow our traffic in.

[Video description begins] Port_22 displays at the top of the inbound ports list. [Video description ends]

Let's go back and test it out. So back here in PuTTY, let's try again.

[Video description begins] He again opens the PoTTY configuration window and clicks the Open button. [Video description ends]

I'm going to click Open. This time we get something immediately.

[Video description begins] The IP address window opens again and a security alert message displays in a window. He clicks Yes in the window. [Video description ends]

So it's asking me, do we trust the unique fingerprint for the server, because it's the first time we've connected. I'll choose Yes, and it wants me to log in. Well, I've specified credentials when I deployed this. So we're using username and password authentication as opposed to public key authentication, which is a choice when you deploy a Linux virtual machine.

[Video description begins] He returns to the IP address window and the text- login as- displays there. He types cirving and presses Enter. A line of text appears. It reads: cirving @52.235.38.158's password. [Video description ends]

So it now wants the password for this account. So I'll go ahead and supply that. And after I've done that, if the credentials are correct, we will be logged into the virtual machine remotely over the Internet. And we can see in fact, that's been done, and if I do an ifconfig to show the interface, we can see not the public IP, just like with Windows, but rather the private IP, which is derived from the address range assigned to the subnet that this Azure virtual machine was deployed into.

[Video description begins] The command prompt changes to cirving@eastlinuxvm1. He types and executes the following command: ifconfig. The output displays. It shows the configuration details of the remote machine. He highlights the public IP address 10.1.1.5. [Video description ends]

Azure Virtual Machine PowerShell Management

[Video description begins] Topic title: Azure Virtual Machine PowerShell Management. The host for this session is Dan Lachance. [Video description ends]

In this demonstration, I'll be using PowerShell to create a new Azure virtual machine. Here in the PowerShell ISE, I've already got a script ready to go. In the first line of the script, it's actually lines 1 and 2 but I've got the back tick symbol here as the line continuation character. I've got a statement using a PowerShell cmdlet called Get-AzVMImage.

So this is important because we can specify the image name that we want to build the new virtual machine from. And so, let's go ahead and highlight that first set of code for Get-AzVMImage. And I'm going to go ahead and run that selection.

[Video description begins] A Windows PowerShell ISE window displays. There is a menu bar and a tool bar at the top. A tab titled it_clazfd_ enus _06.jps1 is active. The screen below is divided horizontally into 2 scrollable sections. The top section has lines numbered from 1. Code is written in lines 1 through 19. In lines 1 and 2, the following command is written and highlighted: Get -AzVMImage -Location "Central US" -PublisherName "MicrosoftwindowsServer" -Offer "windowsserver" -Skus "2012-R2-Datacenter". He clicks the Run button denoted by a page and a right-facing arrow. The highlighted command displays in the bottom section of the window. [Video description ends]

And down below, we can see some of the images that are available here based on Microsoft Windows Server.

[Video description begins] The command output displays in the section below. It is a table with all the images and their Version, FilterExpression Skus, Offer, PublisherName, Location, and Id. [Video description ends]

So we can see here that the SKU names are listed in the list. However, I'm going to go ahead and comment those back out in my script. And I'm going to clear the screen down at the bottom.

[Video description begins] He types a # in the beginning of lines 1 and 2. In the bottom section, he types the command cls at the command prompt. [Video description ends]

Now to actually work with a virtual machine. To build it through PowerShell, the first thing I'm doing is building a variable here called $creds for credentials. And we're using the PowerShell Get-Credential cmdlet, which will pop up a graphical dialog box where I can specify both a username and a password. And so that's going to be stored in the $creds variable that I will refer to later.

[Video description begins] He highlights the following command in line 6: $creds = Get -Credential -Message "New VM username and password?". [Video description ends]

Then I'm creating a variable called vmconfig. And I'm setting the resource group that I want to deploy this virtual machine into. I'm setting the name of the virtual machine, the location, the image I want to create the virtual machine from, in this case Win2016Datacenter. I'm also giving a name for the public IP address resource. Here's the $creds I'm passing for the credential. And then I'm opening port 3389. So what these all are in the vmconfig section here, ResourceGroupName, Name,

[Video description begins] He highlights the following code written in lines 8 through 17, line by line: $vmconfig = @ { ResourceGroupName = 'rg1' Name = 'vm33452' Location = 'Canadaeast' ImageName = 'win2016Datacenter' PublicIPAddressName = 'eastwindowsvm2_ pubIP' Credential = $creds OpenPorts =3389 }. [Video description ends]

Location, ImageName, these are parameters. And I could just as well use dash in front of each of these parameter names, and then pass the values. All I'm doing here is organizing it into a single resource or a single variable rather called $vmconfig that I simply refer to here. And I pass it to the appropriate cmdlet. The appropriate cmdlet here is New-AzVM to build a virtual machine based on the configuration defined above, the parameters, and their values.

[Video description begins] He highlights the following command in line 19: New - AzVM @vmconfig. [Video description ends]

So let's go ahead and run this entire script by clicking the Run Script button.

[Video description begins] He clicks the run script button denoted by a right-facing arrow. A new window titled Windows PowerShell credential request displays. It has a line of text, New VM username and password?, and 2 fields- User name and Password. [Video description ends]

And sure enough, it pops-up and it says, new VM username and password, that comes from -Message up above. So after I've specified those credentials, I'll go ahead and click OK. And it's going to go ahead and create my virtual machine based on the settings defined here in PowerShell.

[Video description begins] The bottom section shows a progress bar with the text : Creating Azure resources. [Video description ends]

Once the script completes, we're going to go ahead and switch over to the Azure portal to check for our new virtual machine, vm33452.

[Video description begins] Once the process completes, configuration of the VM resource displays. [Video description ends]

And sure enough, here in the portal, if I go to the Virtual machines view, which I've done. We can see our new virtual machine listed, and it's running. So by default, when you deploy a virtual machine, even through PowerShell, its state is set to Running. So it's started up, and ready to go.

[Video description begins] He opens the Microsoft Azure portal in a browser window. The Virtual machines window is open now and vm33452 displays in the list of VM there. Its Status is shown as Running. [Video description ends]

Azure Virtual Machine Scale Sets

[Video description begins] Topic title: Azure Virtual Machine Scale Sets. The host for this session is Dan Lachance. [Video description ends]

Microsoft Azure virtual machine scale sets are used for load balancing, where we have a series of identical virtual machines working together to serve up some kind of an application. It also supports auto-scaling. So for example, depending on the demand, we can increase the number of backend virtual machines supporting the application through the scale set.

So we can control this through the instance count property. This is the number of instances that are running in the scale set, and we can even set that to a minimum value. Pictured on the screen, we have a diagram where, on the left, we've got a client that is trying to access a web application over port 80.

[Video description begins] Screen title: Azure Virtual Machine Scale Sets. On the left is a man's icon labelled Client (Port 80). On the right, are three rectangular boxes, one inside the other. There are 3 boxes inside the smallest rectangle. All the 3 boxes have the text VM (port 80) written inside them. The smallest rectangle depicts Subnet . The next rectangle depicts VNET. [Video description ends]

So that connection goes to the load balancer. So if the client is typing in www.app.com, that is resolving to the public IP address of our load balancer. And so that's how the request gets from the client to the load balancer.

[Video description begins] An arrow points from the client to a Load Balancer, depicted by a load balanced on a circle. [Video description ends]

Now, the load balancer periodically checks that the backend virtual machines, of which we have three here, listed on the far right. It periodically checks to make sure that they are responsive. Because if we have a virtual machine that is not responsive, then client requests are not forwarded to it. Otherwise, we have three virtual machines in the backend in our scale set, in this diagram, that can be used to service client request. And so it increases performance while providing high availability. Because if we have virtual machines in the backend that aren't running, then client requests are simply directed to other ones that are still responsive. We manage our scale sets using the Azure portal.

[Video description begins] 3 arrows emerge from the load balancer and go towards the 3 rectangles on the right. [Video description ends]

[Video description begins] Screen title: Scale Set Management. [Video description ends]

We can use the Azure CLI. For example, you can use the az vmss virtual machine scale set create command to create one. In PowerShell, the equivalent to create a new virtual machine scale set would be New-AzVmss. And then finally, we can also use an ARM template, where we can specify our resource type of Microsoft.Compute/VirtualMachineScalesets.

Deploy an Azure Virtual Machine Scale Set

[Video description begins] Topic title: Deploy an Azure Virtual Machine Scale Set. The host for this session is Dan Lachance. [Video description ends]

In this demonstration, I will use the Azure portal to deploy a new Azure Virtual Machine Scale Set. To get started, I'm going to click Create a resource over on the left, and

[Video description begins] The Microsoft Azure portal homepage displays in a browser window. He clicks Create a resource in the navigation pane and a window titled New opens. [Video description ends]

I'm going to search for the word scale. And here it is, Virtual machine scale set. I'm going to go ahead and choose that, and then I'll click Create.

[Video description begins] A Virtual machine scale set window opens. It has a brief description of scale sets and a Save for later button. The publisher name and documentation links are also present. [Video description ends]

The purpose of creating this virtual machine scale set is to make sure that we have a number of identical backend virtual machines supporting an application. So, basically, we're going to have a frontend load balancer that supports this capability.

[Video description begins] A new window titled Create virtual machine scale set displays. It has a section titled BASICS with many fields. [Video description ends]

So I need a name for my virtual machine scale set. I'm going to call it webapp3 and vmss, virtual machine scale set. So because the backend virtual machines all need to be identical, when you define the scale set, which we're doing, you need to choose the Operating system disk image, okay?

[Video description begins] He types webapp3vmss in the Virtual machine scale set name field. [Video description ends]

So, let's say it's going to be Windows Server 2016 Datacenter. Of course, you could also have some kind of a custom image that has an application loaded within it. Next thing I'm going do here is deploy it to a resource group, and specify the location, and down below I need to specify credentials. Username and password, in this case, for the Windows operating system since that's the image I selected for my scale set. Notice this is different than if you create an Azure load balancer, which we'll see in another demo.

[Video description begins] He clicks the drop-down arrow in the Operating system disk image field. A list of OS display, grouped under Windows and Linux. [Video description ends]

[Video description begins] He leaves the default value of Pay-As-You-Go in the Subscription field, and selects Rg1 in the Resource group field, and Canada East in the Location field. [Video description ends]

[Video description begins] He types cirving in the Username field and the password in the Password and Confirm Password fields. [Video description ends]

Because with an Azure load balancer, as opposed to specifying the operating system image and the credentials here, the load balancer can reference existing virtual machines that are already out there. And they don't even all have to be identical. So that's a little bit different than what we're doing here with the virtual machine scale set.

Here's the Instance count property, where it defaults to having 2, but I can change that. I can also set the instant size, which determines things like the amount of CPU power and the amount of memory. I'm going to click Change size, I'll just choose something very basic, let's say 1 VCPU and 1 GB of RAM.

[Video description begins] He selects 81s VM SIZE in the Select a VM size window. [Video description ends]

And I'll go ahead and select that. And as we scroll further down, I'm going to let it use managed disks, which is by default. Under advanced settings, I can determine whether I want to allow scaling beyond 100 instances. I shouldn't need that, so I'm not going to turn that on.

[Video description begins] He clicks Show advanced settings link below the Use managed disks field. Enable scaling beyond 100 instances field shows up and he leaves its default value of No unchanged. [Video description ends]

And I can also enable autoscaling. Autoscaling changes the number of backend instances automatically based on things like CPU busyness or threshold.

[Video description begins] He selects Enabled in the Autoscale field. [Video description ends]

So here, we can see that if we've got a CPU threshold above 75%, then we can increase by a virtual machine. One VM based on the image that we specified when we were creating this.

[Video description begins] In the Scale Out section, he points to the value in the CPU threshold field. It is 75. Then, he points to the value in the next field, Number of VMs to increase by. It is 1. [Video description ends]

That's for scaling out, adding virtual machines to support a busy workload. The opposite is scaling in, so both of them are horizontal scaling, but scaling it reduces the virtual machines based on a CPU threshold. And this is a good setting. It's important because we don't want to have virtual machines running we don't need, because we're paying for that. And, as I go further down,

[Video description begins] In the Scale in section, he points to the value in the CPU threshold field. It is 25. Then, he points to the value in the next field, Number of VMs to decrease by. It is 1. [Video description ends]

I'm going to decide whether I want to use an application gateway for load balancing, although I don't have any already defined. I could also choose load balancing as a solution, while I am defining the scale sets, I'm really doing two things at once.

[Video description begins] He scrolls down and reaches the field Choose Load balancing options. He selects the radio option Load balancer. [Video description ends]

So I can give a public IP address name for the load balancer IP address. So, if we scroll back up at the top here, the name of this scale set is webapp3vmss. I'll copy that, and I'm just going to go ahead and use that as part of the name here, I'll call it pubIP at the end.

[Video description begins] He pastes the Virtual machine scale set name in the Public IP address name field and adds IP after it. [Video description ends]

And then I can use a domain name label to which the following suffix listed down here, .canadaeast, that's my region, .cloudapp.azure.com will be added.

[Video description begins] He pastes the Virtual machine scale set name in the Domain name label field. [Video description ends]

Of course, that can be customized, but I'll accept that default. And then finally, I have to choose a virtual network. So I'm going to choose EastVNet1, and I've got 1 subnet, that's important because that's where we will be deploying these virtual machines. They're going to assume IP addresses from that subnet address range.

[Video description begins] He selects EastSubnet 1 (10.1.1.0/24) in the Subnet field. [Video description ends]

So it's important to make the correct selection. Do we need a public IP address for each and every instance, it's set to off? I'm going to say, no, because these are running in the backend to support an app. And you might wonder, how do I gain access to them if I need to manage them? Well, you might have another virtual machine outside of the scales set running in that subnet that does have a public IP address.

And, so you can connect to it, for example, from on-premises, and once you're connected to it, you would be on the private network. And you could then manage these additional virtual machines from this scale set. So it means having less public IP addresses, which saves on cost. Okay, so the next thing we're going to do is just click Create. So now let's go to the All resources view, and let's take a look at our newly created scale set.

[Video description begins] He clicks All resources in the navigation pane and its window opens on the right. [Video description ends]

So I'm going to filter this view for vmss, and here we can see we've got our webapp3vmss virtual machine scale set. We can also see the load balancer and the public IP address resources.

[Video description begins] He types vmss in the Name field. 3 items display. They are the VM scale set he just created , the load balancer, and the Public IP address. [Video description ends]

I'm going to go ahead and click on the virtual machine scale set to open up its Properties blade. And in the Overview section on the right, we can see the public IP address here, that's actually for the load balancer component.

[Video description begins] He highlights the Public IP address of the webapp3vmss virtual machine. It is 40.80.249.17. [Video description ends]

And if I click on Instances on the left, we can see the virtual machine instances here in their current state.

[Video description begins] He clicks Instances in the Settings group. 4 instances display on the right. [Video description ends]

And if I were to click, for example, on Scaling, this is where during the creation we had the option of configuring autoscaling. So for scaling out and also for scaling in.

[Video description begins] He clicks Scaling in the Settings group and points to the Scale mode field on the right. [Video description ends]

If I were to click Operating system, we can see here that it's the Windows Operating System, based on the image we selected. Same with the sizing, we can see the size of the virtual machine, which determines the underlying horse power, like the number of VCPUs and the amount of RAM.

[Video description begins] He clicks Operating system and then Size. The VM size he selected before is highlighted [Video description ends]

I'm going to close out of that, and I'm going to click on our load balancer that was created for the scale set.

[Video description begins] He returns to the All resources window and clicks the second item in the list. [Video description ends]

And notice, again, that we've got the public IP address here, that's the frontend for client connectivity to the backend configuration.

[Video description begins] The load balancer window opens on the right, and he highlights the Public IP address field value. It is 40.80.249.17. [Video description ends]

And notice that if I were to click the Backend pools here, we can see that we've got a backend pool that was created for us automatically, and here are the virtual machine instances. And, of course, we can see the private IP addresses that they've been assigned.

[Video description begins] He clicks Backend pools in the Setting group. A list of 4 instances displays on the right. Their names, NETWORK INTERFACE, and PRIVATE IP ADDRESS properties display. [Video description ends]

Now while we've got virtual machines instances listed here, if I click on the Virtual machines view over on the left, I don't see virtual machines here that result from the use of a scale set.

[Video description begins] He clicks Virtual machines in the navigation pane of the Azure home page. [Video description ends]

Azure Load Balancing

[Video description begins] Topic title: Azure Load Balancing. The host for this session is Dan Lachance. [Video description ends]

The Azure Load Balancer is used to take incoming client requests and spread them out amongst backend virtual machines that support an application. This means we have a result of increased performance because we've got more virtual machines to service client requests. It also supports high availability. So client requests bypass unresponsive VMs.

What this means is that the load balancer is configured to periodically probe backend virtual machines to make sure they respond. And for those that do not respond, client requests will not rerouted to those specific instances. We can configure a public load balancer. That means that the load balancer is Internet-facing and it will be assigned a public IP address.

And so when clients enter the URL for a web app, it needs to resolve to that load balancer public IP address. So that's for inbound Internet traffic. But we can also define an internal load balancer that would be used not over the Internet, but instead within an Azure VNet, maybe for some kind of internal line of business application running in the cloud.

It can also be used for on-premises traffic that's coming into Azure, for example, through a VPN in a hybrid cloud configuration scenario. With the Azure Public Load Balancer, we have health probes that verify the backend virtual machine responsiveness. And that actually gets configured within what's called a load balancer rule. You'll see that when you open up a load balancer in Azure and take a look at its properties blade. So in the diagram, we've got clients on the Internet that make a connection to the load balancer, which in turn will send a request to backend hosts.

[Video description begins] A diagram displays progressively. At the top is a cloud icon. An arrow emerges from the icon bottom and leads to a load balancer icon. 3 arrows emerge from the bottom of the load balancer icon and lead to 3 computer icons depicting 3 clients. [Video description ends]

Load balancers in Azure can be managed like most resources in a number of ways, such as through the Azure Portal, the web GUI, or the Azure CLI.

[Video description begins] Screen title: Azure Load Balancer Management. [Video description ends]

So for example, to create a load balancer in the CLI, we could issue the az network lb, for load balancer, create. In PowerShell, the equivalent would be the New-AzLoadBalancer cmdlet. And if you're using an ARM template to deploy load balancer resources, then you would refer to the Microsoft.Network/LoadBalancers resource type.

Configure Azure Load Balancing

[Video description begins] Topic title: Configure Azure Load Balancing. The host for this session is Dan Lachance. [Video description ends]

In this exercise, you're going to start by using the Azure portal, so the web graphical interface, to create a storage account. Then, after the account is created, upload a file to it using the portal. Then you will create an Azure file share, which is essentially a cloud-based shared folder. Next, you will create a key vault and then store a secret within the vault. Think about how you might approach each of these items by pausing the video.

And then after you've thought about it, come back to view the solutions. Here in the portal, I can create a storage account by going to the Create a resource link over in the upper left. Then I can choose the Storage category, and choose Storage account.

[Video description begins] The Microsoft Azure portal home page displays in a browser window. He clicks the Create a resource menu from the navigation page and then Storage category in the Azure Marketplace column displayed in the New window. Then he clicks Storage accounts in the Featured column. [Video description ends]

So I have to specify things like a resource group into which to deploy this new Storage account.

[Video description begins] Create storage account window displays. He selects Rg1 in the Resource group field. [Video description ends]

Storage account name, not using uppercase letters. So let's say we call this stor1990. Then I have to specify the Azure location to deploy this to.

[Video description begins] He selects Canada east in the Location field. [Video description ends]

And I have to determine whether I want it to be Standard or Premium, and the account kind. So if I want BlobStorage, I would choose that.

[Video description begins] He leaves the default value of Standard in the Performance field and selects Blob Storage in the Account kind field. [Video description ends]

Let's say I want the Cool access tier. And assuming that's all I want to do, I would just review my settings, let the validation succeed, and then create the storage account.

[Video description begins] A window with a progress bar and text- Submitting deployment- displays in the top-right corner of the screen. [Video description ends]

Once that's done, I can go to the Storage accounts view on the left, and I will see my newly created storage account. I may have to refresh. And here I'll see my newly created storage account, stor1990. If I click it, from within here, I can upload content.

[Video description begins] He clicks Storage accounts in the main navigation panel and stor1990 displays in the list of items on the right. When he clicks its name, its properties window displays. [Video description ends]

For example, by going to Blobs, where I could create a container, which I will call pics.

[Video description begins] He clicks Blobs in the Blob service group in the navigation pane. A New container window opens on the right. [Video description ends]

I'll leave it as Private, no anonymous access to the content.

[Video description begins] He types pics in the Name field and leaves the default value of Private in the Public access level field. [Video description ends]

And within pics, I can then select to upload content.

[Video description begins] He clicks the OK button and pics blob displays in the content list. He clicks its name and pics window opens. He clicks the Upload button on the top and an Upload blob window appears on the far-right. [Video description ends]

From here, I can then click the Select a file icon over on the far right.

[Video description begins] He clicks the folder icon next to the Files field. [Video description ends]

And once I've specified a file, I can click Upload to upload that file.

[Video description begins] He selects the file CustomerTransactions.xlsx and clicks the Upload button. The file name displays in the Current uploads section of the Upload a blob window. A window with the text : upload completed for CustomerTransactions.xlsx appears in the top-right corner of the screen. [Video description ends]

Now, the next thing I need to do is create an Azure file share. Now, what we can do is within the Properties blade for a storage account, we can work with files. Now, notice here as I look through the Properties blade of this, we're looking at the Blobs within the storage account. So if I back up one level, let's go back into that, as you scroll down through, sometimes in a storage account, depending on how it was created, you'll see the Files option. Other times, as in this one, you will not. That's because of the way that this was created for Blob storage.

[Video description begins] He closes the cross on all the 3 open windows and reaches the Storage accounts window. He clicks stor1990 in the list and scrolls through its navigation pane, looking for the Files menu. [Video description ends]

So if I were to open up a different storage account, like stor1989, and scroll down. Notice here that the Files service is available.

[Video description begins] He closes this window and goes back to the Storage account window. He clicks stor1989 and its properties page displays. He locates the Files menu in the File service group in the navigation pane. [Video description ends]

So I'm going to go ahead and click Files. I'll click File share and call it something.

[Video description begins] He clicks File and its window opens on the right. He clicks the File share button with a large plus icon and a File share window opens on the far right. There are 2 fields -Name and Quota, and 2 buttons- Create and Discard. [Video description ends]

So for example, if this is going to be for budgets, then I would call it budgets. And for the Quota, maybe I'll set a size of 4 gigabytes, and I'll click Create.

[Video description begins] A window with the text - Successfully created storage file share- appears in the top-right corner of the screen. Budgets file share displays in the properties window now. [Video description ends]

Now, when I go into budgets, I then have to bear in mind that if I click Connect, here I can see the commands that could be used to connect to it from either an on-premises machine or even a virtual machine deployed in the Azure cloud.

[Video description begins] A budgets window opens. It has buttons- Connect, Upload, Add directory, Refresh, Delete share, and Quota. Location is budgets and no files are listed below. [Video description ends]

[Video description begins] A Connect window opens in the far right. It has 3 tabs- Windows, Linux, and MacOS. Windows tab is active now. Drive Letter is Z and 2 commands displays with a copy icon beside them. [Video description ends]

For instance, using the net use command on a Windows host to map drive letter Z to this specific shared folder. The last thing I need to do here is create a key vault and create a secret within it. I'm going to choose Create a resource, and I'm going to type in key vault, and I'm going to click Create.

[Video description begins] He clicks Create a resource in the main navigation pane and types key vault in the Name field. Then, he selects Key vault from the drop-down results and a Key Vault window opens. [Video description ends]

So I have to give it a name, I'm going to call this kv1991. And I'm going to deploy this into an existing resource group, and I'll accept all of the other defaults, and I'll click Create.

[Video description begins] In the Create key vault window, he types kv1991 in the Name field and selects Rg1 in the Resource group field. [Video description ends]

Now, a key vault needs to have secrets in it that are accessed by code. Which the code might need in turn to authenticate to other services elsewhere. So if I go to All resources, my view, and I filter, let's say by kv, we'll see our new vault, kv1991.

[Video description begins] When the key vault kv1991 is created, he closes the window and clicks All resources in the main navigation pane. He types kv in the Name field to filter by name. kv1991 displays in the results below. He clicks kv1991. [Video description ends]

Within it, I can click Secrets if all I want to do is to find, for example, a secret password as opposed to security keys, cryptographic keys, or PKI certificates.

[Video description begins] He clicks Secrets in the Setting group in the kv1991 navigation pane. No secrets display on the right. [Video description ends]

So I'm going to choose Generate/Import, and I'm going to manually create a secret here called secret2. And I'll put in a secret value here, and then I'll click Create.

[Video description begins] A window with the text - Deployment succeeded- displays in the top-right corner of the window. He closes it and clicks the Generate/Import button at the top of the window. [Video description ends]

[Video description begins] A Create a secret window opens. The Upload options contains Manual. He types secret2 in the Name field and types a value in the Value field. A window with the text- Creating the secret 'secret2' - appears at the top of the screen. Secret2 displays in the list of secrets in the kv1991 storage vault. [Video description ends]

Now, here within the portal, I can actually click on secret2 and follow down through that to the point where I can click the Show Secret Value button to actually expose that key vault secret item.

[Video description begins] When he clicks on secret2, its properties window displays. It shows 1 item in the CURRENT VERSION section with a status of Enabled. [Video description ends]

[Video description begins] When he clicks the item, its window opens. He scrolls down to the Secret section and clicks the Show Secret Value button. The field below displays the text testing, and this is the value he typed while creating secret2. [Video description ends]

Azure Serverless Computing

[Video description begins] Topic title: Azure Serverless Computing. The host for this session is Dan Lachance. [Video description ends]

Microsoft Azure has a number of service offerings that are under the classification of serverless computing, but what does this mean? Because in the end, there's always an underlying server that's used, for example, to support a database application and code that's running. However, we're talking here about automated server deployment and management, what this means is a managed service. So that we don't have to worry about actually deploying a virtual machine and the operating system and the tools within it that will run our code.

So, really we're talking about focusing more on code and applications that result from that. So, really this is of primary interest to developers. An example of this would be working with what are called Azure functions, which we'll talk about in more detail later. Azure functions allow you to create and run code on-demand in the cloud without having to worry about provisioning a server that has the appropriate engine that can run that code. It's taken care of for you.

Now we can implement Azure functions in a number of different ways. It could be used for a web application, it could be a mobile device app that we're developing, that is configured to talk to Azure resources through our functions. We could look at Internet of Things, or IoT, received data, such as through the Azure IoT Hub. In Azure, that could trigger a function that we've created as an Azure function.

And so the key is here, we've got a container, so to speak, in which we can run our code without having to define the underlying server operating system details to support the running of that code. You might even have an Azure function through serverless computing that takes a look at files that people might upload to an Azure storage account. And when that file is uploaded, that is a trigger that fires off the Azure function that maybe categorizes or adds metadata to that file or does something specific with it.

Deploy an HTML Azure App Service

[Video description begins] Topic title: Deploy an HTML Azure App Service. The host for this session is Dan Lachance. [Video description ends]

In this demonstration, I'm going to use the Azure portal to deploy an HTML Azure app service, in other words, an HTML website.

[Video description begins] The Microsoft Azure portal home page displays in a browser window. [Video description ends]

So here in the Azure portal, I'm going to start in the upper left by clicking Create a resource. And then under the Web category, I'm going to choose Web App.

[Video description begins] In the navigation pane, he clicks Create a resource and the New window displays. He clicks Web in the Azure Marketplace column, and a list of apps with their logos display in the Featured column. He clicks Web App from here. [Video description ends]

I'm going to call this web app webapptest172. And notice it that's going to be part of the url, because it's going to add .azurewebsites.net as a DNS domain name suffix. Although I can configure custom DNS domain names if I wish.

[Video description begins] The Web App window displays. He types webapptest172 in the App name field. The same text gets displayed in the Resource Group below too. [Video description ends]

I'm going to stick with that one, and I'm going to put this in an existing resource group. Although you could build a brand new resource group into which you deploy all of the items that will be related to this web application.

[Video description begins] He clicks Use existing in the Resource group field, and selects Rg1 from the drop-down menu. [Video description ends]

I can choose either Windows or Linux for the backend operating system hosting my application, my web app. I'm going to leave it as just code, as opposed to using a darker image to run the app.

[Video description begins] He leaves the default value of Windows in the OS field and Code in the Publish field. [Video description ends]

And then I have to deal with the service plan. And I'm going to click on Service plan and we're going to create a new one.

[Video description begins] He clicks the arrow in the App Service plan/Location field. The App Service plan window appears on the right. He clicks the Create new button there. [Video description ends]

Going to call this testwebapp172_ or dash rather, serviceplan.

[Video description begins] A New App Service Plan window appears on the far-right side. He types testwebapp172-serviceplan in the App Service plan field. [Video description ends]

The next thing I'm going to do is specify the location, which in this case will be Canada East. And notice that in this service plan for the app, this is where I can specify the pricing tier or the sizing for the underlying virtual machine or machines that will support my app.

[Video description begins] He clicks the arrow in the Pricing tier field. A new window with Recommended pricing tiers displays. There are 3 tabs in the window, namely Dev/Test, Production, and Isolated. He clicks the first tab. [Video description ends]

So I'm going to go ahead and choose the Dev / Test category. And I'm going to choose this F1 pricing tier, where I've got 1 gigabyte of memory and I'm going to go ahead and click Apply, and then I'll click OK.

[Video description begins] 3 Recommended pricing tiers display. He selects the first one named F1, and clicks the Apply button. F1 Free displays in the Pricing tier field now. testwebapp172-serviceplan displays in the App Service plan field now. [Video description ends]

So we can now see that that app service plan is now tied to this web application. And then I'm going to go ahead and click Create to build the web app. After a moment, over on the left, if I go to App Services, I will see that my application now exists and that its status is listed as Running.

[Video description begins] He clicks App Services in the main navigation pane and all the available app services are listed on the right. webapptest172 displays there. [Video description ends]

Otherwise I can keep clicking Refresh up at the top until I see it. Of course, I can always look up my alerts until I see that the deployment succeeded for this specific resource.

[Video description begins] He clicks the bell icon located next to the Search field in the Azure window. All the notifications display. Deployment succeeded displays there. [Video description ends]

So I'm going to go ahead and click on my web app, and in the overview panel, what I'm interested in to start with is the URL.

[Video description begins] He closes the notifications window and clicks webapptest172. Its properties window displays. [Video description ends]

So notice the URL here is comprised of the name of the resource with .azurewebsites.net which we saw while we were creating it. I'm going to go ahead and copy that.

[Video description begins] The URL is https: //webapptest172.azurewebsites.net. A copy button is present next to it. When he clicks the button, the text Copied displays as a tip. [Video description ends]

And we're going to open up that website in a new web browser window. And this is what I can expect to see by default, an Azure page that says, Your App Service app is up and running.

[Video description begins] He pastes the copied URL in a new browser window and a Microsoft Azure page displays. [Video description ends]

And if I go back to the Azure portal, indeed, we can see that is the URL, and indeed everything is working. Now, if I scroll down, we also have a lot of configuration settings for our web applications. So if I go to Application settings, we can determine the language support in the back end. For example, the version of PHP, which can matter, depending on what kind of functionality you're writing into your PHP scripts.

[Video description begins] He clicks Application settings in the Settings group in the navigation pane. Its window displays on the right. v4.7 displays in the .NET Framework version field. He clicks the drop-down arrow in the PHP version field. 5 options display, namely Off, 5.6, 7.0, 7.1, and 7.2. [Video description ends]

Also, we can see the .NET Framework version, Python and Java, if we're going to be using those. So we have a number of options here that we can configure for our specific application. As I go further down on the application settings page, I can also see the default documents. So these are the documents that will be pulled up when the URL is connected to.

Such as Default.htm or, what we're going to be interested in in a moment, index.php. As I scroll further and further down, we can also specify custom domain names. Now, in this particular case, we'd have to upgrade the app service plan type in order to do that, but we can do it.

[Video description begins] He clicks the Custom domains menu in the navigation pane and a page opens on the right. It is titled Custom Hostnames and it is used to configure and manage custom hostnames assigned to your app. There is a button with a caption- Click here to upgrade your App service plan to assign custom hostnames to your app. [Video description ends]

So what I'm going to do then is I'm going to take an existing on-premises PHP file I've created. And we're going to upload it here to our web app and make sure that it takes effect. So here's my sample PHP file on-premises. It really just comes back and says, sample PHP page hosted in Azure. And sets the title in the browser to My Azure Web App.

[Video description begins] He opens a document in MS-Word. It has HTML code for the main page of his web app. Between the title tags is the text - My Azure Web App. [Video description ends]

What I want to do is save this file locally on-premises and then upload it to the site, and I'm going to do that through FTP. So back here in the portal, I'm still looking at the properties blade for my web application. I'm going to scroll back up and choose Overview. And then from here, I can see I've got the FTP hostname, which is important, but what I want to do is get the publishing profile.

[Video description begins] He goes back to the web app window and clicks Overview in the navigation page. He clicks the Get publish profile button denoted by a download symbol. [Video description ends]

This is going to provide me with the credentials, not just the FTP username, but also the FTP password. So I'm going to go ahead and download the publishing profile. Once that's been downloaded, I'm going to go ahead and click to open it up.

[Video description begins] The downloaded publish profile displays in the start bar of Windows and he clicks it. A Notepad file displays. [Video description ends]

What I'm interested in primarily is the username here listed and also the user password for FTP.

[Video description begins] He highlights userName and userPWD in the file. [Video description ends]

I've downloaded and installed the free FileZilla FTP GUI tool. So the first thing you need here is the host.

[Video description begins] He opens FileZilla. It has a menu bar and a tool bar at the top. Next, are 4 fields, namely Host, Username, Password, and Port. Then there is a Quickconnect button. All the files and folders in the network display below in a hierarchial format. [Video description ends]

So back here in the portal, I'm on the Overview part of the Properties blade. So I'm going to go ahead and copy the FTP hostname and paste that into FileZilla.

[Video description begins] He goes back to the Azure window. He clicks the copy button next to FTP hostname and pastes it in the Host field in FileZilla. [Video description ends]

Next, I'm going to go to the publishing profile I downloaded and copy the FTP username and password and then paste them into the appropriate fields here in FileZilla. And once I've done that, I'll then click, Quickconnect. And after a moment, we can see on the right, under the Remote site section, we have access to our web application, but in the underlying file system through FTP. So if I go to wwwroot, the root of the website, we've got the default Azure placed hostingstart.html file.

[Video description begins] Below the fields and Quickconnect button, the screen is divided into left and right sides. The left side shows files and folders in the Local site, and the right side shows the Remote side. In the Remote side section, he clicks site located in the root. [Video description ends]

[Video description begins] 3 folders display under site. One of them is wwwroot. When he clicks it, hostingstart.html file displays below, on the right side. [Video description ends]

But what I want to put there is my index.php. So that is available on my local machine under samplefiles, which I'll choose on the left.

[Video description begins] He goes to the Local site section in the left, and clicks samplefiles folder there. [Video description ends]

There's index.php, so I'm just going to drag that over and drop it so that it shows up into the wwwroot folder for my site.

[Video description begins] index.php file appears on the left side below. [Video description ends]

And here in a browser, I've still got the URL for my site, but if I refresh, I'm now going to see it's picking up my Sample PHP page that I've uploaded.

[Video description begins] He closes FileZilla and goes to the new Azure window there. A line of text displays there. It says: Sample PHP page hosted in Azure. [Video description ends]

So you can use other tools to manage the content and the code for your website, such as Visual Studio. You might run that on-premises, so that you can work with your code and then push it up into the cloud into your Azure web application.

Azure Functions

[Video description begins] Topic title: Azure Functions. The host for this session is Dan Lachance. [Video description ends]

Azure Functions fall under the classification of serverless computing. These allow developers in the cloud to create and run code. And so the fees that are charged are based only on when that code is actually running. With Azure Functions, developers don't have to worry about deploying virtual machines and virtual networks, and taking care of all of that underlying infrastructure. Because this is a managed service, developers can focus more on the code and the applications that they need to think about and work on, instead of the supporting infrastructure.

So this is really for software developers. And as a software developer, you would be interested in knowing whether or not you can use the language of your choosing, that you're familiar with, with Azure Functions. There are a wide variety of languages that can be used, including PHP, Java, Node.js, C#, and F#. Now, what happens is we have to think about what will trigger the functions to execute.

[Video description begins] Screen title: Azure Function Usage. [Video description ends]

There are plenty of different types of triggers that can be defined for an Azure Function, such as a CosmosDBTrigger. This means that when data is either modified or placed into a Cosmos DB database, then the code can be triggered at that point in time. If users upload content to a storage account container, then the BlobTrigger would kick in, and we could have it execute an Azure Function of our choosing.

You might do that to further organize uploaded files, or to do something specific to them. Such as maybe adding a watermark to a video image that's uploaded or to a graphic file image. The HTTPTrigger, as the name implies, gets triggered based on a specific type of HTTP request. And so when that is triggered, we can define which Azure Function runs. There's also a QueueTrigger, which is based on messages coming into an Azure storage queue that developers could query. And then when that occurs, an Azure Function could run.

Azure Functions, because we're talking about development here, can be defined in a number of different places, such as through the Azure portal. Or you might use a tool that you're familiar with, such as Visual Studio. You might use the Azure CLI to work with functions. And of course, you might use specific language tools related to languages such as Java and Python.

[Video description begins] An infographic displays. It has a cloud with a hammer and screw driver in the center. 4 branches emerge from the symbol. Each ends in a caption.- Azure Portal, Visual Studio, Java and Python, and Azure CLI, respectively. [Video description ends]


Azure App Event Grid
  - The purpose of Event Grid is it lets developers configure certain triggers by events such as making a change to an Azure resource. That can then connect to some kind of endpoint or hook into some other aspect of an application for notifications. And in our particular case, we're simply going to set it up such that when a change is written to an Azure virtual machine, we want to be notified via email.
  - The first thing we'll do here, to get started is we will create a Logic App. Here in the Azure portal, I'm going to click Create a resource over on the left, and I'm going to type in the word logic and then from the results, I'll choose Logic App.
[Video description begins] He clicks Create a resource in the navigation pane, and types logic in the Search field. He selects Logic App from the drop-down results. A Logic App window displays. It has a brief explanation about logic apps and a Create button. [Video description ends]

Then I'm going to go ahead and click on Create so we can get to the point where we start configuring this resource. So after specifying a name, here I'm going to call it vmchange_logicapp, and in this case, I'm going to put it into an existing resource group and I've specified the location. After having done that I'm simply going to click on Create.

[Video description begins] The Logic App window displays. He types vmchange_ logicapp in the Name field. He leaves the default value of Pay-As-You-Go in the Subscription field. He leaves the default option- Use existing- in the Resource group field, and selects Rg1 in the field below. He selects Canada east in the Location field, and leaves the default value Off in the Log Analytics field. [Video description ends]

And after a moment, if I go to my All Resources view and if I type in logic, then we could see our Logic App listed.

[Video description begins] He clicks All resources in the navigation pane and types logic in the Name field on the right. vmchange_ logicappdisplays in the list below. He clicks it and its properties page displays. [Video description ends]

If I click on it, it starts the Logic App's designer where we can get some help on how to work with this and we can also choose from some basic templates.

[Video description begins] The Logic Apps Designer window opens. Basic templates are listed there. [Video description ends]

So like when a message is received in a queue, or when an http request is received, and so on. And if I scroll down a little bit beyond all that, I can choose to build a blank Logic App which I'm going to do here.

[Video description begins] He scrolls down and clicks the tile with a large plus symbol and caption Blank Logic App. A new page opens It has buttons, Save, Discard, Run, Designer, Code View, Templates, Connections, and Help. A Search field is also present to search connections and triggers. Below are 6 tabs- For You, All, Built-ins, Connections, Enterprise, and Custom. The For You tab is active now and there are no recent connections to show. [Video description ends]

And I'm going to search for event grid.

[Video description begins] He types event grid in the Search field and clicks the All tab. Many event grids with their logos display below. [Video description ends]

So I'm going to choose Azure Event Grid. And then when a resource event occurs for Azure Event Grid.

[Video description begins] He clicks the Azure Event Grid and a new tile appears. It has a field, Tenant, with the text Default Directory written in it. There is a Sign in button below the field. [Video description ends]

And then I'm going to click Sign in to authenticate to my Azure subscription And then I'm asked to confirm that I want to allow the provisioning to provide access to Azure Event Grid.

[Video description begins] A confirmation dialog box appears. He clicks Allow access. [Video description ends]

And I'm going to choose Allow Access. And then I have to fill in this detail related to the subscription and also the resource type that I'm interested in in this particular case. So I'm going to choose Microsoft.Resources.ResourceGroups for the resource type and I'm going to set the resource name to a resource group that I already have Rg1 that I know virtual machines have been deployed into. And then I'm going to go ahead and save this Logic App.

[Video description begins] A new tile appears titled When a resource event occurs. He selects Pay-As-You-Go in the Subscription field. [Video description ends]

[Video description begins] He clicks the Save button in the Logic Apps Designer window. [Video description ends]

Now, I'm going to go back into the Logic app designer here in my Logic app.

[Video description begins] In the vmchange_ logicapp window, he clicks Logic app designer in the Development Tools group in the navigation pane. [Video description ends]

And underneath, when a resource event occurs, I'm going to click New step, and then I'm going to choose Built-in and I need some kind of conditional way to determine what's going to happen next. But to do that I'm going to scroll down and choose Condition. That's a control.

[Video description begins] A Choose an action window opens. He selects the Built-ins tab. [Video description ends]

[Video description begins] He scrolls down to the section with 2 tabs- Triggers and Actions. All the actions are listed in the Action tab and he selects Condition. [Video description ends]

And I'm going to specify data.operationName is equal to and what we're looking for is any right activity for our Azure virtual machines related to the resource group.

[Video description begins] A tile titled Condition opens. It has boxes with text and drop-down arrows. At the top is a box with text And. It branches down into 2- one a check box with 3 boxes captioned data.operationName, is equal to, and Microsoft Compute. The second branch has text +Add and a drop-down arrow. [Video description ends]

So that's my condition. Now, down below, for true, and also, of course, if that turns out to be true versus false, we can add an action. So if it's true, if a change has been made to write to a virtual machine, I'm going to click Add an action.

[Video description begins] Below the tile, are 2 more tiles titled if true and if false. He clicks Add an action in the if true tile, and a Choose an action box window appears again. [Video description ends]

And from here I can choose what it is exactly that I want to do. So for example I could search for email if I want to send off an email notification. And lets say I choose Gmail.

[Video description begins] He types email in the search field and clicks Gmail icon in the search results. A new window with actions related to Gmail pops up. [Video description ends]

And then what do you want to do in Gmail? Well what I want to do is send an email message. And from here what I would have to specify are the details related to working with Gmail. If it's the first time I've ever done this, I'm going to have to sign into my Gmail account. Because I've already done that I can specify who I want to send the message to. So a list of email addresses, and I can also add other additional items such as the subject and body here of the message.

[Video description begins] He clicks the action Send email and a new window pops up. It is titled Send email and has fields such as To, Body, and Subject. He checks the elements he wants in his email. [Video description ends]

So maybe in the body, what I want to do is add, let's say, the specific type of event and if I scroll over maybe for the subject, I want to list the subject of the event. And again up here, I can specify some email addresses. So I'm going to go ahead and do that. Email addresses should receive this notification and then I'll click the save button.

[Video description begins] He clicks the Body field and a panel pops up in the right side. It has 2 tabs- Dynamic content and Expression. There is a Search field on the top and various events listed below. Some of them are Event Time, Event Type, and ID. He clicks all the events he wants. [Video description ends]

[Video description begins] He types an email address in the To field. [Video description ends]

After that's done, if I were to go for example to a virtual machine and open up its properties blade and make a change to it such as let's say go to sizing and just choose a different size and click on resize.

[Video description begins] He goes back to the Virtual machines page and clicks Size in the Settings group in the navigation pane. [Video description ends]

I'm writing to that virtual machine, and so our Logic app should capture that event.

[Video description begins] He clicks 81s in the VM SIZE column. A window with the text- Resizing virtual machine- pops up at the top of the machine. [Video description ends]

And so if I go back, let's say to all resources, the view filter for logic, and there's our Logic app. Under the overview part of that if I scroll down I can see for example the status is succeeded down under the run history for our current configuration.


SQL Overview
  - Structured Query Language, or SQL, is the language that's used to access relational database objects and their data. So with a SQL-compliant type of database system, data tables are related via common fields. This is called normalization, and it's a technique that's used to reduce the amount of repeated data. We'll see that in a moment when we see an example of how we might link tables together.

SQL objects include the server that hosts one or more databases. Within a database, we can have one or more tables that actually store the data. Within the table, we have it broken down by fields or columns. So, for example, we might have a table called Customers, and one of the fields might be Customer ID. When we actually populate a table with data, we end up with a record otherwise called a row.

In our example with the Customers table, the record or row would include everything about a customer. So a customer ID, the first and last name, mailing information, and so on. Databases can also contain stored procedures, essentially which are scripts that can be run against items within the database.

We can also create views within a SQL-compliant database that can bring together columns from different tables so we can view it all in one place. And, of course, we can construct a multitude of different types of queries, essentially asking questions about the data stored in tables, such as show me all customers that have spent more than $10,000 in the last quarter.

[Video description begins] Screen title: SQL Table Relationship. [Video description ends]

Pictured on the screen, we've got two tables, and this is used to demonstrate the relationship between tables.

[Video description begins] Two tables are displayed on the screen. The first table is titled Customers table. It has the following three columns: CustID, Lname, and Fname. It contains two rows of data for customer IDs 001 and 002, respectively. The second table is titled Purchases table. It has the following three columns: CustID, Date, and Amount. It has two rows of data for customer ID 001. [Video description ends]

At the top, we've got a Customers table, where we have three columns: Customer ID, Lname for last name, and Fname for first name. Then, we have a second table called Purchases table. It's got three columns, but notice what it has in common with the Customers table, the Customer ID. So Customer ID 001 exists only once in the Customers table.

However, that customer, hopefully, will have more than one purchase with us, and so we can expect to have repeating entries for that Customer ID in the Purchases table. But we don't want to repeat the last name and the first name and everything else about a customer. All we need is one unique item, in this case, the Customer ID to be repeated to link all the purchases in the Purchases table to the customer in the Customers table.

Common SQL-compliant database products would include Microsoft SQL Server, MySQL, Oracle Database, and Sybase to name just a few. In other demonstrations, we're going to take a look at how we would deploy Microsoft Azure SQL Database in the cloud, which is a managed service where we don't have to worry about the underlying virtual machine deployment and the SQL software; it's already built in.


Azure SQL Database
  - One characteristic of cloud computing is rapid provisioning of resources and rapid elasticity, and that's definitely true with Microsoft Azure SQL Database. With SQL Database, we're talking about database as a service, called DBaaS, which is also platform as a service in the cloud.

[Video description begins] Platform as a Service is abbreviated to PaaS. [Video description ends]

It's a managed service, and that's where the rapid elasticity comes in because we don't have to worry about deploying virtual machines and then installing the Microsoft SQL Server software. All of that is taken care of for us when we deploy Azure SQL Database. So, it's very quick to get up and running. We can also bring our own license. BYOL, bring your own license, will let you reuse any existing SQL Server additional licenses that you want to reuse in the cloud when you deploy Azure SQL Database.

You also have the option of migrating your on-premises SQL data into Azure, so you can reuse your data instead of having to recreate it in the cloud. Azure SQL Database supports scaling in the form of horizontal scaling. This means scaling out, by adding database replicas and also database sharding, which allows us to have more information split up horizontally. What this really means is taking large datasets and breaking them into smaller pieces to improve performance, that's where sharding comes from.

But also, Azure SQL Database allows us to scale vertically, increasing or decreasing the underlying horsepower for each SQL Server instance. And we can do that by configuring virtual machine instance sizing. We can manage Azure SQL Database using the GUI, through the Azure portal, or through the Azure CLI. For example, we could use the az sql db create command to create a database. The equivalent in PowerShell would be New-AzSqlDatabase.

And if you're using ARM templates to deploy and manage Azure resources, you would be looking at a resource type of Microsoft.Sql/Servers. With Azure SQL Database, there is a specific database server firewall enabled, and that's on by default for SQL. So if you want to make a connection into Azure SQL Database from externally, you are going to have make sure you create a firewall rule to allow inbound port 1433 traffic.

Port 1433 is what's used by SQL Server by default. There is even an option where you can add your client IP. So when you are using the portal and you are viewing the firewall information for this, for SQL Database, then what you'll notice is that it'll show your current client IP address that you are connected from, your public IP address if you are behind a NAT router.

And so you can conveniently simply add that as the IP you want to allow through the firewall. And then I would allow you, for example, to use tools like SQL Server Management Studio on-premises to reach into the Azure cloud to your SQL Database instance to manage it.

[Video description begins] Screen title: Azure SQL Database Details. [Video description ends]

When you deploy Azure SQL Database, whether it's through the GUI or through command line tools, there are a number of things you have to take into account, one of which is whether you want to deploy a new SQL Server instance or use an existing one that you might have already deployed. SQL Server instances have the DNS domain suffix of .database.windows.net by default.

You need to specify a unique server and a database name when you are deploying Azure SQL. You have to determine which location, or Azure region, into which you want to deploy Azure SQL Database. From a performance standpoint, you also have an option of selecting database transaction units, or DTUs.

This is a culmination of factors that control the performance when it comes to reading and writing to and from databases. It includes things like the amount of virtual CPU, the amount of memory, and how fast disk I/O is.

The last consideration is that if you're deploying Azure SQL Database for testing purposes or any other temporary reason, make sure that you remove any unused databases and server instances as soon as you are finished. You don't want to leave these things running in the background because you are charged for all of the time that it's left running. So make sure to delete it when you are finished with it.


Azure SQL Database GUI Deployment
  - Azure SQL database is a managed service, and in the cloud, what this really means is that we have an easy way to quickly provision something, in this case, an Azure SQL database, without worrying about the underlying complexities, like virtual machines, and the operating systems, and installing the database software, in this case, Microsoft SQL Server.

[Video description begins] A Microsoft Azure webpage is displayed on the screen. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present with the following options: Create a resource, Home, Dashboard, All services, and Favorites. The main body of the page has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

So here in the portal, let's see how this works by starting with clicking on Create a resource in the upper left.

[Video description begins] He clicks on Create a resource from the navigation pane. A screen titled New appears. It has a search bar and a column titled Azure Marketplace. Some of the items here are: Get started, Recently created, Compute, Networking, Databases, etc. A column titled Featured is also present on this screen. Some of the items here are: Azure SQL Managed Instance, SQL Database, SQL Data Warehouse, etc. [Video description ends]

And in the Databases category, on the right, I will then choose SQL database.

[Video description begins] From the Azure Marketplace column, he selects Databases. From the Featured column, he selects SQL Database. A page titled Create SQL Database appears. It has four tabs: Basics, Additional settings, Tags, and Review +Create. The Basics tab is currently open. It has drop-down menus for Subscription and Resource Group. The default value for Subscription is Pay-As-You-Go. The default value for Resource Group is cloud-shell-storage-eastus. [Video description ends]

Now the first thing I have to do is tie this to a resource group. I've already got one, so I'm going to put it in that resource group, and I have to give this a Database name.

[Video description begins] He clicks the drop-down menu for Resource Group. A list of options appears. He selects Rg1 from the list. He then scrolls down the page. A section titled Database Details appears. It has an input box for Database name and a drop-down menu for Server. There is a question that reads: Want to use SQL elastic pool? It has two options with radio buttons: Yes and No. No is currently selected. For Compute+storage, there is a link for Configure database. At the bottom, there are two buttons: Review+Create and Next: Additional settings. [Video description ends]

So I'm going to start by calling this sqldb172. Now, this name needs to be checked for uniqueness. And once it is okay, and it's been typed in correctly, and there are no invalid characters, we'll get this little green check mark over on the right.

[Video description begins] In the input box for Database name, he types sqldb172. [Video description ends]

Now, down below, I can either tie this to an existing SQL server instance hosted in Azure or I can click Create new, which I'm going to do here.

[Video description begins] There is a link titled Create new below the drop-down menu for Server. He clicks the link. A screen titled New server slides onto the screen. It has input boxes for Server name, Server admin login, Password, and Confirm password. A drop-down menu for Location is also present. The default location is Canada East. Below it, there is a check box for Allow Azure services to access server, which is currently checked. At the bottom there is a Select button. [Video description ends]

And I'm going to call this sqlsvr, for SQL Server, 172. Again, it has to wait for the name to be checked, and it looks good. Then I'm going to specify some server admin login info, so user name, and I will confirm the password.

[Video description begins] In the input box for Server name, he types sqlsvr172. [Video description ends]

[Video description begins] In the input box for Server admin login, he types cirving. He types the password and confirms the password. [Video description ends]

And after that, I'm going to go ahead and click on Select.

[Video description begins] He clicks the Select button. The New Server screen disappears. The value for Server is (new)sqlsvr172 (Canada East). [Video description ends]

So we're creating a new SQL server instance, which is good. I'm just going to scroll down here, I'm going to leave it on the default Compute and storage configuration.

[Video description begins] The value for Compute+storage is Standard S0 10DTUs, 250 GB. [Video description ends]

So 10 DTUs, that's database transaction units, and 250 gigabytes. Although we could click on Configure database and depending on our workload, we could choose the amount of data that we want to be able to accommodate and the database transaction units using these two sliders. So it's always dependent upon your specific workload.

[Video description begins] He clicks the Configure database link. A page titled Configure appears. It has three options: Basic, Standard, and Premium. Basic is currently selected. It has two sliders for DTUs and Data max size. The current DTU is 0 and Data max size is 250 GB. At the bottom, there is an Apply button. He clicks it and the screen shifts to the previous page. [Video description ends]

In this case, I'm going to click Next: Additional settings.

[Video description begins] He clicks the Next: Additional settings button. The screen shifts to the next tab: Additional settings. It has three options for Use existing data: None, Backup, and Sample. None is currently selected. For Collation, there is an input box with the following default value: SQL_Latin1_General_CP1_ CI_AS. At the bottom, there are three buttons: Review + Create, Previous, and Next: Tags. [Video description ends]

Then, I can determine whether I want to have a blank database or use some kind of existing data from a sample data set or a backup. I'm going to leave it blank by leaving it on None. Then I'll click Next.

[Video description begins] He clicks the Next: Tags button. The screen shifts to the next tab, Tags. It has drop-down menus for Name, Value, and Resource. The Resource is Database. At the bottom, there are three buttons: Review + Create, Previous, and Next: Review + Create. [Video description ends]

I can tag this, perhaps to tie it to a project or department, but I'm going to leave that as it is. And finally, on the Review page, I can see the estimated cost per month.

[Video description begins] He click the Next: Review + Create button. The screen shifts to the next tab, Review + Create . It contains Product Details, Terms, Basics, etc. At the bottom, there are two buttons: Create and Previous. [Video description ends]

So we wana be careful with this. This is one of those types of resources in Azure you wanna make sure you shut down or even delete the moment you no longer need it, so you don't incur charges.

[Video description begins] He highlights the estimated cost per month, which is 16.50 USD, listed under Product Details. [Video description ends]

So I'm going to go ahead and click Create.

[Video description begins] He clicks the Create button. An overview page appears. It reads: Your deployment is complete. It has a button labelled Go to resource. It has a navigation pane with the following options: Overview, Inputs, Outputs, and Template. The Overview page is currently open. There is a table on this page. It has following columns: Resource, Type, Status, and, Operation Details. The table has three rows of data. [Video description ends]

And after a moment, we have a message that says our deployment is complete. See it's created a firewall rule here to allow inbound port 1433 traffic to SQL. Then we've got the database and the server.

[Video description begins] The presenter points to the data in the table. He then closes the Overview page and shifts to the home screen of Microsoft Azure. [Video description ends]

Of course, we're going to see all those items here if we go to the All resources view and start to filter the view.

[Video description begins] From the navigation pane, he clicks on All resources present under Favorites. A page titled All resources appears. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. A few rows of data are present in this table. [Video description ends]

So if I just type in sql, here we can see our SQL server deployed in the cloud.

[Video description begins] He types sql in the input box for Name. Two rows of data can now be seen in the table. [Video description ends]

But we didn't have to worry about all the complexities that would normally be involved with that if we had deployed a SQL server on premises ourselves. So let's click on the SQL server here in Azure to pop up its Properties blade.

[Video description begins] He clicks on sqlsvr172 present in the table. A screen titled sqlsvr172 appears. It has a navigation pane. The main body contains details about the server. At the bottom, there are two tabs: Notifications and Features. The Features tab is currently selected. It has four tabs: All, Security, Performance, and Recovery. [Video description ends]

And from within here, for example, if I go to the SQL databases view, I can see it is hosting one of our SQL databases, sqldb172, which currently has a status of Online.

[Video description begins] From the navigation pane, he selects SQL databases. A page titled sqlsvr172-SQL databases appears. It has a table with the following three columns: Database, Status, and Pricing Tier. It has one row of data. The Database is sqldb172, Status is Online, and Pricing Tier is Standard S0: 10DTUs. [Video description ends]

And I'll just close out of this.

[Video description begins] He closes the current screen. [Video description ends]

If we go into our SQL database instance, so I'll click on it to pull up its Properties blade, then we have options we can configure, such as Geo-Replication, if we want it replicated to other locations in different Azure regions.

[Video description begins] On the All resources page, from the list of resources present in the table, he clicks sqldb172(sqlsvr172/sqldb172). A page titled sqldb172(sqlsvr172/sqldb172) opens. It has a navigation pane and the following buttons: Copy, Restore, Export, Set server firewall, etc. A list of information about the server is also displayed here. [Video description ends]

[Video description begins] He selects Geo-Replication under Settings from the navigation pane. A world map is displayed on the screen. [Video description ends]

And as we go further down, we'll see that we have a number of options related to things like security. So we've got some Advanced Data Security options available for the database, so vulnerability assessments, also, Dynamic Data Masking to mask some kind of important information, such as the entering of credit card numbers, and so on. And this is configured by configuring what are called masking rules.

[Video description begins] In the navigation pane, there is a section titled Security. It has the following options: Advanced Data Security, Auditing, Dynamic Data Masking, and Transparent data encryption. He clicks on Advanced Data Security. A page for Advanced Data Security opens up. It has a button labelled Enable Advanced Data Security on the server. [Video description ends]

[Video description begins] He clicks on Dynamic Data Masking, and the page for Dynamic Data Masking opens. It has a section titled Masking rules, where there is a table with two columns: Mask Name and Mask Function. The table is currently empty. Below it, there is a section titled Recommended fields to mask, where there is a table with three columns: Schema, Table, and Column. The table is currently empty. [Video description ends]

And if I go back to the Overview here for our database, I can see the SQL server name. This is what you need if you want to make a connection to this, for example, from on-premises. Now, at the same time, even from here within the database Properties blade, I could click the Set server firewall button, and, I can see my current Client IP address as seen on the Internet.

[Video description begins] He shifts to the page titled sqldb172(sqlsvr172/sqldb172). From the list of information, he points to the Server name, which is sqlsvr172.database.windows.net. [Video description ends]

[Video description begins] He clicks the Set server firewall button. A page titled Firewall settings appears. It has three buttons: Save, Discard, and Add client IP. For Allow access to Azure service, there are two buttons: On and OFF. On is currently selected. Client IP address is 71.7.176.108. There is a table with input boxes for Rule Name, Start IP, and End IP. [Video description ends]

[Video description begins] He highlights the Client IP address. [Video description ends]

And so from here, we have the option of making sure that we add a rule that could, for example, contain that IP address or other address ranges that we want to allow access from. Notice we've got an Add client IP button at the top that fills all that stuff in for me to allow connectivity.

So I'm going to go ahead and save that firewall rule because the machine I'm sitting at right now that does have this public IP address I do want to allow in through the firewall for SQL purposes.

[Video description begins] He clicks the Add client IP button. A new row gets added in the table. The Rule Name is ClientIPAddress_2019-3-13. Start IP is 71.7.176.108 and End IP is 71.7.176.108. He clicks the Save button. [Video description ends]

[Video description begins] A pop-up message appears. It reads: Successfully updated server firewall rules. The presenter clicks the OK button. The pop-up message closes. He then closes the Firewall settings page. [Video description ends]


Azure SQL Database CLI Deployment
The first thing I'll do is run az sql --help. This is a great way to get a sense of what the syntax will be as you begin working with this.

[Video description begins] In the command prompt window, he types az sql -- help. The group name and its function appear. This is followed by a list of subgroups and their definitions and a list of commands and their definitions. [Video description ends]

So notice, I can work with Azure SQL databases as well as servers, and so on.

[Video description begins] From the list of subgroups, he highlights db and server. The definition of db is Manage databases. The definition of server is Manage SQL servers. [Video description ends]

So I'm going to clear this screen, and I'm going to start by running az sql server create --name. Let's say, I call it sqlsvr456. And then, I'm going to put that in a --resource-group that I've already got called rg1. And --location. I'll put canadaeast, and then --admin-user for the SQL server, specify a value for that.

And then --admin-password. Now make sure you have the two dashes in front of these parameters, otherwise you get some kind of an error related to that. And I'm gonna go ahead and specify the password here that I want to use for that deployment. I'm going to go ahead and press Enter to build our Azure SQL Server instance.

[Video description begins] He types cls and presses Enter. The screen is now clear. He then types az sql server create -- name sqlsvr456 -- resource-group rg1 -- location canadaeast -- admin - user cirving -- admin - password Pa$$w0rdABC123. He presses Enter. A result is being generated. [Video description ends]

Now that that's successfully completed, I'm going to go ahead and build a SQL database tied to that server.

[Video description begins] A list of information is generated in the result. It includes details of administrator login, administrator login password, fully qualified domain name, id, identity, kind, location, name, resource group, state, tags, type, and version. [Video description ends]

So az sql db create, so I've got to deploy this into a resource group. So --resource-group rg1 in this case and --server. I've got to tie it to a server, so in this case, sqlsvr456. And I want to call this database --name, let's say, db456.

[Video description begins] He types cls and presses Enter. The screen is now clear. He then types az sql db create --resource-group rg1 -- server sqlsvr456 -- name db456. He presses Enter. A list of information is generated in the result. Some of these are: longTermRetentionBackupResourceID, maxLogSizeBytes, recoverableDatabaseID, sampleName, etc. [Video description ends]

Now that, that's done, the next consideration is that if you're gotta be using any kind of code or tools on-premises to reach into Azure to work with that SQL database, then you're going to need to have a firewall exception created in Azure. So let's go ahead and deal with that under that assumption. So I'm going to clear the screen with CLS, and I'm going to run az sql server firewall-rule create.

We're creating a firewall rule for SQL server, --resource-group rg1, and the server, --server sqlsvr456, and -n. What do you want to call this rule? How about AllowInboundSQL? And then --start-ip-address. Now because I'm behind a NAT firewall, I need to specify the public IP address as known out on the Internet for my connection here for this to work properly. So I'm going to go ahead and specify that address.

And after I've done that, I can then begin to specify the ending IP address. In this case, it's going to be the same thing because it's one IP. So once I've verified that that is correct, and it looks correct, I'm going to use --end-ip-address. And in this case, I'm going to use the exact same thing because it's only one IP I want to allow in, and that's my public IP address. So I'm going to make sure I don't have any typos here. And I'm going to press Enter to create that, and it looks like it's created. Let's go ahead and check our work in the Azure portal.

[Video description begins] He types cls and presses Enter. The screen is now clear. He then types az sql server firewall -rule create --resource-group rg1 -- server sqlsvr456 - n AllowInboundSQL --start -ip-address 71.7.176.108 --end -ip-address 71.7.176.108. He presses Enter. The following details are generated in the result: endIpAddress, id, kind, location, name, resourceGroup, startIpAddress, and type. [Video description ends]

[Video description begins] A Microsoft Azure webpage opens. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present on this screen. The options here are: Create a resource, Home, Dashboard, All services, and Favorites. The main body of the page has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

Here in the portal in the left-hand navigator, I'm going to click SQL databases. Here's db456. We can see it's tied to our server.

[Video description begins] He clicks on SQL databases from the navigation pane. A page titled SQL databases appears. It has the following buttons on the top: Add, Reservations, Edit columns, Refresh, etc. It has a table with the following columns: Name, Status, Replication, Server, Pricing Tier, Location, and Subscription. The table has one row of data. The SQL database name is db456 (sqlsvr456/db456). He clicks on this name. A page titled db456 (sqlsvr456/db456)appears. It has a navigation pane. The main body of the page has details about the following options: Resource Group, Status, Location, Server name, Elastic pool, etc. [Video description ends]

So if I go take a look at it, it look like it's online and ready to go.

[Video description begins] He points to the Status. It reads Online. [Video description ends]

I'm going to go to All resources, and I'll filter the list of Azure resources with things that start with SQL.

[Video description begins] From the navigation pane, he clicks on All resources present under Favorites. A page titled All resources appears. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. A few rows of data are present in this table. [Video description ends]

[Video description begins] He types sql in the input box for name. Two rows of data are now present in the table. [Video description ends]

There's our SQL server, sqlsvr456, and in the Properties blade, I'm going to scroll down under the Security section until I see firewalls and virtual networks, and notice here on the right, sure enough, we've got our AllowInboundSQL rule for our public IP address.

[Video description begins] He clicks on sqlsvr456 in the table. A screen titled sqlsvr456 appears. It has a navigation pane. The main body has details about the server. Some of these are: Status, Location, Subscription, etc. At the bottom, there are two tabs: Notifications and Features. The Features tab is currently selected. It has four tabs: All, Security, Performance, and Recovery. [Video description ends]

[Video description begins] In the navigation pane, there is a section titled Security. It has the following options: Advanced Data Security, Auditing, Firewalls and virtual networks, and Transparent data encryption. He clicks on Firewalls and virtual networks. A page for Firewalls and virtual networks opens up. For Allow access to Azure services, there are two buttons: On and OFF. OFF is currently selected. Client IP address is 71.7.176.108. There is a table with input boxes for Rule Name, Start IP, and End IP. One row of data is present in this table. The Rule Name is AllowInboundSQL, Start IP is 71.7.176.108, End IP is 71.7.176.108. [Video description ends]

Azure SQL Database PowerShell Deployment

[Video description begins] Topic Title: Azure SQL Database PowerShell Deployment. Your host for this session is Dan Lachance. [Video description ends]

In this demonstration, I'm going to use PowerShell to deploy an Azure SQL database. Here in this PowerShell ISE, I've got this in a script. I've commented out the first line, that's what the hashtag or pound symbol means because I've already connected to my Azure account and authenticated it.

[Video description begins] A window titled Windows Powershell ISE opens up. It has a tool bar and a menu bar. A tab titled Create_SQL_Server_DB.ps1 is open. It has 8 code lines. A Console pane is also present on this window. [Video description ends]

[Video description begins] He highlights code line 1. It reads: # Connect-AzAccount. [Video description ends]

So the first thing we're doing here is building a new Azure SQL server using that cmdlet. We are tying it or deploying it into a resource group. I am specifying a SQL server name. We are specifying the Azure location or region, and then we are specifying -SqlAdministratorCredentials. Now I've got the back tick symbol here that's not just a single quote, it's a back tick, and that allows me to continue the expression on the next line here in PowerShell.

[Video description begins] He points to code line 3. It reads: New-AzSqlServer -ResourceGroupName rg1 -ServerName sqlsrv3362 -Location canadaeast -SqlAdministratorCredentials `. [Video description ends]

And what I'm doing is then running new object of type System.Management.Automation.PowerShell or PSCredential. And the argument list is going to have the username, in this case cirving. And then I'm going to convert to a secure string, a password that I'm specifying here, within double quotes.

[Video description begins] He points to code line 4. It reads: $ (New-Object -TypeName System.Management.Automation.PSCredential -ArgumentList "cirving" , $ (ConvertTo-SecureString - String "Pa$$w0rdABC123" -AsPlainText -Force)). [Video description ends]

Then what's happening is we're building our new Azure SQL database and tying it to our SQL server instance that we've created above. And I'm calling this database db112.

[Video description begins] He points to code line 6. It reads: New-AzSqldatabase -ResourceGroupName rg1 -ServerName sqlsrv3362 -DatabaseName db112. [Video description ends]

And then I'm adding a new Azure SQL Server firewall rule to allow incoming SQL traffic from the appropriate IP address, which happens to be my public IP address for my on-premises environment.

[Video description begins] He points to code line 8. It reads: New-AzSqlServer FirewallRule -ResourceGroupName rg1 -ServerName sqlsrv3362 -FirewallRuleName "AllowIncoming SQL" - StartIpAddress 71.7.176.108 -EndIpAddress 71.7.176.108. [Video description ends]

So I'm going to go ahead and run this script by clicking the Run Script button up at the top.

[Video description begins] He clicks the Run Script button present in the tool bar. A list of details is displayed in the Console pane. [Video description ends]

And after a moment or two, we can see it looks like the script has completed running. So let's go check our work in the portal. Let's take a look for sqlsrv3362 and db112.

[Video description begins] He highlights sqlsrv3362 in code line 3, and db112 in code line 6. [Video description ends]

Here in the portal, I've gone to the All resources view and filtered for anything that starts with SQL, S-Q-L, and we can see indeed, we've got sqlsrv3362.

[Video description begins] A Microsoft Azure webpage opens up. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present on this screen. It has the following options: Create a resource, Home, Dashboard, All services, and Favorites. The All resources page is currently open. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. In the name input box, sql is entered. A list of sql resources is displayed in the table. [Video description ends]

And, if I open that Properties blade up, and go down under Firewalls and virtual networks, we can see our Allow incoming SQL firewall rule that was created.

[Video description begins] He clicks the 1st sql resource listed in the table: sqlsrv3362. A page titled sqlsrv3362 appears. It has a navigation pane. The main body has details about the server. Some of these are: Status, Location, Subscription, etc. At the bottom, there are two tabs: Notifications and Features. The Features tab is currently selected. It has four tabs: All, Security, Performance, and Recovery. [Video description ends]

[Video description begins] In the navigation pane, there is a section titled Security. It has the following options: Advanced Data Security, Auditing, Firewalls and virtual networks, and Transparent data encryption. He clicks on Firewalls and virtual networks. A page for Firewalls and virtual networks opens. For Allow access to Azure services, there are two buttons: On and OFF. OFF is currently selected. Client IP address is 71.7.176.108. There is a table with input boxes for Rule Name, Start IP, and End IP. One row of data is present in this table. The Rule Name is AllowIncoming SQL, Start IP is 71.7.176.108, and End IP is 71.7.176.108. The presenter closes this page and shifts to the All resources page. [Video description ends]

And we can also see that we've got DB or database 112 that was created as the result of our PowerShell script.

[Video description begins] He points to the 2nd sql resource listed in the table: db112 (sqlsrv3362/db112). [Video description ends]

Now the only other thing to bear in mind is don't leave these database resources running in Azure if you're not using them. Because you will pay for using them even if they're running, and you're not actually using them. So remove these resources when you no longer need them immediately.

Connect to Azure SQL Database

[Video description begins] Topic Title: Connect to Azure SQL Database. Your host for this session is Dan Lachance. [Video description ends]

Once you've deployed Azure SQL database in the Azure cloud, how do you connect to it? That's going to be the focus of this demonstration.

[Video description begins] A Microsoft Azure webpage opens. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present on this screen. The options here are: Create a resource, Home, Dashboard, All services, and Favorites. The All resources page is currently open. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. In the input box for name, sql is entered. A list of sql resources is displayed in the table. [Video description ends]

Here in the Azure portal, I've already deployed Azure SQL database. So I've got a SQL server instance and a database already up and running.

[Video description begins] He points to the two sql resources listed in the table: sqlsrv3362and db112 (sqlsrv3362/db112). [Video description ends]

So I'm going to go ahead and click on the link to open up the Properties blade for my SQL server instance.

[Video description begins] He clicks on sqlsrv3362. A page titled sqlsrv3362 appears. It has a navigation pane. The main body contains details about the server. Some of these are: Status, Location, Subscription, etc. At the bottom, there are two tabs: Notifications and Features. The Features tab is currently selected, which has four tabs: All, Security, Performance, and Recovery. [Video description ends]

And the first thing I'll do is scroll all the way down under Security and look at Firewalls and virtual networks.

[Video description begins] In the navigation pane, there is a section titled Security. It has the following options: Advanced Data Security, Auditing, Firewalls and virtual networks, and Transparent data encryption. He clicks on Firewalls and virtual networks. A page for Firewalls and virtual networks opens up. For Allow access to Azure services, there are two buttons: ON and OFF. OFF is currently selected. The Client IP address is 71.7.176.108. There is a table with input boxes for Rule Name, Start IP, and End IP. One row of data is present in this table. The Rule Name is AllowIncoming SQL, Start IP is 71.7.176.108, and End IP is 71.7.176.108. [Video description ends]

I've already added a firewall exception to allow my current client IP address, my public IP.

[Video description begins] He highlights the client IP address. [Video description ends]

I've made a firewall rule to allow traffic from this address. This is my on-premises public IP as it's known out on the Internet.

[Video description begins] He highlights the Start IP and End IP in the table. [Video description ends]

The other thing that we can do here, let me just close this out, is open up the Properties blade for our deployed SQL database in the cloud.

[Video description begins] He closes the current page and shifts to the All resources page. [Video description ends]

And from here, we can see the server name that we can simply copy.

[Video description begins] He clicks on db112 (sqlsrv3362/db112). A page titled db112 (sqlsrv3362/db112)appears. It has a navigation pane. The main body has details about the server. Some of these are: Status, Location, Subscription, Server name, etc. He copies the Server name. The Server name is: sqlsrv3362.database.windows.net. [Video description ends]

So what I'm going to do now is go to my on-premises environment and launch the SQL Server Management Studio tool.

[Video description begins] A window titled Microsoft SQL Server Management Studio appears. It has a menu bar and a tool bar. It has a section titled Object Explorer. It has a Connect button with a drop-down arrow. A few more buttons are present here. [Video description ends]

Specifically, this is the Microsoft SQL Server Management Studio tool for SQL Server 2014. What I'm going to do here is click Connect over on the left.

[Video description begins] He clicks the Connect button. A list of options appears. These are: Database Engine, Analysis Services, Integration Services, Reporting Services, and Azure Storage. [Video description ends]

What I want to do is connect to a database engine.

[Video description begins] He selects Database Engine. A window titled Microsoft SQL Server 2014 appears. It has drop-down menus for Server type, Server name, Authentication, and Login. An input box for Password is also present. The default value for Server type is Database Engine. The default value for Server name is sqlsrv3362.database.windows.net. The default value for Authentication is SQL Server Authentication. The default value for Login is cirving. At the bottom, there are four buttons: Connect, Cancel, Help, and Options. [Video description ends]

And I want to make sure that I've got the SQL Server name that I copied from Azure listed here, and I do.

[Video description begins] He highlights the Server name. [Video description ends]

Now when I deployed that SQL Server, I specified an admin name of cirving, so I've got that there. Now, I just have to supply the password that I specified when I deployed that SQL server instance. Once I've done that, I can go ahead and click on the Connect button.

[Video description begins] He enters the password. He clicks the Connect button. In the Object Explorer section, a server named sqlsrv3362.database.windows.net opens. It has two folders: Databases and Security. [Video description ends]

Because the firewall allows connectivity from this host behind a NAT firewall, we can see now that we are indeed connected to our Azure SQL database instance. And if I start going down under Databases to take a peek, here's db112 or database 112, where if we flip back to the portal, we can see indeed that is the name of our Azure deployed SQL database.

[Video description begins] He expands the Databases folder. It has two folders: System Databases and db112. He then expands the db112 folder. It has a list of folders. Some of these are: Tables, Views, Synonyms, etc. He shifts to the Microsoft Azure webpage. The page titled db112 (sqlsrv3362/db112) is currently open. [Video description ends]

[Video description begins] He highlights the title of the page. [Video description ends]

Azure Database Migration Service

[Video description begins] Topic title: Azure Database Migration Service. Your host for this session is Dan Lachance. [Video description ends]

The Azure Database Migration Service allows you to keep your existing data and simply move it into the cloud. So we're talking about moving database workloads from an on-premises IT environment into Microsoft Azure. Now, the movement of that data can happen through a site-to-site VPN tunnel that you would've first defined between your on-premises network and an Azure virtual network or you might do that movement of data through an ExpressRoute circuit.

ExpressRoute does not go through the Internet. Instead, it is a dedicated network circuit from your on-premises network directly into the Azure cloud. And depending on where you are in the world, this may not be an option. But generally, it is conceptually a choice that we at least have to examine when we're going to be moving or migrating data from on-premises into Azure.

The Azure database migration process begins with discovery. What we're talking about discovering are database items available on-premises. The next thing is assessing the workloads that use those databases to determine whether or not they can be migrated into Azure.

[Video description begins] Screen title: Azure Database Migration Process. [Video description ends]

The convert part of the process means that we want to take a look at the database schema on-premises. The schema is the blueprint of what type of data is allowed to be stored, and we want to make sure that, that schema will function correctly in the Azure environment. This is normally not an issue unless, on-premises, you've customized your database schema to support non-standard data types, for example.

Now, that's all considered pre-migration in terms of tasks. The next thing that we have is to actually migrate the schema and data objects. Depending on the amount of data you're talking about and the speed of your network link to the Azure cloud, we'll determine how long it will take to bring across very large data sets.

Next, we've got data synchronization, so the data that's migrated into Azure is kept in sync with on-premises data during this transitory process. Finally, at the cutover stage, we actually cut ties with the on-premises data source. And so there is no longer synchronization between on-premises and Azure cloud hosted data.

[Video description begins] An illustration appears. There are two boxes titled: Pre-migration and Migration. The Pre-migration box has Discover, Assess, and Convert. The Migration box has Migrate schema & data object, Data sync, and Cutover . Discover is connected to Assess which is then connected to Convert. Convert is connected to Migrate schema & data object, which is then connected to Data sync. Data sync is connected to Cutover. [Video description ends]

As we'll see in a different demonstration, we start the process of working with Azure Database Migration by actually creating a database migration instance. And we can do that, for example, through the Azure portal. From there, it will take us through all of these steps in the database migration process.

SQL Server Migration Assessment

[Video description begins] Topic Title: SQL Server Migration Assessment. Your host for this session is Dan Lachance. [Video description ends]

The SQL Server Migration Assessment tool allows you to run this on-premises, so you can assess what you currently have running and which are likely candidates for being migrated into the Azure cloud.

[Video description begins] The Microsoft webpage is open on the screen. The Download Center page is currently open. It has a list of options at the top. Some of these are: Windows, Office, Web browsers, etc. The page has a section titled Microsoft Data Migration Assistant v4.2. A Download button is present in this section. [Video description ends]

So here, I've gone to the Microsoft.com website, and I've searched the downloads for the Migration Assistant. So we can see here, Data Migration Assistant v4.2, at least at the time of this recording. So I have to go ahead and download that tool.

[Video description begins] He clicks the Download button. A File explorer opens up. The Downloads folder is currently open. It has a file named DataMigrationAssistant.msi. [Video description ends]

Once it's downloaded, I'm going to go ahead and run the installer, so it's called the DataMigrationAssistant.msi. So I'm just going to right-click on that and choose Install.

[Video description begins] He right-clicks on the file. A list of options appears. He selects Install from the list. A window titled Microsoft Data Migration Assistant Setup is launched. It has three buttons at the bottom: Back, Next, and Cancel. [Video description ends]

I'll choose Next, I will accept the terms of the license agreement, I'll click Next again, and it'll just go ahead and run the installation.

[Video description begins] He clicks the Next button. The End-User License Agreement appears on the next screen. It has a check box for accepting the terms of the License Agreement. He checks the box. At the bottom there are four buttons: Print, Back, Next, and Cancel. He clicks the Next button. A Privacy Statement appears on the next screen. It has three buttons at the bottom: Back, Install, and Cancel. He clicks the Install button. A progress bar displaying the installation status appears on the next screen. At the bottom, there are three buttons: Back, Next, and Cancel. Once the installation is complete, the screen reads: Completed the Microsoft Data Migration Assistant Setup Wizard. This screen has a check box for Launch Microsoft Data Migration Assistant. At the bottom, there is a Finish button. [Video description ends]

Before you know it, it's done. So, there's a check box here on the installation screen to launch the Microsoft Data Migration Assistant. Of course, I could launch it after through my Start menu, but I'm going to go ahead and turn that check mark on, and I'm going to click Finish.

[Video description begins] He checks the box for Launch Microsoft Data Migration Assistant and clicks the Finish button. [Video description ends]

Now it's loading the Microsoft Data Migration Assistant.

[Video description begins] A window titled Data Migration Assistant opens up. On one side of the window, there is a toolbar. [Video description ends]

So the first thing I'll do here is click the new button, the plus sign, to create a new migration project.

[Video description begins] He clicks the + sign in the toolbar. A window titled New appears. It has two options for Project type: Assessment and Migration. Assessment is selected by default. There is an input box for Project name. Drop-down menus for Source server type and Target server type are also present. The default value for Source server type is SQL Server. The default value for Target server type is Azure SQL Database. At the bottom, there is a Create button. [Video description ends]

Notice, we can build in assessment project, which is what we're going to do, to assess our on-premises SQL databases to test their suitability to be migrated to Azure. However, we can also actually perform a migration. However, we're not going to build a migration project here, just assessment. So the project name here is going to be HfxProj1 because my city, where this is happening on-premises, is Halifax.

[Video description begins] In the input box for Project name, he types HfxProj1. [Video description ends]

And the source server will be SQL, the target will be Azure SQL database. I'll click create, then I'm going to click Next.

[Video description begins] He clicks the Create button. A page titled HfxProj1 appears. It has three steps of creation. The first step called Options is currently displayed. Three options for report type are present here. These are: Check database compatibility, Check feature parity, and Benefit from new features. The first two are selected by default. At the bottom, there is a Next button. [Video description ends]

I'm going to give it some details, such as the name of the on-premises server that I want to use.

[Video description begins] He clicks the Next button. The screen shifts to the next step, which is Select sources. It has two buttons: Add sources and Remove sources. A window titled Connect to a server slides onto the screen. It has drop-down menus for Server name and Authentication type. The default value for Authentication type is Windows Authentication. For Connection properties, there are two options with check boxes. These are: Encrypt connection and Trust server certificate. Both the options are checked by default. At the bottom, there is a Connect button. In the drop-down menu for Server name, he selects srv2016-1. He retains the default value for Authentication type. [Video description ends]

So I'll fill that in, that's my on-premises SQL server. It uses Windows Authentication, but I could choose the appropriate authentication type. If you're not sure what to choose here, talk to your on-prem SQL people and I'm going to click Connect. We're not going to change anything else here.

[Video description begins] He clicks the Connect button. The window titled Connect to a server disappears. A new window titled Add sources appears. It has a check box for srv2016-1. Below it, there are four options with check boxes for sources. These are: CM_S01, ReportServer, ReportServerTempDB, and XTrans. At the bottom, there is an Add button. [Video description ends]

So I know it's made a good connection because I have a list of valid databases that are being hosted on that SQL server instance. So I'm just going to choose one of the databases in question that I'm thinking about migrating to Azure, and I'm going to click Add.

[Video description begins] He selects CM_S01. srv2016-1 gets automatically selected. He clicks the Add button. The window titled Add sources disappears. On the screen for the second step, which is Select sources, CM_S01 is added. At the bottom of this screen, there are two buttons: Back and Start Assessment. [Video description ends]

So we can see it's been added here. And we can add and remove sources at will here to determine what we want our assessment to be run against. So I'm going to go ahead and click Start Assessment in the bottom right.

[Video description begins] He clicks the Start Assessment button. The screen shifts to the third step, which is Review results. On one side of this screen, there are two options with radio buttons. These are SQL Server feature parity and Compatibility issues. SQL Server feature parity is selected by default. Below these options, there is a search box and below this box, srv2016-1 is present. In the main body of the screen, a tab titled Feature parity appears. It has a list of Unsupported features, and a section titled Cross-database references not supported in Azure SQL Database. This section has two sub-parts: Details and Databases. At the bottom, there is an Export report button. [Video description ends]

Once the assessment is complete, we can see that we are in step 3, where we're reviewing the results. And so we can see our SQL server on-premises listed over here on the left. And then we can see our listed items of unsupported features.

So the service broker feature not being supported in the Azure cloud and when we select that unsupported feature, we get some details listed over here on the right.

[Video description begins] From the list of Unsupported features, the presenter selects Service Broker feature is not supported. Its details appear in the Details section of the page. He scrolls down the Details section. A section titled Recommendation appears. [Video description ends]

Then we also get some recommendations about what we might be able to do to resolve this issue once the database has actually been migrated into the Azure Cloud.

[Video description begins] He highlights the following lines from the Recommendation section: Once the database has been migrated to Azure, you can look into Azure Service Bus functionality to implement a generic, cloud-based messaging system instead of Service Broker. [Video description ends]

Now, also remember that once you're happy with this, you can actually add an actual migration project,

[Video description begins] He clicks the + sign in the toolbar. The window titled New appears. He selects Migration for Project type. [Video description ends]

where you can actually start moving data into the cloud. So this migration assistant is a great tool then for you to evaluate your on-premises SQL databases and how likely they are as candidates for running in the Azure cloud.

[Video description begins] He closes the window titled New. [Video description ends]

Azure SQL Geo-Replication

[Video description begins] Topic Title: Azure SQL Geo-Replication. Your host for this session is Dan Lachance. [Video description ends]

To increase fault tolerance and availability for Azure SQL, we have the option of enabling Geo-Replication.

[Video description begins] A Microsoft Azure webpage displays on the screen. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present here with the following options: Create a resource, Home, Dashboard, All services, and Favorites. The main body of the page has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

Here in the portal, I've already got Azure SQL deployed. So if I go to the All resources view, and if I filter that view for anything that has a prefix of SQL, we can see both.

[Video description begins] From the navigation pane, he clicks on All resources present under Favorites. A page titled All resources appears. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. A few rows of data are present in this table. [Video description ends]

We've got a SQL Server and a single SQL database instance hosted on that server.

[Video description begins] He types sql in the input box for name. Two rows of data can be seen in the table. [Video description ends]

If I click on the server to open up its Properties blade, you'll notice that we don't have any options related to Geo-Replication, at least not at the server level.

[Video description begins] He clicks on sqlsvr172 in the table. A screen titled sqlsvr172 appears. It has a navigation pane. The main body contains details about the server. At the bottom, there are two tabs: Notifications and Features. The Features tab is currently selected. It has four tabs: All, Security, Performance, and Recovery. [Video description ends]

However, if I get back out of that, if I go into my database property sheet, notice that Geo-Replication is an option.

[Video description begins] He closes the sqlsvr172 page and shifts to the All resources page. He clicks on the 2nd sql resource listed in the table. A page titled sqldb172(sqlsvr172 /sqldb172) opens up. It has a navigation pane. The main body contains details about the server. Some of these are: Status, Location, Subscription, Server name, etc. He selects Geo-Replication from the navigation pane. A world map is displayed on the screen. [Video description ends]

Now currently, we can see that we've got an area on the map here in Eastern Canada where we've currently got our current SQL database deployed. That's the Azure region or location.

[Video description begins] The East Canada region has a blue tick mark on the map. [Video description ends]

Now if you're not so great with your geography, don't worry if you don't know what you're looking at on the map. Because when I scroll down a little bit, I can see indeed Canada East is that current region.

[Video description begins] He scrolls down the page. Below the map, there is a table with three columns: Server/Database, Failover Policy, and Status. In the table, there are two sections: Primary and Secondaries. Under Primary, Canada East is listed. Its Server/Database is sqlsvr172 /sqldb172, Failover Policy is None and Status is Online. [Video description ends]

And I can see the status of that replica is that it's online. Down below, it says for secondaries, Geo-Replication is not configured, and we can see here the list of target regions.

[Video description begins] He scrolls down the page. A section titled Target Regions appears. Some of the regions listed here are: West US, Canada Central, Brazil South, North Europe, etc. [Video description ends]

So let's say, for example, I want this to be replicated to Canada Central as a different region.

[Video description begins] He clicks on Canada Central from the list. A page titled Create secondary appears. It has the following criteria: Region, Database name, Secondary type, Target server, Elastic pool, and Pricing tier. The Region is Canada Central, Database name is sqldb172, Secondary type is Readable, Target server is Configure required settings, Elastic pool is None, and Pricing tier is Configure required settings. Lock icons are present next to Region, Secondary type, Elastic pool, and Pricing tier. At the bottom, there is an OK button. [Video description ends]

Now the secondary type is by default set to a readable replica of our Azure SQL database. Then for the target server, I have to configure settings. Now, what that means is I need another Azure SQL server instance.

[Video description begins] He clicks on Target server. A page titled Server appears. It has a button titled Create a new Server. No servers are listed on this page. Next to it, another page opens up. It is titled New Server. It has input boxes for Server name, Server admin login, Password, Confirm password, and a drop-down menu for Location. Below these fields, there is a check box that reads: Allow Azure services to access server. This check box as a tick mark. At the bottom, there is a Select button. [Video description ends]

Now if I don't have one, I have to create one here. So I'm going to call this sqlsvr172_central. You'll notice that it's going to tell me over here if it likes underscores, upper case letters, lower case letters. So notice here, it doesn't like a lot of the items that are specified here in the name.

[Video description begins] In the input box for Server name, he types sqlsvr172 _central. An error message pops up. It reads: Your server name can contain only lowercase letters, numbers, and '-', but can't start or end with '-' or have more than 63 characters. He then types sqlsvr172 central. [Video description ends]

So I'm just going to call it sqlsvr172central. And I'm going to specify the credentials and password here which do not have to be the same as they are for the primary server holding the master writable replica.

[Video description begins] The credentials for Server admin login, Password, and Confirm password appear. The login id is cirving. [Video description ends]

Once that's been filled in, I'm just going to go ahead and click Select.

[Video description begins] He clicks the Select button. The pages titled Server and New Server close. [Video description ends]

So really we're creating a new SQL Server instance to accommodate our Geo-Replication to a different Azure region. I'm going to leave the standard pricing tier as it is, and I'm going to click OK.

[Video description begins] In the page titled Create secondary, Target server changes to sqlsvr172 central (Canada Central) and Pricing tier changes to Standard S0: 10 DTUs, 250 GB. He clicks the OK button. The Create secondary page closes. [Video description ends]

Now currently, we have a message about the deployment being in progress up here in the upper right in the Azure portal.

[Video description begins] He points to the pop-up box titled Deployment in progress. [Video description ends]

And if I kind of scroll up here, and take a look here, after a moment, we'll see that it will reflect that we've got Geo-Replication from the Canada East region to the Canada Central region.

[Video description begins] On the map, a green dot with a check mark appears in the Canada Central region. [Video description ends]

And before you know it, you'll have this little check mark in these regions that are filled in with the solid color. You can see the other regions that are not filled in, and they do not have a check mark. So therefore, we don't have a replica of this SQL database in those locations.

And if we scroll down, we can see that represented in textual form. So, not only do we have our original online master replica in Canada East listed here at the top, but we can also see we've got a secondary now in the Canada Central region, and it's currently listed as being readable.

[Video description begins] He scrolls down to the table. Under Secondaries, Canada Central is listed. Its Server/Database is sqlsvr172 central/sqldb172. The Status is Readable.... [Video description ends]

Now if you click the ellipsis button, the three dots with the context menu next to the word readable, you'll have the option to force a failover which essentially promotes this to be the primary replica, and the other current primary would then become a secondary. It does say that this can cause some data loss while you're doing this.

[Video description begins] The presenter clicks the three dots next to Readable. Three options appear: Pin to dashboard, Forced Failover, and Stop Replication. He clicks on Forced Failover. A pop-up box titled Failover appears. It has two buttons: Yes and No. [Video description ends]

Also notice that we do have the option also of stopping replication, so, for instance, if there's a failure or a disaster in our primary region.

[Video description begins] He again clicks on the three dots next to Readable. He then points to Stop Replication from the list of options. [Video description ends]

NoSQL Overview

[Video description begins] Topic title: NoSQL Overview. Your host for this session is Dan Lachance. Screen title: NoSQL Databases. [Video description ends]

In the Microsoft Azure Environment, you can choose to deploy a SQL compliant type of database or a NoSQL database, so it's important to know the difference between the two. With NoSQL databases, we have a less rigid schema than with a traditional, relational database, such as MySQL or Microsoft SQL Server.

The schema, remember, is the blueprint of what type of data is allowed to be stored. And, so, with NoSQL, it's really designed to be much less structured to allow or accommodate for the storage of many different types of data. NoSQL is also designed for high scalability because it's really what is often used to work with very large data sets.

[Video description begins] Screen title : NoSQL Database Characteristics. [Video description ends]

So high performance and availability are a big part of NoSQL. Essentially, the manipulation and analysis of big data. With NoSQL, each stored row can actually store different types of data. That's unlike a relational database structure that has a blueprint or schema that defines exactly what can be stored in each row within a table.

We don't have that kind of structured limitation with a NoSQL database. There are a number of different types of storage configurations for NoSQL databases, such as key and value pairs, or NoSQL document stores, or graph database stores, but in the end, a NoSQL database is not relational.

[Video description begins] Screen title: NoSQL Horizontal Scaling. [Video description ends]

NoSQL uses horizontal scaling extensively as a traditional relational database system can as well.

[Video description begins] A diagram appears on the screen. It has a box with four blocks placed side by side. Below the box, there is a forward arrow. [Video description ends]

What this means is scaling out, as we see pictured in the diagram by adding database servers to handle the workload. Now this can be done for clustering purposes, for load balancing, and for replication of data.

[Video description begins] Screen title: Common NoSQL Database Products. [Video description ends]

Common NoSQL products include Azure CosmosDB, Azure Redis memory caching, which allows us to take data and cache it in memory. Data that is accessed frequently, so that subsequent requests are a service from the cache, which is much quicker than reading it from disk.

There are also numerous database options, including for NoSQL available to the Azure marketplace. So you can choose to deploy a new Virtual Machine instance that has a variety of different NoSQL products installed and configured for you already.


CosmosDB
  - Azure CosmosDB is a NoSQL database option available in Microsoft Azure. So it's a NoSQL solution that is globally distributed across Azure regions. 
    - This global distribution means that users can contact the nearest replica of CosmosDB to work with the data. 
    - That way, there's a better user experience instead of accessing it across multiple Azure regions.
  - Azure CosmosDB supports default encryption of data at rest, and it's used by a lot of popular services that you've probably heard of, like Xbox, Office 365, and Skype. When you start to deploy Azure CosmosDB, you begin by creating an Azure CosmosDB account, as we'll see in another demonstration. You also get to select the appropriate API for the account type.

Now, we might choose, for example, a certain type of account type like Gremlin if we want to use graph databases, or MongoDB if we're using a document type of database, and so on. So we'll see that as well when we configure Azure CosmosDB in a demonstration.

We can import data into CosmosDB from a number of different sources, including SQL databases. Now, even though CosmosDB is generally considered NoSQL, we still have the option of bringing in SQL data to store it in here. We also can specify CSV or comma separated value files as a data source or JSON files, and even standard NoSQL compliant databases like MongoDB.


Deploy CosmosDB
  - Microsoft Azure CosmosDB is a great choice when you have vast amounts of unstructured data that you want to store and manage in the cloud. I'm going to start here in the left-hand navigator in the Azure Portal by clicking Create a resource, and I'll search for cosmos.

[Video description begins] A Microsoft Azure webpage displays on the screen. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present here with the following options: Create a resource, Home, Dashboard, All services, and Favorites. The main body of the page has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]
  - From here, I'll choose Azure Cosmos DB, and then I'll click Create.

[Video description begins] He selects Azure Cosmos DB from the list of options. Some information about Azure Cosmos DB is displayed on the screen. Below it there is a Save for later button. A drop-down menu for Select a software plan is present on the screen, in which Azure Cosmos DB is selected by default. At the bottom, there is a Create button. He clicks the Create button. A page titled Create Azure Cosmos DB Account opens up. It has four tabs: Basics, Network, Tags, and Review + create. The Basics tab is currently open. It has two sections: Project details and Instance details. The Project details section has drop-down menus for Subscription and Resource Group. The Instance Details section has an input box for Account Name and drop-down menu for API. The default value for Subscription is Pay-As-You-Go. The default value for Resource Group is cloud-shell-storage-eastus. The default value for API is Core (SQL). At the bottom, there are three buttons: Review + create, Previous, and Next: Network. [Video description ends]

The first thing I'll do is place this into a resource group, and then, down below I need to create a Cosmos DB account name.

[Video description begins] He clicks the drop-down menu for Resource Group. A list of options appears. He selects Rg1 from the list. [Video description ends]

[Video description begins] He scrolls down the page. A drop-down menu for Location is also present on this page. Its default value is Australia East. For Geo-Redundancy, there are two options: Enable and Disable. Enable is selected by default. For Multi-region Writes, there are two options: Enable and Disable. Disable is selected by default. [Video description ends]

So I'm going to call this cosmosdb-acct172. Now, be careful because in some cases in Azure if you start to use weird symbols like underscores, it'll tell you. So luckily we have this kind of easy notification that there's something wrong with the name, so, just be aware of that.

[Video description begins] In the input box for Account Name, he types cosmosdb_acct172. An error message pops up. It reads: The name can contain only lowercase letters, numbers, and the '-' character, and must be between 3 and 31 characters. Name is invalid. He then types cosmosdb-acct172. [Video description ends]

Now for the API, I can determine exactly what it is I want to configure Cosmos DB as because really it's kind of like a multi-model type of solution. So, do we want to treat it as a core SQL solution?

Or do we want it to adhere to the MongoDB API standard, or Cassandra, or is it a table type of data store or a graph type of NoSQL data store? So in this example, I'll choose Azure Cosmos DB for Mongo DB API.

[Video description begins] He clicks the drop-down menu for API. A list of options appears. He selects Azure Cosmos DB for MongoDB API. [Video description ends]

Now, I would do that if I knew that I had perhaps an application or some code that was already written that needs to talk to the MongoDB API to access data. I'm going to specify an appropriate location.

[Video description begins] He clicks the drop-down menu for Location. A list of options appears. He selects Canada East from the list. [Video description ends]

I'm going to disable geo-redundancy. Notice, that was enabled by default for Cosmos DB. I also, of course, want to leave multi-region writes disabled since I've disabled geo-redundancy. When I click Next, I then have to place this into an Azure Vnet. So I'll choose a Vnet and a subnet.

[Video description begins] He clicks the Next: Network button. The screen shifts to the next tab titled Network. It has drop-down menus for Virtual Network and Subnet. He clicks the drop-down menu for Virtual Network. A list of options appears. He selects EastVnet1 from the list. A section titled Configure Firewall appears. For Allow access from Azure Portal, there are two options: Allow and Deny. Allow is selected by default. For Allow access from my IP (71.7.176.108), there are two options: Allow and Deny. Deny is selected by default. He clicks the drop-down menu for Subnet. A list of options appears. He selects EastSubnet1 from the list. At the bottom, there are three buttons: Review + create, Previous, and Next: Tags. [Video description ends]

And right now it's set to allow access from the Azure Portal, which is great. So I can use this portal GUI interface to make a connection to my Cosmos DB and also to connect to it and look at performance metrics and things of that nature.

And conveniently, it also has my current public IP address listed here, and I can click Allow to add, essentially, a firewall exception for CosmosDB, so that if I need to get in from on-premises, maybe I'm using a MongoDB GUI management tool, for example, that I will be able to get in, or maybe I've got some code segments running on-premises on a server that need to talk in Azure to my Cosmos DB to work with that. So I'm going to turn on Allow for that, and I'm going to click Next for tagging.

[Video description begins] He clicks on the Allow button for Allow access from my IP (71.7.176.108). [Video description ends]

Well, there's no tags in here to assign here so I'll just go to Review and create, and once the validation says it succeeded, I will create my Cosmos DB deployment.

[Video description begins] He clicks the Next: Tags button. The screen shifts to the next tab titled Tags. It has input boxes for Key, Value, and Resource Type. The default value for Resource Type is Azure cosmos DB account. At the bottom, there are three buttons: Review + create, Previous, and Next: Review + create. [Video description ends]

[Video description begins] He clicks the Next: Review + create button. The screen shifts to the next tab titled Review + create. A list of information is displayed on the screen. It has three sections: Basics, Virtual Network, and Firewall. At the bottom, there are two buttons: Create and Previous. [Video description ends]

[Video description begins] A pop-up message appears at the top of the screen. It reads: Validation Success. The presenter clicks the Create button. A page titled Microsoft.Azure.CosmosDB-20190313130002-Overview appears. It has a navigation pane with the following options: Overview, Inputs, Outputs, and Template. The Overview tab is currently selected. The main body of the screen has the following heading: Your deployment is complete. Above the heading, the following buttons are present: Delete, Cancel, Redeploy, and Refresh. It has a Go to resource button. At the bottom there is a table with the following four columns: Resource, Type, Status, and Operation Details. The table has one row of data. [Video description ends]

And after a moment we can see our deployment is complete, and we've even got a view over on the left here called Azure Cosmos DB, and I can see my deployed instance listed here.

[Video description begins] From the navigation pane of the home page, the presenter clicks on Azure Cosmos DB. A page titled Azure Cosmos DB appears. It has the following buttons: Add, Reservations, Edit columns, and Refresh. It has a table with the following columns: Name, Status, Location, and Subscription. One row of data is present in the table where the Name is cosmosdb-acct172, Status is Online, Location is Canada East, and Subscription is Pay-As-You-Go. [Video description ends]

I'm going to click it, and here we can see a number of interesting items.

[Video description begins] He clicks on cosmosdb-acct172. A page titled cosmosdb-acct172 appears. It has a navigation pane. The following buttons are present on this page: Add Collection, Refresh, Move, Delete Account, etc. Below the buttons, there is a list of details. These include: Status, Resource group, Read Location, Write Location, etc. [Video description ends]

For example, as I scroll down in the Overview part of the Properties blade, I can see the region into which it was deployed. This looks like Eastern Canada.

[Video description begins] He scrolls down the page. A section titled Regions appears. It has a world map. The East Canada region has a blue tick mark. [Video description ends]

And as I scroll down, I've also got this Data Explorer option, where I have buttons to create a new database or a new collection to begin working with data.

[Video description begins] From the navigation pane, he clicks on Data Explorer. A Data explorer page opens up. The following buttons are present here: New Database, New Collection, and Open Full Screen. [Video description ends]

Of course, you can do this programatically or using command line tools or even GUI tools that you might even run on-premises. You would just need to make sure you have a way to access Cosmos DB, and we're talking really here about adding a firewall exception, so here in the Properties blade, if I go to Firewall and virtual networks, notice here that my client IP, my public IP on the Internet, has been added here as being allowed in.

[Video description begins] He clicks on Firewall and virtual networks from the navigation pane. A page for Firewall and virtual networks opens up. For the field, Allow access from, there are two options with radio buttons: All networks and Selected networks. The Selected networks option is selected by default. A section titled Virtual networks is also present. It has a table with the following columns: Virtual Network, Subnet, Address Range, Endpoint Status, Resource Group, and Subscription. One row of data is present here. Virtual Network is eastvnet1, Subnet is 1, Address Range is 10.1.0.0/16, Resource Group is rg1, and Subscription is Pay-As-You-Go. Below it, there is a section titled Firewall. The following IP address is listed here. 71.7.176.108. [Video description ends]

[Video description begins] He highlights the following IP address: 71.7.176.108. [Video description ends]

But you would also need to go to Connection String because you would have to have the correct Cosmos DB host name, the port number to connect to, as well as the Cosmos DB user name, and either the primary or secondary password.

[Video description begins] From the navigation pane, he clicks on Connection String. A page for Connection String opens up. It has two tabs: Read-write Keys and Read-only Keys. Read-write Keys is currently open. It has the following criteria listed: Host, Port, Username, Primary Password, Secondary Password, Primary Connection String , Secondary Connection String, etc. The presenter highlights the values for Host, Port, and Username. The Host is cosmosdb-acct172.documents.azure.com. Port is 10255, and Username is cosmosdb-acct172. [Video description ends]

So you would do that, for example, if you were using some kind of MongoDB type of GUI tool on-premises that you wanted to reach into the cloud to this instance to make changes to.


Big Data Overview
  - Microsoft Azure provides numerous offerings related to working with Big Data. With Big Data, as the name implies, we're talking about vast datasets, large quantities of data that need to be processed and analyzed. Now, this has become more and more of a thing in recent years due to the Internet revolution and how much data is being produced on a daily basis.
  - Well, we might be getting this data from Internet of Things or IoT devices like baby monitors or surveillance cameras. Big Data sources can also include financial information, financial transactions for customers in a banking institution for instance or through medical research or even through cookies.

Cookies are preference files used on web browsers to track user preferences on websites, and also sometimes to track security authentication tokens used by users on websites. And so that can be a valuable source of data for things like marketing companies to know people's web browsing habits and their preferences, and that could be derived from cookies.

But all of this data needs to somehow be collected in a location that makes sense that can accommodate that amount of data, so a NoSQL database. And then it needs to be processed so we can draw meaningful insights from that data. Big Data has a number of characteristics that we need to consider, such as the amount of data that needs to be transmitted over a network. And then stored in some kind of a storage location, whether it's a data lake or a specific single database.

We have to think about the rate at which data is produced. How much data do we expect will be produced per day? Because when we have incoming data into our Azure solution for Big Data, we are paying a fee depending on how much data is coming in or going out in addition to being stored and being computed through a cluster.

We have to think about the wide variety of data types that we might be interested in working with such as financial transactions or customer web surfing habits through the collection of cookie data. And then we have to think about the accuracy of that data. One of the things we can do with Big Data is transform it to a different format that would be acceptable for our processing engine, at the same time, we can weed out irrelevant data.


Azure SQL Data Warehouse
The analysis of big data involves both the storage of vast datasets along with the processing of that raw data to result in meaningful insights. So part of Azure SQL data warehouse is certainly the Data storage component, but we've also got Parallel processing. This is done by having a cluster of compute nodes that work together to analyze big data stores.
So it can execute complex queries using what's called PolyBase. PolyBase differs a little bit from standard structured query language because it's designed to run against large datasets that get read from Apache Hadoop. And Apache Hadoop is a clustering solution designed for Big data analytics. Pictured on the screen, we have a sense of what the architecture looks like for Azure SQL data warehouse. Beginning on the left, we've got an application or an application component, that issues transact SQL or T-SQL commands.
Now, this gets sent to what's called the control node. The control node, like the name implies, controls the underlying cluster of compute nodes that actually perform the work. And so we can send a transact SQL command to the control node. The control node is then responsible for allocating that to compute nodes. And because we've got more than one compute node, plural nodes, it means that we can run some of these tasks at the same time or in parallel. Now, this is using underlying Azure storage to store, not only the data that we run the queries against but also any transformations that might result from the execution of those queries.
[Video description begins] An illustration appears. An icon titled App T-SQL commands is connected to a Control node. The Control node is further connected to Compute nodes on two sides. The Compute nodes then connect to an Underlying Azure storage. [Video description ends]

When you configure Azure SQL data warehouse, one setting you will specify is the data warehouse units or DWUs, which is a combination of performance factors related to things like CPU computing power, the amount of memory, and database input and output. All that together forms a data warehouse unit.

And the more data warehouse units you have, then the better performance you'll have when processing big datasets using the compute nodes within the cluster. Just like when deploying Azure SQL database, Azure SQL data warehouse also uses firewall rules to control inbound traffic.

So for example, you would have to add a rule for the appropriate IP address, or addresses, to allow inbound traffic to SQL over port 1433. To save on costs, you can also pause processing of data by the compute nodes. So therefore, you're only being billed for the storage related to Azure SQL data warehouse. And when you have sporadic testing that might be taking place, this is an important strategy to reduce costs.


Create an Azure SQL Data Warehouse
Azure SQL Data Warehouse is different than a standard Azure SQL database deployment, in that it's designed for parallel processing, so that we can quickly get results when we wanna run complex queries against large amounts of data.

[Video description begins] A Microsoft Azure web page opens up. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present on this screen. It contains the following options: Create a resource, Home, Dashboard, All Services, and Favorites. The main body of the page has a section titled Azure services. It contains the following options: Virtual Machine, Storage Account, App Services, etc. [Video description ends]

To get started with deploying an Azure SQL data warehouse here in the portal, I'll click Create a resource in the upper left.

[Video description begins] In the navigation pane, he clicks on Create a resource option. A section titled New opens. It has two columns: Azure Marketplace and Popular. The Azure Marketplace column contains the following options: Get started, Recently created, Compute, etc. The Popular column contains the following options : Windows Server 2016 Datacenter, Ubuntu Server 18.04 LTS, Web App, etc. [Video description ends]

And from the categories, I'm going to choose Databases and then on the right, I'm going to choose SQL data warehouse.

[Video description begins] From the Azure Marketplace column, he selects Databases. A column titled Featured appears. It contains the following options: Azure SQL Managed Instance, SQL Database, SQL Data Warehouse, etc. [Video description ends]

[Video description begins] He selects SQL Data Warehouse. A new window titled SQL Data Warehouse appears. It has input boxes for Database name and Collation. Drop-down menus are present for Subscription, Resource group, and Select Source. The default value for Subscription is Pay-As-You-Go. The default value for Select source is Blank database. The default value for Collation is SQL_Latin1_General_CP1_CI_AS. Selection fields for Server and Performance level are also present. The default value for Performance level is Gen2:DW1000c. At the bottom, a Create button is present. [Video description ends]

Now, you're going to need to use an Azure SQL server instance here. And if you don't already have one, you'll be able to create one throughout this process. So let's start by giving a database name here. I'm going to call this sqldatawarehousedb172, and I'm going to put this in an existing resource group.

[Video description begins] In the input box for Database name, he types sqldatawarehousedb172. [Video description ends]

[Video description begins] He clicks the Resource Group drop-down menu. A list of options appears. He selects Rg1. [Video description ends]

And for a data source I can have a Blank database, I could choose a Sample such as AdventureWorksDW, or I could simply take the source from a Backup of a database. In this case, why don't we go with some sample data from AdventureWorksDW.

[Video description begins] He clicks the Select source drop-down menu. A list of options appears. He selects Sample. He clicks the Select sample drop-down menu. A list of options appears. He selects AdventureWorksDW. [Video description ends]

Then I've got to specify the SQL server instance here. I'm going to click on that. And on the right, any existing SQL servers that I might want to tie this Data Warehouse to, I could. But in this case, we don't have any, so I'm creating a new one. We're going to call this sqlsvr172, and I'm going to specify the server admin for SQL server and I'll confirm the passwords.

[Video description begins] He clicks the Server field. A page titled Server opens. It has a Create a new server button. The presenter clicks the button. A page titled New Server opens. It has input boxes for: Server name, Server admin login, Password, and Confirm password. A drop-down menu for Location is also present. The default value for Location is Canada East. Below it there is a check box for Allow Azure services to access server. It has a tick mark. A Select button is present at the bottom. [Video description ends]

[Video description begins] In the input box for Server name, he types sqlsrv172. In the input box for Server admin login, he types cirving . He enters a password in the input boxes for Password and Confirm password. [Video description ends]

I'm going to deploy this SQL server instance in the Canada East Azure location or region and then I'll click Select. So we got the server taken care of, but the problem is I have a little notification symbol here that says, SQL Data Warehouse Generation 2 is not supported in this region, okay.

[Video description begins] He clicks the Select button. The Server and New Server pages disappear. The Server is: sqlsrv172 (Canada East). [Video description ends]

[Video description begins] He points to a red notification icon present next to the Server field. [Video description ends]

Well, that's set automatically as a default down below the performance level.

[Video description begins] He clicks on the field for Performance level. A page titled Configure performance opens. It contains two tabs: Gen2 and Gen1. Gen2 is selected by default. It has a scale with a pointer for Scale your system. The pointer is currently at DW1000c. At the bottom there is an Apply button. [Video description ends]

So if I choose Generation 1, Gen1, then the error message goes away.

[Video description begins] He shifts to the Gen1 tab. It has a scale with a pointer for Scale your system. The pointer is currently at DW400. The price of the system mentioned here is 5.32 USD/hour. [Video description ends]

At least for the region that I've selected it in. And this ties into the fact that some specific Azure service configurations are only available in some regions. Now, I can also determine which data warehouse unit selection or DWU that I want. And as I kind of scale my system up, notice of course the price per hour, US dollars goes up the more data warehouse units or DWUs that you allocate to your data warehouse.

[Video description begins] He moves the pointer and places it at DW1500. The price changes to 19.96 USD/hour. [Video description ends]

Remember that a data warehouse unit, or DWU, is a collection of performance factors like CPUs and memory. And so the best way to work with this, before you've got experience running your workloads in data warehouse is to start at a reasonably small DWU value and then gauge the performance as you run queries against the data. And if you need to, you can scale this up later or scale up back down.

[Video description begins] He moves the pointer again and places it at its original position DW400. The price again goes down to 5.32 USD/hour. [Video description ends]

So at this point, I'm going to click Apply, and down below, I'm going to click Create.

[Video description begins] He clicks the Apply button. The Configure performance window closes. [Video description ends]

[Video description begins] He clicks the Create button on the SQL Data Warehouse page. [Video description ends]

And we can now see the deployment is in progress.

[Video description begins] The Microsoft Azure home page is open. The presenter points to a notification at the top right corner. It reads: Deployment in progress. [Video description ends]

So SQL data warehouse is similar to when you deploy Azure SQL database in that you've got to determine how you need to make a connection into the database, such as from a non-premises environment perhaps where you're running SQL Server Management Studio. And, again, it's gonna make a connection over standard SQL ports like 1433 and so you'd have to add a firewall exception to allow that to happen. So now, on the left, I'm going to go to the SQL data warehouses view where we can see, we've got our SQL data warehouse database.

[Video description begins] In the navigation pane, under Favorites, he clicks on SQL data warehouses. A page titledSQL data warehouses opens. It has three buttons: Add, Edit columns, and Refresh. This page has a table with the following columns: Name, Status, Replication, Pricing Tier, Location, and Subscription. The table contains one row of data. The name is sqldatawarehousedb172. [Video description ends]

And if I click on it and open it up, then we can see if we scroll down, in the properties blade, for example, if I go to Quick start.

[Video description begins] He clicks on sqldatawarehousedb172. A page titled sqldatawarehousedb172 opens. It has a navigation pane with the following options: Overview, Activity log, Tags, etc. In the main body, the following buttons are present: Pause, Scale, Restore, New Restore Point, and Delete. Below the buttons, a list of information is present. It includes details about: Resource Group, Status, Server name, etc. [Video description ends]

I can see that we have a number of tools that we can use so that we can work with data in SQL data warehouse.

[Video description begins] In the navigation pane, under Settings, he clicks on Quick start. In the main body, a new page titled sqldatawarehousedb172 - Quick start opens. It has three sections: Get the tools, Integrate with your app, and Learn more. [Video description ends]

And that is available through the Microsoft Azure SDK, Azure PowerShell and also the Azure SQL Data Warehouse Migration Tool.

[Video description begins] He points to the options present under Get the tools section. [Video description ends]

And we also have information about Integration with our application, because the idea is that we'll have some kind of an application that is going to be interested in running these types of complex queries and gaining insights from data that is stored and managed by SQL data warehouse. Now, I do have a Geo-backup policy which takes a snapshot on a daily basis. However, this is kind of unlike the standard Azure SQL database geo-replication, because that same type of geo-replication option is simply not available with Azure SQL data warehouse.

[Video description begins] In the navigation pane, under Settings, he clicks on Geo-backup policy. A new page for Geo-backup policy opens. It has two buttons for Geo-backup policy: Enabled and Disabled. Currently, enabled is selected. [Video description ends]

The other thing to watch out for is if I scroll down as we were talking about, if I go to Firewalls and virtual networks. We can add allowances for which IPs are allowed to make a connection into SQL data warehouse.

[Video description begins] In the navigation pane, under Security, he clicks on Firewalls and virtual networks. A page for Firewalls and virtual networks opens. At the top, the following buttons are present: Save, Discard, and Add clientIP. For Allow access to Azure services, there are two buttons: ON and OFF. Currently, ON is selected. The Client IP address is: 71.7.176.108. A table is displayed with three columns: RULE NAME, START IP, and END IP. [Video description ends]

Again, over port 1433, the standard SQL port, and we can even add our current client's IP address in and I'll just go ahead and save that if we really wanted to.

[Video description begins] He clicks the Add client IP button. A row is displayed in the table. The RULE NAME is: ClientIPAddress_ 2019-3-14, the START IP is: 71.7.176.108, and the END IP is: 71.7.176.108. He clicks the Save button. [Video description ends]

[Video description begins] He clicks the Save button. [Video description ends]

Also, as we scroll down, notice here that we've got a preview feature here called Query editor.

[Video description begins] In the navigation pane, under Common Tasks, he clicks on Query editor (preview). A page for Query editor (preview) opens. It has a drop-down menu for Authorization type. There are input boxes for Login and Password. The Authorization type is: SQL server authentication. The Login is: cirving. At the bottom, there is an OK button. [Video description ends]

And so if I put in the credentials that I specified when I configured the SQL server.

[Video description begins] He enters the password and clicks the OK button. [Video description ends]

Then we can go ahead and actually start to peruse and work with some of the data in a very simple way, at least here directly in the portal.

[Video description begins] The Query editor (preview) is open. It has four buttons: Login, New Query, Open query, and Feedback. The window has two sections: sqldatawarehousedb172 (cirving) and Query 1. Below Query 1, there is a section with two tabs: Results and Messages. The Results tab is currently open. Under the heading sqldatawarehousedb172 (cirving), three pointers are displayed: Tables, Views, and Stored Procedures. [Video description ends]

So if I expand tables, let's say, I see that we've got, for example, a dbo.DimCustomer as a table, and I can even start working with queries.

[Video description begins] He clicks on Tables. It expands. The following options appear: dbo.DatabaseLog, dbo.DimCurrency, dbo.DimCustomer, etc. He clicks ondbo.DimCustomer. [Video description ends]

So maybe select all of the columns from dbo.dimcustomer, and then I can run that query, and we'll start to get the results listed here.

[Video description begins] In the Query 1 section, in line 1, he types: select * from dbo.dimcustomer. He clicks the Run button. [Video description ends]

Now, this is just a quick way to look at this, of course, you're going to have an application or some kind of a way to hook into this using other tools to actually work with this data properly for analysis.

[Video description begins] In the Results tab, a table appears. It has the following columns: Customer Key, Geography Key, Customer Alternate Key, and Title. [Video description ends]

Bear in mind, one of the reasons you might use Azure SQL data warehouse over just standard Azure SQL database is because when you run queries, now this is not a complex query, so imagine a much more in depth, detailed complex query. But when you do run queries here, what's going to be happening is that the query is going to be handled by a specific back end node that's got its own compute resources, like CPU and memory, as opposed to standard Azure SQL database, which does not support multiple parallel processing.

[Video description begins] He clicks the Close button at the top right corner of the Query editor (preview). A message box appears with the text: Your unsaved edits will be discarded. He clicks the OK button. [Video description ends]

The other thing to keep in mind is, you actually have the option of pausing your Azure SQL database warehouse if you're not actually going to use it.

[Video description begins] The SQL data warehouses page is open. He clicks onsqldatawarehousedb172 in the table. The sqldatawarehousedb172 page opens. [Video description ends]

So for the storage portion, you would still be paying, but not for the compute portion. And you can see up at the top here that we do have in the overview part of the properties blade, a pause button, which we could use to do just that and then we could resume it when we want to continue processing.

[Video description begins] He points to the Pause button. [Video description ends]

Azure HDInsight

[Video description begins] Topic title: Azure HDInsight. Your host for this session is Dan Lachance. [Video description ends]

Azure HDInsight is a Big data analytics solution that's hosted in the cloud, and so it's considered a Managed service, and with managed services in the cloud we're talking about something that's easy to provision and configure compared to if you had to set it up yourself manually on-premises. Azure HDInsight uses a number of underlying open source frameworks but it does allow for Node clusters working together to process large amounts of data, whether that data is like a real time feed through a data pipeline or whether that data is coming from some kind of massive data storage warehouse.

HDInsight Underlying Technologies includes but it is not limited to Apache Hadoop. Apache Hadoop is an open source framework that's used for distributed processing clusters. Apache Spark is similar in that, it is distributed in parallel processing, but what makes it a little bit different is it uses in-memory caching to speed things up.

Apache Kafka is another open source component that allows for real time streaming data pipelines to feed HDInsight. Another aspect of working with HDInsight is Extract, Transform, and Load or ETL, you might be familiar with this term with other database solutions. It's not exclusive to HDInsight, it's more of a standard methodology more than anything else, where we can start by copying a data from source, whether it's a data store of some kind in the database or whether it's real time streamed data.

In the next step, for transform, we can convert the data to a different format so it can easily be consumed by the target that might expect things in a different format, such as dates. Finally, we can load the data into some kind of a storage facility, whether it's a data warehouse or whether it's going to be treated as a real time data feed that's gonna be fed into some other component.

[Video description begins] Screen title: HDInsight Usage. [Video description ends]

So what do we use HDInsight for? Well, we know it's about big data analytics, but can we be a bit more specific than that? While using HDInsight, it can be related to Machine learning or ML, where we can gain insights from vast amounts of data that are fed into it. You can run very large petabyte-scale types of queries against this type of information, or it can be automated so the insights are gained based on code that's written, which can result in predictive analysis for future trends.

On the IoT side of things, the Internet of Things, we can have a large amount of IoT device telemetry that is fed into the HDInsight solutions. So we can draw conclusion from large datasets, whether those are related to the security of IoT devices or due to the nature of those IoT devices. We can draw conclusions, such as those related to monitoring industrial control networks and so on.

Deploy an Azure Hadoop Cluster

[Video description begins] Topic Title: Deploy an Azure Hadoop Cluster. Your host for this session is Dan Lachance. [Video description ends]

In this demonstration, I'm going to use the Azure portal to deploy an Apache Hadoop cluster. This is useful when you need to have multiple parallel processing for big data analytics.

[Video description begins] He opens a Microsoft Azure web page. It has a menu bar on the top. A search bar is also present. On one side is a navigation pane. It contains several options: Create a resource, Home, Dashboard, All Services, etc. Currently Dashboard is displayed. A list of Azure services is also present: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

To get started here in the portal, I'm going to click Create a resource in the upper left, and then I'm going to choose Analytics because what I'm really talking about doing here is using the HDInsight as your offering.

[Video description begins] He clicks on Create a resource. A page titled New appears. It contains a search bar at the top. The page has two columns. The first column is named Azure Marketplace. It has the following options: Get Started, Recently created, Compute, Networking, Analytics, etc. The second column is named Popular. It has the following options: Windows Server 2016 VM, Ubuntu Server 18.04 VM, Web App, SQL Database, etc. [Video description ends]

Now, when I select HDInsight, I've got to give a name for the clusters.

[Video description begins] In the first column, he clicks the Analytics option. A column titled Featured appears. It has the following options: Azure Data Explorer, HDInsight, Data Lake Analytics job, etc. He selects HDInsight. A new page titled HDInsight opens up. The page has the following two tabs: Quick create and Custom. Currently, the Quick create tab is selected, which has the following three steps: 1. Basics, 2. Storage, and 3. Summary. At present, step 1: Basics is selected. A page titled Basics is open. It has an input box for Cluster name, Cluster login username, Cluster login password, and Secure Shell (SSH) username. There are drop-down menus for Subscription, Resource group, and Location. There is a field for Cluster type. At the bottom, a Next button is present. The default value for Subscription is Pay-As-You-Go. The Cluster login username is admin. [Video description ends]

So I'll call it hdinsightcluster172. And it's going to use the .azurehdinsight.net DNS suffix by default.

[Video description begins] In the Cluster name input box, he types hdinsightcluster172. Below the input box, he highlights the following text: .azurehdinsight.net. [Video description ends]

and after a moment we'll have a check mark here that indicates that, that name is valid and unique, then I have to specify the cluster type. And this is where from the Cluster type drop-down list, I can specify I want to use a specific framework, in this case, Hadoop. It's going to use the Linux operating system.

[Video description begins] He clicks on Cluster type. A new page titled Cluster configuration appears. It has drop-down menus for Cluster type and Version. There are two options for Operating system: Linux and Windows. He clicks the drop-down menu for Cluster type. A list of options appears. He selects Hadoop from the list. The Version changes to Hadoop 2.7.3 (HDI 3.6). For Operating system, Linux gets selected. A section titled Features appears. [Video description ends]

And then I can choose from variations of the version of Hadoop, depending on how I'm going to interface with the cluster and what exactly I'm going to do with that. So I'm going to go ahead and just leave the default selection.

[Video description begins] He clicks the Select button at the bottom. The Cluster configuration page closes. [Video description ends]

I have to specify a Cluster login username, which I'm going to do here, and password. This is what I can use, for instance, if I log into the website to view overall metrics and details related to my Apache Hadoop Cluster.

[Video description begins] He types cirving in the Cluster login username input box. He types a password in the Cluster login password input box. [Video description ends]

And if I plan on using SSH for cluster access, then I can use the SSH username.

[Video description begins] The Secure Shell (SSH) username is set as sshuser. Below it, there is a check box for Use cluster login password for SSH. It has a tick mark. [Video description ends]

Notice here it's set to use the cluster login password also for SSH. So I'm going to tie this into an existing Resource group. I'm going to specify the appropriate Azure location for my config, after which I'll then click Next.

[Video description begins] He clicks the drop-down menu for Resource group. A list of options appears. He selects Rg1. He clicks the drop-down menu for Location. A list of options appears. He selects Canada East. [Video description ends]

[Video description begins] He clicks the Next button. In the HDInsight page, step 2: Storage is selected. A page titled Storage is displayed. It has a drop-down menu for Primary storage type. The Primary storage type is set as Azure Storage by default. For Selection method there are two options: My subscriptions and Access key. My subscriptions is selected by default. Below it there is a field for Select a Storage account. An input box for Container is also present. Its default value is hdinsightcluster 172-2019-03-14t11-47-2. Below it, there are fields for Additional storage accounts and Data Lake Storage Gen1 access. At the bottom, a Next button is present. [Video description ends]

Next, for the primary account storage type, I'm going to choose Azure Storage as opposed to Data Lake variations. This is going to be for data that is used by HDInsight as well as for logs that get generated. And I'll leave it on My subscriptions for access to that account. So, then I have to go through and choose a Storage account. So I'll choose one of my storage accounts.

[Video description begins] He clicks on Select a Storage account. A page titled Storage accounts appears. It has two options. He selects stor14567. The Storage account page closes. [Video description ends]

And then it'll make a storage container with the name listed down below here. And at this point, I'm going to click Next.

[Video description begins] He clicks the Next button. In the HDInsight page, step 3: Summary is selected. A page titled Cluster summary is displayed. It includes the following details: Basics, Security + networking, Storage, etc. [Video description ends]

So once the validation has passed, I'll be able to click Create to initiate my HDinsight Cluster, which in this case, is configured in the back end to use Apache Hadoop. So I'll go ahead and click on Create to start the process.

[Video description begins] He clicks the Create button. The HDInsight page closes. The Azure home page is open. A notification appears which reads: Submitting deployment. [Video description ends]

While that's happening, understand that the next couple of steps would really be for developers, where they would use some kind of a tool to interface with Hadoop to work with the data and workloads related to that data. Tools like Microsoft Visual Studio, the Azure storage explorer, or you could even, for example, use SSH to connect to the cluster and start actually issuing commands based on the Hadoop command syntax. So now I'll click on the All resources view on the left, and I'll filter it by hd as a prefix.

[Video description begins] In the navigation pane, he clicks on All resources. The All resources page opens. In the name input box, he types hd. One item is displayed in the table: hdinsightcluster172. [Video description ends]

And there's our HDInsight cluster, which I will click on.

[Video description begins] A page titled hdinsightcluster172 appears. At the top, the following buttons are present: Move, Delete, and Refresh. The page has a navigation pane with options such as: Overview, Activity log, Tags, Settings, etc. The main body has various details about the resource, such as Resource group, Status, Location, URL, etc. [Video description ends]

The first thing I'm interested in, in the Overview part of the properties blade is the URL, which I will copy to my clipboard, and I'm going to go ahead and open that up in another tab here in my web browser.

[Video description begins] He copies the URL: https: // hdinsightcluster172.azurehdinsight.net. [Video description ends]

I am then prompted to specify the Username and Password that I configured when I can set up this cluster in the first place. So we're going to go ahead and pop in those credentials.

[Video description begins] In the new tab, a Sign in box appears. He types cirving in the Username input box. He types the password in the Password input box. He presses Enter. [Video description ends]

And that's going to give me the cluster website page where I can start viewing a bunch of details. For example, from here I can go to Hosts, where I can get a list of a lot of the hosts that are being used here within my cluster for Apache Hadoop.

[Video description begins] An Ambari web page opens in the new tab. Five tabs are present at the top: Dashboard, Services, Hosts, Alerts, and Admin. Currently, the Dashboard is open. A navigation pane is present on the left. It contains the following options: HDFS, YARN, MapReduce2, etc. In the center pane, three tabs are present: Metrics, Heatmaps, and Config History. Currently, the Metrics tab is open. Two drop-down lists are present. In the first drop-down, Metric Actions is selected and in the second drop-down, Last 1 hour is selected. Beneath them, various details are displayed, such as HDFS Disk Usage, HDFS Links, Memory Usage, etc. [Video description ends]

[Video description begins] He clicks the Hosts tab at the top. The Hosts tab opens. An Actions drop-down list is present. A search input box is present to Filter by host and component attributes or search by keyword. Beneath it, a table is displayed with 9 columns: Name, IP Address, Rack, Cores, RAM, Disk Usage, Load Avg, Versions, and Components. [Video description ends]

If there are any alerts, as we can see it listed up here at the top in red, and also by a specific host here, then I can click to read any of those specific ideas.

[Video description begins] He points to the notification of 5 alerts displayed at the top of the screen. [Video description ends]

[Video description begins] In the 5th row under the Name column, he clicks on number 3 displayed next to the Name: wn4-hdinsi.e4hiqroou53uhf15yranlxhfmb.vx.internal.cloudapp.net. A new page opens by the same name. A table is displayed with four columns: Service, Alert Definition Name, Status, and Response. He points to the 3 items in red. Their status is CRIT. [Video description ends]

So we could see here that some of these alerts are related to connectivity issues because nothing has been actually done in this cluster at all thus far. So if I go back to the Dashboard, we'll see how we can get an overall usage of the data nodes that are available.

[Video description begins] He clicks the Dashboard tab at the top. The Dashboard opens. [Video description ends]

The Hadoop distributed file system, or HDFS disk usage among the nodes in the cluster.

[Video description begins] He clicks the first item in the Dashboard: HDFS Disk Usage. The following information is displayed: DFS used 1.6 MB (0.00%), non-DFS used 24.4 GB (1.55%), and remaining 1.4 TB (93.35%). [Video description ends]

Now you will use a variety of different tools to start loading data into Apache Hadoop as we mentioned. So this is what we would do at the administrative level. And from this point forward, it would be up to developers to interface with the Hadoop cluster to present data and workloads to be processed.

Azure Data Lake Analytics

[Video description begins] Topic title: Azure Data Lake Analytics. Your host for this session is Dan Lachance. [Video description ends]

Microsoft Azure Data Lake Analytics is a managed service offering in the Azure Cloud. It's designed for large scale data storage. We are talking about at the petabyte level. Now bear in mind, one petabyte equals approximately one million gigabytes. We're talking about potentially working with trillions of files. We can even take data, for example, that we might have stored in Azure storage account as blobs.

And we can actually copy that over into an Azure Data Lake store. For data analysis, we have to think about the kind of work-load power that we're going to need to work against these large types of data so that we can gain insights. And one consideration is configuring the Data Lake Analytic Unit, the DLAU. So this is a unit of measurement that's used to determine the underlying horsepower that's going to run our jobs where we can start to extract insights from this data.

So for example, each analytic unit contains a number of CPU course that are allocated to process data and also a chunk of memory. So at the time of this recording, one AU, one data lake analytic unit is two CPU cores and six gigabytes of RAM. So making a change to the data lake analytic unit really depends on the type of workload you envision will be handled through Azure data lake analytics. So this tells us then that we're talking about a large-scale parallel processing solution that uses node clusters.

[Video description begins] Screen title: Azure Data Lake Big Data Query Tools. [Video description ends]

We can use the Microsoft Visual Studio IDE, the integrated development environment, as a way to gain access to our Azure data lake and to begin running queries. We can also use the Eclipse IDE. We can use the IntelliJ IDE. All of these different integrated developer environments allow you to write code in a variety of different languages.

It really boils down to using whatever you are most familiar with, however, it's important to understand that these three IDEs are supported to hook into Microsoft Azure. And so in other words, there's an Azure toolkit that keeps getting updates for each and every one of these three items.

And these three items, these three IDEs also have plugins, even give them extended capabilities. So, Azure data lake storage then can be used to feed data into an Apache Hadoop cluster for parallel processing as part of data analysis. The Apache Hadoop cluster uses the Hadoop Distributed File System or HDFS. The jobs that we submit against that use what's called U-SQL.

[Video description begins] U-SQL is a large-scale data analysis language. [Video description ends]

This is even a type of project that you can launch if you're using GUI IDE tools like Microsoft Visual Studio. So U-SQL then, is just a simple language that you'll learn very quickly if you are already familiar with structured query language or just SQL.

Create a Data Lake Analytics Account

[Video description begins] Topic Title: Create a Data Lake Analytics Account. Your host for this session is Dan Lachance. [Video description ends]

Just like a lake in the real world can have many incoming streams or tributaries to result in the water collected in the lake, Azure data lake in the Azure cloud allows us to specify a multitude of data sources to allow data to be fed into data lake.

[Video description begins] He opens a Microsoft Azure home page. It has a menu bar on the top. A search bar is also present. A navigation pane is present with the following options: Create a resource, Home, Dashboard, All Services, etc. A list of Azure services is also present: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

Not only is it data storage, but we're talking about analysis of that data. So to get started here, I'm going to go into the Azure portal and click Create a resource in the upper left.

[Video description begins] He clicks on Create a resource. A page titled New appears. It contains a search bar at the top. The page has two columns: the first column is named Azure Marketplace. It has the following options: Get Started, Recently created, Compute, Networking, Analytics, etc. The second column is named Popular. It has the following options: Windows Server 2016 Datacenter, Ubuntu Server 18.04 LTS, Web App, SQL Database, etc. At the top, there is a search bar. [Video description ends]

Now because we're talking about analytics, I'm going to choose the Analytics category. And you'll see over on the right that we have Data Lake Analytics, which I will click.

[Video description begins] In the first column, he clicks on Analytics. A column titled Featured appears. It has the following options: Azure Data Explorer, HDInsight, Data Lake Analytics, etc. [Video description ends]

[Video description begins] He selects Data Lake Analytics. A new page titled New Data Lake Analytics opens up. An input box is displayed for Name. Drop-down lists are present for Subscription, Resource group, and Location. By default, the Resource group is cloud-shell-storage-eastus, The Subscription is Pay-As-You-Go and the Location is East US 2. A field for Data Lake Storage Gen1 is also present here. There are two options for Pricing packages: Pay-as-You-Go and Monthly commitment. Pay-as-You-Go is selected by default. A Create button is present at the bottom. [Video description ends]

So, what we can do is feed data into our Azure data lake. And then that data can be processed and transformed and manipulated for the purposes of gaining insights as to all of that collection of raw data, it can even be used for things like machine learning. So, I have to create a new data lake analytics account. I'm going to call this datalake172, and notice it's going to add the .azuredatalakeanalytics.net DNS suffix at the end.

[Video description begins] In the Name input box, he types: datalake172. [Video description ends]

[Video description begins] He points to the text present below the Name field: datalake172.azuredatalakeanalytics.net. [Video description ends]

I will deploy this into an existing resource group and choose a location that makes sense for me, and then down below, I've got to also create a data lake storage account.

[Video description begins] He clicks the drop-down menu for Resource group. A list of options appears. He selects Rg1. He clicks the drop-down menu for Location. A list of options appears. He selects Central US. [Video description ends]

[Video description begins] He clicks on Data Lake Storage Gen1. A page titled Select Data Lake Storage appears. There is an input box for name. A button titled Create new Data Lake Storage Gen1 is also present. Beneath it, the following text is displayed: 0 accounts found and No existing Data Lake Storage Gen1. [Video description ends]

So I'm going to click Create new Data Lake Storage Gen1, it's already got a name for it, that's fine, let's go with that. I'll leave it on Pay-as-You-Go and Encryption as enabled, so I'll click OK for that, and then I'll click Create to actually create this resource.

[Video description begins] A page titled New Data Lake Storage Gen1 appears. It has an input box for Name. The value here is datalake172adls. For Pricing package, there are two options with radio buttons: Pay-as-You-Go and Monthly commitment. Pay-as-You-Go is selected by default. The default value for Encryption settings is Enabled. An OK button is present at the bottom. [Video description ends]

[Video description begins] He clicks the OK button. The New Data Lake Storage Gen1 page and the Select Data Lake Storage Gen1 page disappear. [Video description ends]

[Video description begins] In the New Data Lake Analytics page, he clicks the Create button. [Video description ends]

Okay, so now I'm gonna go to the All resources view on the left and I'll filter it for things that start with the data.

[Video description begins] The Azure home page opens. In the navigation pane, he clicks on All resources. A new page titled All resources opens. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. Below these options, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. [Video description ends]

[Video description begins] In the name input box, he types data. Two items are displayed in the table: datalake172 and datalake172adls. [Video description ends]

We can see the two resources that resulted from our configuration, the data lake storage and the data lake analytics resource, which I'm going to click on to pop into the properties.

[Video description begins] He clicks on datalake172. A page titled datalake172 appears. At the top, the following buttons are present: New job, Sample scripts, Data explorer, etc. The page has a navigation pane on the left with the following options: Overview, Activity log, Tags, Settings, etc. The main body has details about the resource. Some of these are: Pricing tier info, AU-hours used, Estimated cost, etc. [Video description ends]

So when we're in here, notice right away that we have the option of submitting a job. So what we're talking about doing here is, submitting a job for processing for data lake analytics. Now of course, that could be fed data that we've configured into our data lake configuration. And if I were to scroll down, you'll see in the properties blade here indeed we do have data sources.

[Video description begins] In the navigation pane, under Settings, he clicks on Data sources. A new page titled datalake172- Data sources appears. At the top, a button titled Add data source is present. A table is displayed with two columns: Name and Type. A data source named datalake172adls (default) is listed in the table. [Video description ends]

Currently, for our data lake analytic configuration, we've got our data lake account that we specified for storage upon creation, but notice that we could add additional data sources. We also have some other configuration items, like for example, the maximum number of concurrent running jobs.

[Video description begins] In the navigation pane, under Settings, he clicks on Limits and policies. A new page titled datalake172- Limits and policies appears. At the top, two buttons are present: Save and Discard. Scales with sliders are present for the following criteria: Days to retain job queries, Maximum AUs, Maximum number of running jobs, Maximum AUs per job, Priority per job, etc. [Video description ends]

We've got a slider here to draw that up or down, depending upon what our specific needs hour, our processing might entail. If I were to click Tools in the properties blade, we then have a variety of tools that we can work with from a developer perspective to feed data into Data Lake Analytics, and then to determine which job should process that data, so there are Data Lake Tools for Visual Studio, and as we scroll down, it's also available for Azure PowerShell, and Azure CLI.

[Video description begins] In the navigation pane, under Getting started, he clicks on Tools. A new page titled datalake172- Tools appears. Some of the options listed here are: Data Lake Tools for Visual Studio, Azure PowerShell, Azure CLI, etc. [Video description ends]

Add a Data Lake Data Source

[Video description begins] Topic Title: Add a Data Lake Data Source. Your host for this session is Dan Lachance. [Video description ends]

You can feed data into Azure Data Lake Analytics programmatically, through command line tools and also through the GUI here in the portal.

[Video description begins] A Microsoft Azure web page titled All resources opens. It has the following buttons on top: Add, Edit columns, Refresh, etc. Below these buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. In the input box for name, data is entered. Below these options, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. Two items are displayed in the table: datalake172 and datalake172adls. [Video description ends]

So here in the Azure portal, I'm already looking at my Azure data lake analytics resource which I will click on to open up its properties blade.

[Video description begins] He clicks the first data resource listed in the table: datalake172. A page titled datalake172 appears. At the top, the following buttons are present: New job, Sample scripts, Data explorer, etc. The page has a navigation pane on the left with the following options: Overview, Activity log, Tags, Settings, etc. The main body has details about the resource, such as Pricing tier info, AU-hours used, Estimated cost, etc. [Video description ends]

If I scroll down, I have an option here called Data sources where I'll see the data lake account that's already available for data lake analytics.

[Video description begins] In the navigation pane, under Settings, he clicks on Data sources. A new page titled datalake172- Data sources appears. At the top, a button named Add data source is present. A table is displayed with two columns: Name and Type. One data source nameddatalake172adls (default) is listed in the table. [Video description ends]

And if I actually click on that, I can see some details, the name and the type. This is Azure Data Lake Storage Gen1.

[Video description begins] He clicks on datalake172adls (default). A page titled datalake172adls appears. It has details about the data source where the name isdatalake172adls and type is Azure Data Lake Storage Gen 1. He closes the page. The datalake172- Data sources page opens. [Video description ends]

However, if I wish, I can also go down and start exploring the data by clicking Data explorer.

[Video description begins] In the navigation pane, under Data Lake Analytics, he clicks on Data Explorer. A new page titled datalake172adls appears. At the top, the following buttons are present: Filter, New folder, Upload, Access, etc. A table with the following three columns is present: Name, Size, and Last Modified. Two folders are listed in the table: catalog and system. [Video description ends]

Now, this will be based on what we've added as data sources, as we can see. And I can start browsing through all of the files in the file system related to that storage.

[Video description begins] He clicks the folder named system. On the same page and in the same table, a file titled _placeHolder_ is displayed. He clicks the close icon at the top right corner. The datalake172adls page closes. A page titled All resources is open. It has two options: datalake 172 and datalake172adls . datalake172 is selected. A page titled datalake172-Data explorer is open. It has a navigation pane and a list of folders. [Video description ends]

So to add additional storage, I'm going to scroll back up in the properties blade and choose Data sources and then I'll click Add data source.

[Video description begins] In the navigation pane, under Settings, he clicks on Properties. A page for Properties opens. It has a navigation pane. He clicks on Data sources from the navigation pane. A page titleddatalake172- Data sources page appears. He clicks the Add data source button at the top. A new page titled Add data source appears. Drop-down menus are present for Storage type, Selection method, and Azure storage. There is an Add button at the bottom. [Video description ends]

And in this case, I'm interested in Azure storage and what I'm going to do is specify the Select account option or I can choose an Azure storage account.

[Video description begins] He clicks the drop down menu for Storage type. Two options appear: Azure Data Lake Storage Gen 1 and Azure Storage. He selects Azure Storage. [Video description ends]

[Video description begins] He clicks the drop down menu for Storage method. Two options appear: Select account and Account name. He selects Select account. He clicks the drop-down menu for Azure Storage. A list of options appears. He selectsstor14567. [Video description ends]

So I'm going to select an existing Azure storage account that has data that I would like to feed into Azure data lake analytics for further processing. So, I'm gonna go ahead and click Add.

[Video description begins] The Add data source page closes. The datalake172- Data sources page is open. A notification appears at the top right corner. It reads: Data source operation complete. Another Data source named stor14567/ is added to the table. [Video description ends]

And after a moment, we can see that our storage has been added, and, if I click on it, notice here it's not data lake storage, but rather, just simple Azure storage, as in a storage account.

[Video description begins] He clicks on stor14567/. A page titled stor14567/ appears. It contains details about the Name and Type. The Name is stor14567/ and the type is Azure Storage. He clicks the Close button at the top right corner of the page. The page closes. The datalake172- Data sources page is displayed. [Video description ends]

And so now that I've done that, if I scroll down for instance and go to Data explorer, now I may have to refresh this.

[Video description begins] In the navigation pane, under Data Lake Analytics, he clicks on Data Explorer. The datalake172adls page appears. [Video description ends]

So I'll click Refresh, and of course, I'll close what I was looking at previously because now I can see besides my data lake storage, I've also got my storage account here, my Azure storage account stor14567, it was called.

[Video description begins] He clicks the More button. A list of options appears. He clicks on Refresh. [Video description ends]

[Video description begins] He closes the datalake172adls page. 2 pages are simultaneously open: All resources and datalake172- Data explorer. A page titled All resources is open. It has two options: datalake 172 and datalake172adls . datalake172 is selected. A page titled datalake172-Data explorer is open. It has a navigation pane and a list of folders.These folders are: Storage accounts, datalake172adls (default), catalog, system, stor14567, Catalog and datalake172. [Video description ends]

[Video description begins] He expands the stor14567 folder. It contains 3 folders. He points to the folder named pics. [Video description ends]

I can even start browsing through folders in that Azure storage account to expose content. In this case, I've got a jpeg image.

[Video description begins] He clicks on the pics folder. A new page titled stor14567 opens. The following three buttons are present at the top: New folder, Upload, and Properties. A table with the following four column is displayed: Name, Type, Size, and Last Modified. A file named dog.jpg is listed in the table. [Video description ends]

Now, notice here, if I select that, I can get a preview of what is in that specific file.

[Video description begins] He clicks on dog.jpg. A page titled File Preview opens up. The following four buttons are present at the top: Format, Download, Properties, and Delete file. [Video description ends]

Normally, you'll have to download it to do that, as the built in filters often will not show you anything that makes any sense, it really depends on the file type, but notice we can also upload content even from this interface instead of go out to the storage account in Azure, including managing the hierarchy by creating folders and so on.

[Video description begins] He closes the File Preview page. The stor14567 page is displayed. [Video description ends]

And so it's important then to add the appropriate data sources to Azure data lake analytics so that you can begin to submit jobs that will process that data, and we'll see how to do that in another demo.

[Video description begins] In the navigation pane, under Settings, he clicks on Data sources. The datalake172- Data sources page opens. [Video description ends]

Work with Data Lake Datasets

[Video description begins] Topic title: Work with Data Lake Datasets. Your host for this session is Dan Lachance. [Video description ends]

Azure Data Lake Analytics is designed to be used as a large scale centralized data storage repository where data can come from many different sources. But it's also used for submitting jobs, so that we can process that data and gain insights from that data.

[Video description begins] A Microsoft Azure web page titled All resources opens. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present with the following options: Create a resource, Home, Dashboard, All Services, Favorites, etc. There are five buttons at the top: Add, Edit columns, Refresh, Export to CSV and Try preview. Below these buttons, there is an input box for name and drop-down menus for resource groups, types, location, tags, and grouping. The name input box contains the following text: data. A table is displayed with six columns: NAME, TYPE, RESOURCE GROUP, LOCATION, SUBSCRIPTION, and TAGS. The table has two rows with NAME: datalake172 and datalake172adls. [Video description ends]

So, here in the portal, I've gone to the All resources view, I've filtered by data, because I know that my data lake analytics configuration is called datalake172.

[Video description begins] He highlights data in the name input box. [Video description ends]

So I'm going to go ahead and click to open that up. And what I'm interested in doing is submitting a job.

[Video description begins] He clicks on datalake172. A page with the same name opens. It has a search bar followed by a navigation pane with the following options: Overview, Activity log, Access control (IAM), etc. At the top, the following buttons are present: New job, Sample scripts, Data explorer, etc. The main body has details about the resource. Some of the options here are: Pricing tier info, AU-hours used, Estimated cost, etc. [Video description ends]

Now I have a New job button right here at the top in the overview part of the Properties blade. I could also scroll down under the data lake analytics section, and here I would also see New job.

[Video description begins] In the navigation pane under the Data Lake Analytics section, he clicks on the New job option. A new page titled datalake172 - New job appears. At the top, the following buttons are present: Data Explore, Open file, and Save as. There is an input box for Job name. It has the following text: New job. There is a horizontal bar with a pointer for AUs, where the pointer lies at 1. The following details are also present here: Account: datalake172, Estimated cost: USD 0.03/minute, etc. There is a Submit button. A code editor area is also present here. [Video description ends]

So I can give a name to the job, and I'm going to go ahead and specify the code for it down below, this syntax is called U-SQL.

[Video description begins] 10 lines of code appear in the code editor area. Code line 1 reads: @a =. Code line 2 reads: SELECT * FROM. Code line 3 reads: (VALUES. Code line 4 reads: ("Customer1", 190.0),. Code line 5 reads: ("Customer2", 100.0). Code line 6 reads: ) AS. Code line 7 reads: D(customerID, amount );. Code line 8 reads: OUTPUT @a. Code line 9 reads: To "/customerdata.csv". Code line 10 reads: USING Outputters.Csv();. [Video description ends]

So it's kind of a combination of the C# programming language along with structured query language, or SQL to give it a bit more power. And the reason it exists is because structured query language under itself is really not designed to handle Big Data, whereas this is designed to work with that through, in this case Azure Data Lake Analytics. So, what we're doing here is creating a, I'm going to be creating a file here, I'm creating a tiny dataset. Now, of course, we can bring this dataset in from many other ways. But all I'm doing is creating a file here called customerdata.csv, and I'm going to have a CustomerID column, or field definition along with amount, and I can see I'm feeding it a couple of sample rows here, Customer1, with a numeric amount of 190, Customer2, with numeric amount of 100.

[Video description begins] In the code line 9, he highlights: customerdata.csv. [Video description ends]

[Video description begins] He points at code line 7. [Video description ends]

Now, your U-SQL jobs can be much more complex, and they can actually deal with processing of data.

[Video description begins] He highlights code lines 4 and 5. [Video description ends]

All I'm doing here is trying to illustrate a very basic simple example, so you get the sense of the construct. The overall skeletal framework that is used to work with Data Lake Analytics and start to process information. So once this has been done, I'm going to go ahead and click Submit. Now before I do that actually, before I submit I can also adjust the performance, the AUs because what I'm doing here is determining how many things can be processed parallel at once.

[Video description begins] He moves the pointer of the AUs horizontal bar to 13. [Video description ends]

And so depending on the nature of your U-SQL, we'll determine if you need to do this. And because this is very simple, I'm not gonna need to adjust that. So I'm going to go ahead and submit this.

[Video description begins] He moves the pointer back to 1. He then clicks the Submit button. [Video description ends]

So the job is submitted, it's going to take me to a new dashboard where we can see it's currently in the preparation phase, after which it'll be queued for processing, it'll be run, and then we'll be able to examine the result. In this case, the result should be that we've got that customer data file with the data in it.

[Video description begins] A page titled New job appears. It has four buttons: Refresh, Resubmit, Reuse script, and Cancel job. There is a section titled Status: Preparing. It has four steps: Preparing , Queued, Running, and Done, where Preparing is currently active. The following details are also present here: Progress: 0%, AUs: 1, Consumed AU-hours: 0, etc. The other side of the page has the following tabs: Job graph, Script, Data, AU analysis, and Diagnostics. Currently, the Job graph tab is open. It has a drop-down menu for Display and a button for Playback. [Video description ends]

And we can now see that the status of our job is such that it has succeeded.

[Video description begins] The Status changes to Succeeded and Progress shows 100%. [Video description ends]

And so if we go, let's say to the Data tab here, look at any Outputs. We can see indeed we've got customerdata.csv file, but let's back out of here.

[Video description begins] He clicks on the Data tab. The following two tabs are present here: Inputs and Outputs. He clicks on Outputs. There is a table with two columns: Name and Size. It has one item with the name: customerdata.csv. He then closes the page. The screen shifts back to the datalake172-New job page. [Video description ends]

And, why don't we run the Data explorer option here, just to go through our data, and sure enough notice there it is, customerdata.csv, it's in our data lake storage.

[Video description begins] He clicks on the Data Explorer option from the navigation pane. The datalake172adls page opens. It has the following buttons: Filter, New folder, Upload, Access, Folder properties, etc. A table is displayed with three columns: Name, Size, and Last Modified. It has three items with names: catalog, system, and customerdata.csv. [Video description ends]

And, if I we're to actually open that up and preview, we can see our two customers along with the amounts that were specified in our U-SQL script.


IoT Overview
  - The Internet of Things, otherwise called IoT, is really just a general umbrella term that refers to a large variety of devices that communicate over the Internet. Now that relates to Microsoft Azure in the sense that we can register devices with this central location in the Azure Cloud, and we can receive data from these IoT devices and monitor them through the Azure IoT Hub.But we'll talk to that effect later on in more detail. There were plenty of examples of IoT devices.
  - Things like water pressure valves and gauges and their current settings, baby monitors, smart cars that allow their details about their internal systems and their location to be made available over the Internet in a secured manner, medical equipment that can be controlled through the Internet, as well as home automation features, things like environmental control and turning on lights, and so on. This is just but a subset of examples of IoT types of devices that have Internet connectivity.
The thing to be careful of with this is with consumer grade IoT devices. Often, security is just not a priority. In many cases, you'll find that firmware might not even be updatable when there are security holes that are revealed about a specific type of consumer grade IoT product, like a home automation device. As with all security hardening in IT, when it comes to IoT devices we should always take care to make sure that default settings, like credentials to access a web interface on the IoT device are changed.
AlsoIoT devices should be placed on an isolated and secured network. The reason for this is because if an IoT device is compromised, we want to make it as difficult as possible for the attacker to connect to other devices on the network where the IoT device resides. So by putting it on its own protected network, we're adding that extra layer of security.
Pictured on the screen, we have an example of the Shodan website, which is essentially an IoT search engine, where we can search for items, as I have done here, such as home automation, and it will index any discovered devices that might appear to be vulnerable out there on the Internet.

[Video description begins] A screenshot of SHODAN website is displayed on the screen. On the left, there is a navigation pane. It has the following headers: Total Results, Platform, Type, and Author. At the top, there is a search box for Exploits. Home automation is entered in the search box. In the center pane, the following results display: keware technologies homeseer 1.4 - Directory Traversal, Schneider Electric SBO / AS - Multiple Vulnerabilities, etc. [Video description ends]

And so when we work with the Azure IoT services, we have a centralized way to securely receive this information from IoT devices. And from there, we might even feed that data into things like Azure machine learning, to determine if vulnerabilities might exist or if there's suspicious activity related to those IoT devices that needs to be addressed.


IoT Central
  - Azure IoT Central is an Azure managed service. This means the underlying complexities of setting up the infrastructure to support the IoT central service, whereby we can work with our IoT devices, those complexities are hidden from us so we can focus on actually doing what the service offers, which is to centrally manage IoT devices.So it is a separate Azure resource that we deploy, and we have a URL that would use the DNS suffix of .azureiotcentral.com for the IoT central website. So it provides us the ability to centrally manage IoT devices that have been registered through the IoT hub.

It also allows us to monitor them, and we can even build triggers that look at some of the data that we're monitoring and can take specific actions, such as the pressure in a valve in a remote planned exceeding a given threshold value that is considered safe and acceptable. Pictured on the screen, we see an example of the Azure IoT Central Management Portal.

[Video description begins] A screenshot of the IoT 172 portal appears. The page URL: https://iot172.azureiotcentral.com has been highlighted. On the left, there is a navigation pane with the following tabs: Dashboard, Device Explorer, Device sets, etc. The Dashboard is currently open, and it has the following title: CONTOSO. Some of the options here are: Quick Start Demo, Tutorials, Add Device Set, etc. [Video description ends]

Notice that the URL uses the DNS suffix as we've mentioned of .azureiotcentral.com. This one was created using a template and you can see here the page lists CONTOSO, but notice on the left in the navigator we can also explore IoT devices that are registered. As we drill deeper into this sample Azure IoT central management portal, we can start exploring devices.

[Video description begins] A similar screenshot for IoT 172 portal appears. Here the center pane has the title: Refrigerator 1. Below the title, the following tabs are present: Measurements, Settings, Properties, Commands, Rules, and Dashboard. The Measurements tab is currently active. It has a list of Telemetry items. [Video description ends]

In this example, we are exploring a refrigerator IoT enabled device, where we can see some telemetry items such as items related to gyroscopes and pressure, and so on. So depending on the nature of the IoT device will determine what is seen here. But, again, we can configure actions that would look at thresholds that might be exceeded, in this case, maybe a temperature for a refrigerator getting too low or too high. And that could trigger the sending of an email to administrators to do something about this.


IoT Hub
Azure IoT Hub is a separate type of Azure resource that you can deploy much like you might deploy an Azure Virtual Machine. Azure Io Central uses an IoT Hub, but the IoT Hub isn't directly manageable in this particular case. But you might wonder, what does the IoT Hub exactly do? The purpose of the IoT Hub, as the name implies, hub meaning some kind of a centralized repository where we have IoT devices that are connected.

And from there, we can receive messages from IoT devices. So details about the statistics related to what that IoT device does, such as monitoring temperature controls in a building. But we can also configure it so that we send commands to control those remote IoT devices, such as to adjust the temperature. And developers can choose a wide variety of programming languages to do that in. But before all of this can happen, IoT devices need to be connected to the IoT Hub.

And that's done through connection strings that will show up after you've built your IoT Hub resource. So there's a device registration connection string to initially get a device connected to IoT Hub. When you deploy your IoT Hub, one of the things you'll get to deal with is the IoT sizing which really deals with the number of messages for throughput that you want your IoT Hub to be able to handle.

Now, not only device registration is of interest here, but also device message transmission, either from the device to the IoT Hub or command sent from the IoT Hub to devices to control them. So what might we use Azure IoT Hub for? Well, because there's a wide variety of IoT devices out there, the uses are many as well.

We could use it for medical device tracking, not only to track the device itself and where it is, but also, of course, to track all of the detailed statistics provided by that IoT device, which could include things like vital signs of the patient to which that medical device is connected. IoT Hubs can also be used to register and track information related to industrial machinery controls, or remote building, heating ventilation, and air conditioning control.
IoT devices will make a connection to IoT Hub and transmit data using a number of different protocols, depending on the configuration, one of which is HTTPS over TCP port 443. However, we've also got AMQP. This is the Advanced Message Queuing Protocol. This is a standard for IoT device transmission of data that uses port 5672, and it's designed to work on a number of different platforms not, for example, just Windows.

MQTT is the Message Queuing Telemetry Transport. This is another type of protocol used by IoT devices that uses TCP port 1883. Now, which one should you use? Well, for example, AMQP is a mature standardized protocol that provides more potential functionality than MQTT does, but it does so at a cost of higher overhead.


Configure IoT Hub
The Azure IoT Hub is a centralized Azure resource that's deployed in the Azure cloud that allows us to connect a multitude of IoT devices for the purposes of managing those devices and monitoring any data that they might send into Azure IoT Hub.

[Video description begins] A Microsoft Azure webpage opens. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is also present with the following options: Create a resource, Home, Dashboard, All services, and Favorites. The main body of the page has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

To get started here in the portal, I'm going to click Create a resource over on the left, and I'm going to search for IoT.

[Video description begins] He clicks on Create a resource from the navigation pane. A screen titled New appears. It has a search bar. The presenter types iot in the search bar. A list of options appears. [Video description ends]

And I'm going to choose IoT Hub.

[Video description begins] He selects IoT Hub from the list of options. A screen titled IoT Hub appears. It has a drop-down menu for the field Select a Software plan. IoT Hub is the default selection. There is a Create button at the bottom of the page. [Video description ends]

From here, I'll click Create, and I'll start by tying this into an existing resource group I've created.

[Video description begins] He clicks the Create button. A screen titled IoT opens. There are three tabs on this screen. These are: Basics, Size and scale, and Review + create. The Basics tab is currently active. It has a section titled Project Details. There are drop-down menus for Subscription, Resource Group, and Region. An input box for IoT Hub Name is also present. The default value for Subscription is Pay-As-You-Go. The default value for Resource Group is cloud-shell-storage-eastus. The default value for Region is West US. At the bottom there are two buttons: Review + create and Next: Size and scale. He clicks the drop-down menu for Resource Group. A list of options appears. He selects Rg1 from the list. [Video description ends]

And like pretty much deploying any resource in Azure, I'll select an appropriate region or location. And I'm going to give a name to this.

[Video description begins] He clicks the drop-down menu for Region. A list of options appears. He selects Canada East from the list. [Video description ends]

This I'm going to call iothubcentralapp.

[Video description begins] In the input box for IoT Hub Name, he types iothubcentralapp. [Video description ends]

Okay, after I've done that, I'm then going to click Next.

[Video description begins] He clicks the Next: Size and scale button. The screen shifts to the tab titled Size and scale. It has a drop-down menu for Pricing and scale tier. The default value is S1: Standard tier. A scale for Number of S1 IoT Hub units is also present. The pointer is currently on 1. On the page, there is a section with the following details: Pricing and scale tier, Messages per day, and Cost per month. The Pricing and scale tier is S1, Messages per day is 400,000, and Cost per month is 25.00 USD. The following features are enabled: Device-to-cloud-messages, Message routing, Cloud-to-device-commands, and IoT Edge. At the bottom there are two buttons: Review + create, and Previous: Basics. [Video description ends]

Here I can specify the pricing and scale tier, so that we can determine how many messages can be handled by this IoT Hub. These are called IoT Hub units, so this is the scale capacity, and as we need more IoT capacity units, then we can increase them. Now we have to choose the appropriate tier before that even becomes an option.

And notice that what we're looking at here is a number of messages per day that can be processed. And what goes along with that, of course, is the cost increase or decrease, as you increase or decrease the maximum messages per day that you want to be able to process.

[Video description begins] He clicks the drop-down menu for Pricing and scale tier. A list of options appears. He selects S3: Standard tier from the list. The name of the next criterion changes from Number of S1 IoT Hub units to Number of S3 IoT Hub units. The value of Pricing and scale tier changes to S3 and Messages per day changes to 300,000,000. The Cost per month changes to 2500.00 USD. He then changes the value of Number of S3 IoT Hub units to 3. The value of Messages per day changes to 900,000,000 and Cost per month changes to 7500.00 USD. He changes the value of Number of S3 IoT Hub units to 1. The value of Messages per day changes back to 300,000,000 and Cost per month changes back to 2500.00 USD. [Video description ends]

These are messages from IoT devices. Next, I'll click Review and Create, and then I'll just click the Create button.

[Video description begins] He clicks the Review + create button. The screen shifts to the tab titled Review + create. It has information on the basics and size and scale for the IoT hub . At the bottom, there are two buttons: Create and Previous: Size and scale. He clicks the Create button. A page titled All resources appears. It has the following buttons on the top: Add, Edit columns, Refresh, etc. Below the buttons, there is an input box for Filter by name and drop-down menus for resource groups, types, location, tags, and grouping. Below these, there is a table with the following columns: Name, Type, Resource Group, Location, Subscription, and Tags. A few rows of data are present in this table. [Video description ends]

Now, I'm going to go to the All resources view here in the portal, and I'm going to filter it by IoT, since I know that's the prefix used to name my IoT Hub.

[Video description begins] In the Filter by name input box, he types iot. A resource named iothubcentralappappears in the table. [Video description ends]

And now we can see that it's listed here in the view. I'm going to click to open up its Properties blade.

[Video description begins] He clicks on iothubcentralapp in the table. A page titled iothubcentralappappears. It has a navigation pane. The main body has the following buttons: Move, Delete, and Refresh. Below the buttons, a few details are listed. These include: Resource group, Status, Location, Subscription, Hostname, etc. [Video description ends]

The first thing that we want to bear in mind is the hostname that's been assigned to our IoT Hub because we're talking about connectivity from IoT devices over the Internet to the IoT Hub defined in the cloud, which is what we're looking at here.

[Video description begins] The hostname is iothubcentralapp.azure-devices.net. [Video description ends]

Also, if we take a look further down, we can also see we have an IoT devices view.

[Video description begins] He clicks on IoT devices present in the navigation pane. A page for IoT devices appears. At the top, the following buttons are present: Add, Refresh, and Delete. Below the buttons, there is a table with the following columns: Device ID, Status, Last Activity, Last Status Update, etc. The table is currently empty. [Video description ends]

If I click on that, we don't have any devices of course yet, but we can click to add IoT devices by specifying the Device ID.

[Video description begins] He clicks the Add button. A page titled Create a device opens. It has an input box for Device ID, Primary key, and Secondary key. For Authentication type, there are three options: Symmetric key, X.509 Self-Signed, and X.509 CA Signed. Symmetric key is selected by default. A check box for Auto generate keys is also present. This box is checked. For Connect this device to an IoT hub, there are two buttons: Enable and Disable. Enable is selected by default. [Video description ends]

And whether we have a certificate or a symmetric key that's used to authenticate the device to the IoT Hub.

[Video description begins] He closes the Create a device page. The IoT devices page is open. [Video description ends]

Then we've got IoT Edge listed over here on the left-hand side, where we can add an IoT Edge device.

[Video description begins] From the navigation pane, he clicks on IoT Edge. An IoT Edge page opens. It has the following buttons on top: Add an IoT Edge device, Add an IoT Edge deployment, and Refresh. This page has two tabs: IoT Edge devices and IoT Edge deployments. The IoT Edge devices tab is currently open. It has a table with the following columns: Device ID, Runtime Response, Deployment Count, etc. [Video description ends]

[Video description begins] He clicks the Add an IoT Edge device button. A page titled Create a device opens. At the top, an information box is present, which has the following text: Find Certified for Azure IoT devices in the Device Catalog. This page has input boxes for Device ID, Primary key, and Secondary key. The Authentication type is Symmetric key. A check box for Auto generate keys is also present. This box is checked. For Connect this device to an IoT hub, there are two buttons: Enable and Disable. Enable is selected by default. [Video description ends]

Now from here we can look at the supported Azure IoT devices in the Device Catalog.

[Video description begins] He clicks the information box. A new tab opens in the browser. The url is https://catalog.azureiotsolutions.com. A page titled Find your IoT device appears. It has a search bar and a button named See all devices. A section titled Azure IoT Edge Certified is also present here. Some of the devices listed here are: Cloudian AI Box, ARTiGO A1250, OpenBlocks IoT VX2, etc. [Video description ends]

And, for example, if I'm interested in looking at power, some kind of IoT device that tracks power. I might start selecting these devices and reading about them in their support for Azure.

[Video description begins] In the search bar, he types power. A list of devices appears on the screen. He clicks on the device named PMC-5231. A page with the information of this device opens. It has three tabs: Device Specifications, Kit Specifications, and Get Started. The Kit Specifications tab is currently open. The kit is called ICP DAS- Power management IoT Kit. Below the name of the kit, there is a Request Demo button. [Video description ends]

So the idea is that we need to be able to determine which IoT devices in this particular case would support IoT Edge which allows us to write custom code modules that will actually run directly on that device.

[Video description begins] He shifts to the Microsoft Azure tab. He closes the Create a Device page. The IoT Edge page is open. [Video description ends]

And then we can add an IoT Edge deployment to push out the IoT Edge agent and custom modules, code modules that we want to do.

[Video description begins] He clicks on the Add an IoT Edge deployment button. A page titled Create Deployment appears. It has six tabs. The first tab is titled Name and Label is open. It has an input box for Name. At the bottom, there is a Next button. In the input box for Name, he types aa. [Video description ends]

Now these code modules, of course, are docker compatible containers that we want to push out. We can specify the container registry settings to point to those specific items.

[Video description begins] He clicks the Next button. The page shifts to the second tab, titled Add Modules (optional). It has input boxes for Name, Address, User Name, and Password. At the bottom there are two buttons: Previous and Next. [Video description ends]

So we have a number of things that we can do then through the specific IoT Hub.

[Video description begins] He closes the Create Deployment page. The IoT Edge page is open. [Video description ends]

It serves as a central point to manage and monitor IoT devices.


IoT Edge
Azure IoT Edge is an IoT-based solution for the Azure cloud that allows us to have custom code running on IoT devices. So this custom IoT software can run on the device and can even perform processing data functions on the device before even sending data back to the cloud, and specifically, back to an Azure IoT Hub. So the way that this works, generally speaking, is developers can build these custom modules, which really run as docker containers on Azure IoT Edge devices.

And those code modules or containers are then deployed to IoT devices. This also means that if there's a network outage because these IoT devices ideally would be on-premises elsewhere and not in the Azure cloud, if there's a network outage, they can still work with their code logic and process data, and when the network link is re-established, send that to the cloud specifically to the Azure IoT Hub for further processing and storage.

Azure IoT Edge has a number of components that work together, such as the Azure IoT Hub, which is an Azure resource that is used to centrally register IoT devices and manage and monitor them in the Azure cloud. Of course, the Azure IoT devices themselves are components that are used. These could be devices built by any manufacturer out there such as small devices that are used with sensors to determine temperature or power readings or pressure and pump readings or anything like that.

And that IoT device then could be registered with the IoT Hub and it can send data to the IoT Hub. But remember, with Azure IoT Edge, a lot of that data processing and manipulation can happen directly on customized code modules we place or push out to IoT devices before being sent into the Azure cloud. And so the next component is IoT Edge runtime. The Azure IoT Edge runtime needs to be supported on the IoT device.

And essentially, this is what allows us to push out. It's an agent that allows us to push out our code modules or docker compatible containers that have our custom code onto IoT devices. So when you configure an Azure IoT Hub, you can choose to add an IoT Edge device. And when you do, you'll see that there's a web page here where you can search for specific IoT device types that are supported to work with Azure IoT Hub as an IoT Edge device running that agent.

[Video description begins] A screenshot of Microsoft Azure IoT Device Catalog is displayed on the screen. It is titled: Find your IoT device. Below the title, there is a search box. The central pane shows the options for Azure IoT Edge Certified devices. Two devices have been listed here. [Video description ends]

So the process looks like this: We first create an IoT Hub.

[Video description begins] Screen title: IoT Edge Process. [Video description ends]

This is an Azure resource and we might even do it using the portal, let's say, after which we can then look at the connection strings in the IoT Hub to determine how to register our IoT device or devices with that IoT Hub. Next, we would add, as we saw on the previous screen an IoT Edge runtime device.

Essentially, we're pushing the agent out to that device, so it has to support this connectivity to Azure IoT Hub after it's registered. And then as developers build these custom modules or docker compatible containers, we can then create deployments. Really, it's called creating an IoT Edge deployment, where we specify the modules that we want to push out to specific IoT devices connected to IoT Edge that are running IoT Edge runtime.

IoT Solution Accelerators

[Video description begins] Topic title: IoT Solution Accelerators. Your host for this session is Dan Lachance. [Video description ends]

Microsoft Azure IoT solution accelerators are essentially pre-created IoT cloud solutions, where code is already written. However, you can customize it as you see fit. Also, it's got built-in support for IoT device monitoring. And, it also means that required cloud IoT services like IoT Hub get created automatically from this essentially prefab IoT cloud-based solution. So we could use accelerators so that we have a complete IoT cloud solution out of the box, but rarely is that the case.

Often, we will use one of these prefabricated IoT cloud solutions in Azure as a starting point, where we then go back and tweak it and make changes certainly to things like code, especially if we're going to be using IoT edge devices, where we run custom code modules directly on those IoT devices on another network. Pictured on the screen, we can see the Azure IoT solution accelerators website.

[Video description begins] A screenshot of the Azure IoT solution accelerators website is displayed on the screen. The URL of the website https://www.azureiotsolutions.com/Accelerators is highlighted. The page has two sections: Deploy a Microsoft solution accelerator and Explore partner solution accelerators. The Deploy a Microsoft solution accelerator section has the following options: Remote Monitoring, Connected Factory, Predictive Maintenance, and Device Simulation. [Video description ends]

Now, by going to this solution accelerators website, we can browse the variety of prefabricated solutions that could serve as either a complete solution, out of the box, or as we mentioned, as a starting point.

[Video description begins] Screen title: Deploying Azure IoT Solutions Accelerators. [Video description ends]

Once you've located a solutions accelerator that you're interested in from the website, you can select it. And you do that by clicking the Try Now button, at which point you're then going to be asked to specify an Azure subscription and region for the deployment. Next thing is that after it's deployed, which could take a few minutes, it might include some IoT device simulators, so that you can immediately be up and running and start working with the tool.

It might even use backend Azure Cosmos DB storage, but remember that these Azure IoT solutions accelerators are all a little bit different. In the end, one commonality is that they will often have a monitoring dashboard, if not more than one that you will use to navigate through the IoT solutions accelerator.

Azure Cognitive Services

[Video description begins] Topic title: Azure Cognitive Services. Your host for this session is Dan Lachance. [Video description ends]

Artificial intelligence, otherwise called AI, is essentially the imitation of human behavior by technology. In Azure, that could be done using Azure Cognitive Services, which provides a series of developer artificial intelligence tools and also an endpoint URL for making API calls to use artificial intelligence. AI usage is many-factored, including for speech purposes. This would allow developers to add speech-enabled features to their applications, such as voice to text.

There are APIs related to vision metadata tagging for processing of images, language analysis APIs, and intelligence search APIs. So, you get to learn about each of these API calls when you deploy cognitive services in Azure as a resource. The documentation is built in to the Properties blade of cognitive services. And bear in mind that in order for developers to make API calls to the API of their choosing, depending on the need, will require access to a key for the code to authenticate.

Deploy Azure Cognitive Services

[Video description begins] Topic Title: Deploy Azure Cognitive Services. Your host for this session is Dan Lachance. [Video description ends]

In this demonstration, I will deploy Azure Cognitive Services in the portal. This is really more of a developer thing; however, as IT administrators for Azure, we can deploy Azure Cognitive Services, which really serves as an entry point for developers to hook into APIs of their choosing.

[Video description begins] A Microsoft Azure webpage opens. It has a menu bar with a search bar and the following options: Filter, Notifications, Settings, etc. A navigation pane is present on this screen with the following options: Create a resource, Home, Dashboard, All services, and Favorites. The center pane has a section titled Azure services. Some of the options here are: Virtual machines, Storage accounts, App Services, etc. [Video description ends]

To get started here in the portal in the upper left, I'm gonna click Create a resource.

[Video description begins] He clicks the Create a resource option from the navigation pane. A page titled New opens. At the top, a search bar is present. [Video description ends]

And I'll just search for C-O-G, for cog, and I can see Cognitive Services listed, so I'll go ahead and search that.

[Video description begins] In the search bar, he types cog. The following list of options appears: Azure Search, Cognitive Services, Spatial Anchors, etc. He clicks Cognitive Services. A new page titled Cognitive Services opens. It has a drop-down menu for Select a software plan. The default value here is Cognitive Services. Below it, a Create button is present. [Video description ends]

Then, I'll choose Create.

[Video description begins] He clicks the Create button. A page titled Create appears. It has an input box for Name. There are drop-down menus for Subscription, Location, Pricing tier, and Resource group. A check box with the following text is present: I confirm I have read and understood the notice below. At the bottom of the page, a Create button is present. [Video description ends]

I need a name for this, so I'm gonna call this cogservice1.

[Video description begins] In the input box, he types cogservice1. [Video description ends]

And I'm gonna deploy this into a location that makes sense, that's nearest to me geographically where I'll be accessing it.

[Video description begins] The default value for Subscription is Pay-As-You-Go. He clicks the drop-down menu for Location. A list of options appears. These are: Central US, East Asia, Canada Central, etc. He selects Canada Central. [Video description ends]

And I'm gonna choose the only available pricing tier, and I'll deploy this into an existing storage group.

[Video description begins] He clicks the drop-down menu for Pricing tier. S0 appears in the options. He selects it. [Video description ends]

Notice I have to check off this confirmation about Microsoft using data that we send to Bing Search Services, if we talk to Bing-related APIs. So I'm going to go ahead and do that, and just go ahead and create this resource.

[Video description begins] He clicks the drop-down menu for Resource group. The following options appear: Rg1, testwebapp1, testwebapp2, etc. He selects Rg1. He checks the box for I confirm I have read and understood the notice below. [Video description ends]

[Video description begins] He clicks the Create button. The Microsoft Azure Home page appears on the screen. [Video description ends]

Once it's completed, it'll show up like any other resource does. So if I go to All resources and filter with a prefix of cog, there's cogservice1.

[Video description begins] He clicks the All resources option from the navigation pane. A new page titled All resources is displayed. The following buttons are shown below the title: Add, Edit Columns, Refresh, etc. Below the buttons, there is an input box for name. There are drop-down menus for resource groups, types, location, etc. A table is present below these input boxes. It has the following columns: Name, Type, Resource Group, etc. [Video description ends]

And if I click on and open it up, the first thing that's going to be important is the Keys in the Properties blade.

[Video description begins] In the input box for name, he types cog. A resource named cogservice1 appears in the table. He clicks on it. On the next screen, a new page titled: cogservice1 – Quick start is displayed. It has a navigation pane with the following options: Overview, Keys, Quick start, etc. Quick start is currently active. He clicks the Keys option from the navigation pane. [Video description ends]

[Video description begins] A new page titled cogservice1 – Keys appears. At the top, the following buttons are present: Regenerate Key1 and Regenerate Key2. Below it, there are input boxes for Name, Key 1, and Key 2. [Video description ends]

So you've got two Keys here, and you can regenerate one or the other. So you have two so that you can keep one in use for a while as you regenerate the other and over time, vice versa.

[Video description begins] He highlights the values in Key 1 and Key 2. [Video description ends]

So there is more security achieved by rotating keys periodically instead of sticking with the exact same keys. But when developers are making API calls, they're going to need access to one of these keys. Now, if I go to Quick start in the Properties blade.

I can also see reference documentation links for making API calls to the API of the developers choosing, whether it's the face API reference for facial recognition or whether it's translator text API references.

[Video description begins] He clicks on Quick start from the navigation pane. Three guidance steps for Quickstart are written on the screen. The first step is Grab your keys. The second step is Make an API call to this endpoint: https://canadacentral.api.cognitive.microsoft.com/ and the third step is Enjoy coding. The second step has the following built-in testing consoles: Face API reference, Translator Text API reference, Logs, etc. He hovers over a few of them. [Video description ends]

So when I click on any one of these, let's say we choose the face API reference, that's gonna open up a new web browser where we can see the regions in which that specific API is available through Cognitive Services.

[Video description begins] He clicks the Face API reference. A new tab titled Microsoft Cognitive Services opens in the browser. A navigation pane is shown with the following options with drop-down menus: Face, FaceList, LargeFaceList, etc. The Face option is expanded to show the following options: Detect, Find Similar, Group, etc. Detect is currently active. In the center pane, Face API - V1.0 heading is shown with a list of available regions. Next to it, an API definition button is present. [Video description ends]

And then we can see the API definition can then be downloaded either as a Swagger or a WADL file. The developers can use to learn about how to hook in to the API.

[Video description begins] He clicks the API definition button. It expands to show the following options: Swagger and WADL. [Video description ends]

And, as we go further down it has more of an explanation about how this works and how to formulate the request URL.


Azure Machine Learning Studio
  - You can use the Azure portal to create an Azure Machine Learning Studio workspace. 
    - The purpose of Machine Learning Studio, it being a web-based tool, is essentially data analysis, but it runs deeper than that. 
    - Always bear in mind that the source data that we use, that we're going to analyze needs to be trustworthy, and it needs to be accurate.
  - Although we do have some built-in functions that allow us to manipulate data, in other words, to transform the data
    - It's also important to note that there are sample data sets that we can use to learn how to use the Machine Learning Studio interface. 
      - Of course, we can also import our own data sets.
  - The purpose of machine learning in this context is to take vast amounts of data, and based on the factors that we define as being relevant and the training model that we apply to that data, we can then predict future trends or some kind of outcome as it might relate to financial credit risks. Or, as another example, the most likely income to be expected based on marital status, education, city, and many other potential factors, whatever happens to be in your data set that you specify as being relevant.

It might even relate to the healthcare side of things, such as heart disease prediction, given a number of factors. The process of working with a data experiment in Azure Machine Learning Studio begins with working with data. Whether it's sample data that's provided, or whether you import your own custom data. Bear in mind that there are also plenty of pre-defined templates.

There's a gallery that you can select from, that you can open up directly into Machine Learning Studio, so that you can begin working with these items and start to learn how to use the tool. The next thing to do once you've acquired data, or you decide you're going to use sample data, is to start applying functions such as maybe to transform the data into a specific manner, or maybe to exclude data that is not relevant, perhaps values that fall below a certain range.

You can also then apply learning algorithms based on what it is you're looking for in terms of an outcome. Finally, the last thing you would do is to run the data experiment and then visualize the outputs. Pictured on the screen, we can see the result of having created a Machine Learning Studio workspace object in Azure and then launching the Learning Studio.

[Video description begins] A screenshot of the Microsoft Azure Machine Learning Studio webpage is displayed on the screen. There is a side pane and a navigation pane. The navigation pane has a search bar for searching experiment items. It also has various other options, such as Saved Datasets, Data Format Conversions, Data Transformation, etc. The Data Transformation field is expanded to show the following options: Filter, Learning with Counts, Manipulation, Sample and Split, and Scale and Reduce. The Scale and Reduce field is expanded to show the following options: Clip Values, Group Data into Bins, and Normalize Data. The items in the experiment are shown in the center pane. A right pane is also present. It has two tabs: Properties and Project. Currently, the Properties tab is open. Under the option Clip Values, drop-down menus are present for the following fields: Set of thresholds, Upper threshold, Upper substitute value, etc. [Video description ends]

What you end up getting in the middle is a blank canvas. And then you can start dragging items from the left, such as data sets and you can add them into the canvas, but you can also then expand things like functions to manipulate your data. And, in this screenshot, the Data Transformation category has been opened up and the Clip Values item has been dragged over.

Now, when you drag something over and select it on the far right in the panel, you'll see details related to what's selected. In this case, for clipping peak values of a specific piece of data. Now, once you've done this, in the left hand-side of the screen in the navigator, you'll also be able to select different types of training models. And at the very bottom center, which you can't see in this screenshot, you'll have a Run Experiment button that you can use to actually run your experiment and then end up with some kind of predictive analysis as an outcome.
