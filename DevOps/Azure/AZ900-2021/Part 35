                    AZ900 Microsoft Azure Cloud Fundamentals 2021
                    Course Notes Part 35



Deploy CosmosDB (Cont)
  - There's no tags in here to assign here so just go to Review and create, and once the validation says it succeeded we will create the Cosmos DB deployment
    - After a moment we can see our deployment is complete, and we've even got a view over on the left here called Azure Cosmos DB
    - And we can see our deployed instance listed here
    - Click it, and here we can see a number of interesting items
    - Scroll down in the Overview part of the Properties blade, wecan see the region into which it was deployed
  - Scroll down, we also have this Data Explorer option, where there are buttons to create a new database or a new collection to begin working with data.
    - You can do this programatically or using command line tools or even GUI tools that you might even run on-premises
    - You would just need to make sure you have a way to access Cosmos DB
    - And we're talking really here about adding a firewall exception
    - Here in the Properties blade, go to Firewall and virtual networks, notice here that the client IP, the public IP on the Internet
    - This has been added here as being allowed in.
  - You would also need to go to Connection String because you would have to have the correct Cosmos DB host name
    - The port number to connect to, as well as the Cosmos DB user name, and either the primary or secondary password.
    - You would do that, for example, if you were using some kind of MongoDB type of GUI tool on-premises 
    - That you wanted to reach into the cloud to this instance to make changes to


Big Data Overview
  - Microsoft Azure provides numerous offerings related to working with Big Data
    - With Big Data, as the name implies, we're talking about vast datasets, large quantities of data that need to be processed and analyzed
    - Now, this has become more and more of a thing in recent years due to the Internet revolution and how much data is being produced on a daily basis
  - Well, we might be getting this data from Internet of Things or IoT devices like baby monitors or surveillance cameras
    - Big Data sources can also include financial information, financial transactions for customers in a banking institution 
    - For instance or through medical research or even through cookies
  - Cookies are preference files used on web browsers to track user preferences on websites
    - Also sometimes to track security authentication tokens used by users on websites
    - And so that can be a valuable source of data for things like marketing companies to know people's web browsing habits and their preferences
    - That could be derived from cookies.
  - All of this data needs to somehow be collected in a location that makes sense that can accommodate that amount of data, so a NoSQL database
    - Then it needs to be processed so we can draw meaningful insights from that data
    - Big Data has a number of characteristics that we need to consider
    - Such as the amount of data that needs to be transmitted over a network
    - Then stored in some kind of a storage location, whether it's a data lake or a specific single database
  - We have to think about the rate at which data is produced. How much data do we expect will be produced per day? 
    - Because when we have incoming data into our Azure solution for Big Data
    - We are paying a fee depending on how much data is coming in or going out in addition to being stored and being computed through a cluster
  - We also have to think about the wide variety of data types that we might be interested in working with 
    - Such as financial transactions or customer web surfing habits through the collection of cookie data
    - Then we have to think about the accuracy of that data
    - One of the things we can do with Big Data is transform it to a different format that would be acceptable for our processing engine
    - At the same time, we can also weed out irrelevant data


Azure SQL Data Warehouse
  - The analysis of big data involves both the storage of vast datasets along with the processing of that raw data to result in meaningful insights
    - Part of Azure SQL data warehouse is certainly the Data storage component, but we've also got Parallel processing
    - This is done by having a cluster of compute nodes that work together to analyze big data stores
  - So it can execute complex queries using what's called PolyBase
    - PolyBase differs a little bit from standard structured query language because it's designed to run against large datasets that get read from Apache Hadoop
    - And Apache Hadoop is a clustering solution designed for Big data analytics
  - Now, this gets sent to what's called the control node
    - The control node, like the name implies, controls the underlying cluster of compute nodes that actually perform the work
    - So we can send a transact SQL command to the control node
    - The control node is then responsible for allocating that to compute nodes
    - Because we've got more than one compute node, plural nodes, it means that we can run some of these tasks at the same time or in parallel
    - This is using underlying Azure storage to store, not only the data that we run the queries against 
    - But also any transformations that might result from the execution of those queries
  - When you configure Azure SQL data warehouse, one setting you will specify is the data warehouse units or DWUs
    - Which is a combination of performance factors related to things like CPU computing power, the amount of memory, and database input and output
    - All that together forms a data warehouse unit
  - The more data warehouse units you have, then the better performance you'll have when processing big datasets using the compute nodes within the cluster
    - Just like when deploying Azure SQL database, Azure SQL data warehouse also uses firewall rules to control inbound traffic
  - For example, you would have to add a rule for the appropriate IP address, or addresses, to allow inbound traffic to SQL over port 1433
    - To save on costs, you can also pause processing of data by the compute nodes
    - Therefore, you're only being billed for the storage related to Azure SQL data warehouse
    - When you have sporadic testing that might be taking place, this is an important strategy to reduce costs


Create an Azure SQL Data Warehouse
  - Azure SQL Data Warehouse is different than a standard Azure SQL database deployment
    - In that it's designed for parallel processing, so that we can quickly get results when we wanna run complex queries against large amounts of data
  - To get started with deploying an Azure SQL data warehouse here in the portal, click Create a resource in the upper left.
    - From the categories, choose Databases and then on the right choose SQL data warehouse
  - You're going to need to use an Azure SQL server instance here
    - If you don't already have one, you'll be able to create one throughout this process
    - Let's start by giving a database name here, for the example call this sqldatawarehousedb172, and put it in an existing resource group
  - For a data source we can have a Blank database, we could choose a Sample such as AdventureWorksDW
    - Or we could simply take the source from a Backup of a database
    - In this case, we can go with some sample data from AdventureWorksDW.
  - Then I've got to specify the SQL server instance here. I'm going to click on that
    - On the right, any existing SQL servers that we might want to tie this Data Warehouse to, we could
    - But in this case, we don't have any, so create a new one
    - We can call this something likr sqlsvr172, and specify the server admin for SQL server and confirm the passwords
  - We are going to deploy this SQL server instance in the usual location or region and then click Select
    - So we got the server taken care of, but the problem is we have a little notification symbol here that says
    - SQL Data Warehouse Generation 2 is not supported in this region, okay.
    - Well, that's set automatically as a default down below the performance level.
    - If we choose Generation 1, Gen1, then the error message goes away.
    - At least for the region that it selected it in
    - This ties into the fact that some specific Azure service configurations are only available in some regions
    - Now, we can also determine which data warehouse unit selection or DWU that we want
    - And as we kind of scale my system up, notice of course the price per hour, US dollars goes up 
    - The more data warehouse units or DWUs that you allocate to your data warehouse
  - Remember that a data warehouse unit, or DWU, is a collection of performance factors like CPUs and memory
    - So the best way to work with this, before you've got experience running your workloads in data warehouse 
    - Is to start at a reasonably small DWU value and then gauge the performance as you run queries against the data
    - If you need to, you can scale this up later or scale up back down
    - At this point, click Apply, and down below click Create.
    - And we can now see the deployment is in progress.
