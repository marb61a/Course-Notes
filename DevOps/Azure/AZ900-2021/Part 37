                    AZ900 Microsoft Azure Cloud Fundamentals 2021
                    Course Notes Part 37


Azure Data Lake Analytics
  - Microsoft Azure Data Lake Analytics is a managed service offering in the Azure Cloud
    - It's designed for large scale data storage. We are talking about at the petabyte level
    - Bear in mind, one petabyte equals approximately one million gigabytes
    - We're talking about potentially working with trillions of files
    - We can even take data, for example, that we might have stored in Azure storage account as blobs
  - And we can actually copy that over into an Azure Data Lake store
    - For data analysis, we have to think about the kind of work-load power that we're going to need 
    - To work against these large types of data so that we can gain insights
    - One consideration is configuring the Data Lake Analytic Unit, the DLAU
    - This is a unit of measurement that's used to determine the underlying horsepower that's going to run our jobs 
    - Where we can start to extract insights from this data.
  - For example, each analytic unit contains a number of CPU course that are allocated to process data and also a chunk of memory
    - At the time of this recording, one AU, one data lake analytic unit is two CPU cores and six gigabytes of RAM.
    - Making a change to the data lake analytic unit really depends on the type of workload you envision will be handled through Azure data lake analytics
    - This tells us then that we're talking about a large-scale parallel processing solution that uses node clusters
  - We can use the Microsoft Visual Studio IDE, the integrated development environment
    - As a way to gain access to our Azure data lake and to begin running queries
    - We can also use the Eclipse IDE. We can use the IntelliJ IDE
    - All of these different integrated developer environments allow you to write code in a variety of different languages
  - It really boils down to using whatever you are most familiar with
    - However, it's important to understand that these three IDEs are supported to hook into Microsoft Azure
    - In other words, there's an Azure toolkit that keeps getting updates for each and every one of these three items
  - These three items, these three IDEs also have plugins, even give them extended capabilities
    - Azure data lake storage then can be used to feed data into an Apache Hadoop cluster for parallel processing as part of data analysis
    - The Apache Hadoop cluster uses the Hadoop Distributed File System or HDFS
    - The jobs that we submit against that use what's called U-SQL
  - This is even a type of project that you can launch if you're using GUI IDE tools like Microsoft Visual Studio
    - U-SQL then, is just a simple language that you'll learn very quickly if you are already familiar with structured query language or just SQL


Create a Data Lake Analytics Account
  - Just like a lake in the real world can have many incoming streams or tributaries to result in the water collected in the lake
    - Azure data lake in the Azure cloud allows us to specify a multitude of data sources to allow data to be fed into data lake.
  - Not only is it data storage, but we're talking about analysis of that data
    - To get started here, go into the Azure portal and click Create a resource in the upper left.
  - Because we're talking about analytics, choose the Analytics category, you'll see over on the right that we have Data Lake Analytics, which we will click.
  - What we can do is feed data into our Azure data lake
    - Then that data can be processed and transformed and manipulated for the purposes of gaining insights as to all of that collection of raw data
    - It can even be used for things like machine learning so, we have to create a new data lake analytics account
    - In the example call this datalake172 but one matching your schema is needed
    - Notice it's going to add the .azuredatalakeanalytics.net DNS suffix at the end
  - We will deploy this into an existing resource group and choose a location that makes sense for me, and then down below
    - W've got to also create a data lake storage account
  - Click Create new Data Lake Storage Gen1, it's already got a name for it, that's fine, let's go with that
    - Leave it on Pay-as-You-Go and Encryption as enabled, so click OK for that and then click Create to actually create this resource
    - Now go to the All resources view on the left and filter it for things that start with the data.
  - We can see the two resources that resulted from our configuration, the data lake storage and the data lake analytics resource
    - Click on to pop into the properties.
    - When we're in here, notice right away that we have the option of submitting a job
    - What we're talking about doing here is, submitting a job for processing for data lake analytics
    - Of course, that could be fed data that we've configured into our data lake configuration
    - If I were to scroll down, you'll see in the properties blade here indeed we do have data sources
  - Currently, for our data lake analytic configuration, we've got our data lake account that we specified for storage upon creation
    - However notice that we could add additional data sources
    - We also have some other configuration items, like for example, the maximum number of concurrent running jobs
  - We've got a slider here to draw that up or down, depending upon what our specific needs hour, our processing might entail
    - If we were to click Tools in the properties blade, we then have a variety of tools that we can work with 
    - From a developer perspective to feed data into Data Lake Analytics
    - Then to determine which job should process that data, so there are Data Lake Tools for Visual Studio
    - As we scroll down, it's also available for Azure PowerShell, and Azure CLI


Add a Data Lake Data Source
  - You can feed data into Azure Data Lake Analytics programmatically, through command line tools and also through the GUI here in the portal
  - Here in the Azure portal, I'm already looking at my Azure data lake analytics resource which I will click on to open up its properties blade
  - Scroll down, I have an option here called Data sources where I'll see the data lake account that's already available for data lake analytics
  - Click on that, I can see some details, the name and the type. This is Azure Data Lake Storage Gen1
    - If I wish, I can also go down and start exploring the data by clicking Data explorer
    - This will be based on what we've added as data sources, as we can see
    - We can start browsing through all of the files in the file system related to that storage
  - To add additional storage scroll back up in the properties blade and choose Data sources and then click Add data source
    - In this case, I'm interested in Azure storage and what I'm going to do is specify the Select account option or I can choose an Azure storage account
  - Select an existing Azure storage account that has data that we would like to feed into Azure data lake analytics for further processing
    - Then go ahead and click Add
    - After a moment, we can see that our storage has been added and if clicked notice here it's not data lake storage
    - But rather it is just simple Azure storage, as in a storage account
    - Now that we've done that, if we scroll down for instance and go to Data explorer, now we may have to refresh this
  - Click Refresh, and close what we looking at previously 
    - Because now we can see besides my data lake storage, we've also got our storage account here, our Azure storage account example was called stor14567
  - We can even start browsing through folders in that Azure storage account to expose content for example the previously uploaded jpeg image.
    - Notice here, if we select that, we can get a preview of what is in that specific file.
    - Normally, you'll have to download it to do that, as the built in filters often will not show you anything that makes any sense
    - It really depends on the file type, but notice we can also upload content even from this interface 
    - Instead of going out to the storage account in Azure, including managing the hierarchy by creating folders and so on
  - It's important then to add the appropriate data sources to Azure data lake analytics 
    - So that you can begin to submit jobs that will process that data, and we'll see how to do that in another demo


Work with Data Lake Datasets
  - Azure Data Lake Analytics is designed to be used as a large scale centralized data storage repository where data can come from many different sources
    - It's also used for submitting jobs, so that we can process that data and gain insights from that data.
  - In the portal fo to the All resources view and by data, because we know that our example data lake analytics configuration is called datalake172
    - Go ahead and click to open that up. And what I'm interested in doing is submitting a job
    - We now have a New job button right here at the top in the overview part of the Properties blade
    - We could also scroll down under the data lake analytics section, and here I would also see New job
    - We can give a name to the job, and I'm going to go ahead and specify the code for it down below, this syntax is called U-SQL
