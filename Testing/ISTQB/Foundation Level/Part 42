                    ISTQB Certified Tester Foundation Level
                    Course Notes Part 42


Software Test Progress Monitoring and Control
  - Interpreting test metrics
    - During testing, a testing team's objective is to check whether a product meets the specified requirements. 
    - To ensure that the team successfully achieves its objectives, you need to monitor all testing activities. 
    - For effective monitoring, you collect data related to each testing activity 
      - And use it to assess whether the activities are progressing as planned. This data is known as Metrics.
  - Typically, you collect metrics data during and at the end of each test level. 
  - This data helps you easily verify whether the testing activities adhere to the allocated schedule and budget. 
  - It also helps you verify the relevance and effectiveness of the test objectives and the testing activities.
  - Some of the metrics you collect include
    - Percentage of work done
      - To determine the work percentage, you calculate the number of test cases the testers prepared
        - And the time they took to prepare the test cases. 
        - You also calculate the percentage of time and effort the testers spent preparing the test environment. 
        - These calculations help you determine whether testers are working to meet their targets and objectives.
    - Performance of test cases
      - You can also use test cases as metrics to monitor the progress of testing. 
      - To do this, you determine if each test case was performed, 
        - And you further identify the performed test cases that ran successfully. 
      - By monitoring test cases, you can validate the time spent by the testers in preparing the test cases.
  - Defect information
    - When collecting metrics, you also gather information about defects the testers identify. 
    - For example, you determine the defect density
      - This is the ratio of the number of defects testers identify to the size of the component or system they are testing.
  - Other defect-related data you collect include the number of defects that were fixed 
    - And the number of times a fixed component or system had to be retested.

Extent of testing
  - The extent to which testers test a component or system is also an important metric. 
  - For example, you check whether testers tested each code segment thoroughly 
    - Or if they verified the potential risks of a component or system alone.
  - The extent of testing varies with the test level. 
    - For example, testers test code segments during component testing and verify system requirements during system testing. 
    - By monitoring the extent of testing, you can ensure that each test level progresses as planned.
  - Confidence level of testers
    - You can also use the confidence level of testers in the quality of the product being tested as a metric. 
    - If the testers are too confident about the quality, they may not test the product thoroughly 
      - This may mead to overlooking serious defects. 
    - On the other hand, if the testers are not confident of the quality
      - they may put the product through unnecessary tests, which can only prolong the testing process. 
    - You should try to ensure that the testers adopt a balanced and bias-free approach to testing.
  - Deliverable dates and testing costs
    - You collect data related to deliverable dates and testing costs as metrics 
      - This is to determine if each test level can be completed on time and within budget. 
      - For example, if you find that a testing activity is overshooting the allocated budget
        - You can decide whether to stop the activity, modify its scope, or perform it according to the existing plan. 
        - Base the decision on the impact the testing activity has on other testing activities and overall testing progress
  - You can use various tools to record and display test metrics. 
  - For example, you can record the data in a textual format 
    - In an Institute of Electrical and Electronics Engineers (IEEE) 829 test log template 
    - Or in Microsoft Office Excel worksheets. 
    - You can also represent the data in a graphical format using bar graphs.
  - If you only want to maintain a simple record of the tests the testers perform and the defects they identify, 
    -You use an Institute of Electrical and Electronics Engineers (IEEE) 829 test log template. 
    - This template consists of three standard sections: Test log identifier, Description, and Activity and event entries.
    - In a test log you create using the template, 
      - You first add a unique number to identify the log in the Test log identifier section. 
    - Then, in the Description section, you describe the software components being tested and the test environment.
    - The last item you add to the log is information about testing activities and events. 
    - For example, you mention the testing process, the outcome of the testing process
      - As well as defects identified, and a description and location of the defect report.
  - If you want a more detailed record of the metrics, you can create test-summary sheets using Excel. 
  - For example, you can use a summary sheet that contains an overall representation of the test metrics
    - Such as bug ID and planned and actual dates, actual effort, and the test duration, in various columns.
  - In the Test ID and the Test Case rows of the summary sheet, you store the ID and name of each test case, respectively. 
  - For example, to enter the data for a functionality test, you assign the ID as 1 and the Test Case name as Functionality.
  - Under each main test case, you also add the names of its specific sub tests. 
    - For example, under Functionality, you can include data for tests conducted on menu items of a software product 
      - And also its formatting and printing capabilities. 
      - You then summarize the data related to each test case and its sub tests against the Summary entry.
  - In the Run By row of the summary sheet, you specify the initials of the person who conducted the test. 
  - Similarly, in the Test Duration row, you record the duration of each test 
    - Then you use the Plan Effort and Actual Effort rows to keep track of planned and actual testing efforts.
  - Other useful information on the summary sheet includes
    - Status and System Config
      - You store the status of each test case in the Status row 
      - And the code assigned to each type of testing environment in the System Config row. 
      - For example, you specify a status of Pass if a test case runs successfully. 
      - Similarly, you specify a status of Warn for a test case that results in a minor failure
        - The status of Fail for a test case that fails completely
        - And the status of Skip for a test case that the testers don't perform.
