                    ISTQB Certified Tester Foundation Level
                    Course Notes Part 52
                    
                    
  - A test harness is a test environment that simulates aspects of the production environment 
    - In which a software component will eventually run. 
    - The test harness contains small programs such as stubs and drivers that interact with the software being tested. 
    - Stubs and drivers are examples of unit test framework tools. 
    - They are typically skeletal programs designed specifically to call, provide input to
      - Or accept output from, the software modules being tested.
  - Test harnesses can be programmed in-house using 'XUnit' tools. 
    - For instance, you can create a .NET based test harness using an 'NUnit' tool. 
    - You can also obtain free source or commercial versions of test harnesses.
  - A test harness is best used during component or integration testing when parts of the system may be incomplete. 
  - Developers usually use test harnesses to test, identify, and localize defects during development.
  - Test harnesses and unit test framework tools enable you to
    - Provide input and receive output from the software being tested
    - Store and execute tests within the framework and record the results of each test
    - Provide support for debugging
    - Measure coverage at code level
  - Test comparators compare what the software produces to what the software is expected to produce 
    - And store the results either as part of the test case or as computed through a test oracle. 
    - For instance, suppose you are executing a test expecting a particular error to be returned during the test. 
    - The test execution tool returns an error, but it is not the one you expected. 
    - Then to catch the difference and report it, you will use a test comparator.
  - Test comparators are usually included with the test execution tools and are particularly helpful for regression testing 
  - The process of identifying differences between the actual results and the expected results for a component under test 
    - Is called test comparison. Test comparison can be performed in two ways.
  - Which are
  - Dynamic comparison
    - In dynamic comparison, you use a test execution tool to perform the comparison in real time
      - While the test is executing. 
      - This type of comparison helps identify if the compared and the actual results do not match each other during a test. 
      - If the results do not match, you can correct the defect or execute other tests. 
      - You can also monitor error messages through this method.
  - Post-execution comparison
    - In post-execution comparison, you use stand-alone and internally developed tools to perform the comparison 
      - After a test has finished executing.
    - This type of comparison is useful when comparing a large volume of data.
  - Many operating systems have file comparison tools for post-execution comparison.
  - Coverage measurement tools measure the percentage of quantifiable structural elements 
    - Or 'coverage items' that are covered by a given test suite. 
    - Some coverage tools can go further to identify elements that have not been exercised during the test
      - And even to suggest test inputs that will exercise these elements.
Depending on the programming language and the test techniques in use, coverage measurement can be intrusive or non-intrusive.

Note
Software cannot be considered 100% tested if it achieves 100% statement coverage. This only means that those elements that the coverage measurement tool succeeded in identifying have been exercised during the test.

To test an application using a coverage measurement tool, you

instrument the code
You first instrument the code by identifying the coverage items at the component test level.

You use the tools to first identify the coverage items that have undergone a structural test. For example, at the component testing level, the coverage items can be code statements or decisions. Or, at component integration level, they can be calls made to functions or modules.
test the instrumented code
You then run the instrumented code through a series of manual or automated tests.
identify the coverage items that were exercised
Next you use the coverage tool to count the number of coverage items that have been executed by the test suite. You also use it to identify and report the percentage of the exercised coverage items. If the coverage tool you are using is a sophisticated one, you can even use it to identify the test inputs that can exercise paths that include unexercised items.
remove the instrumented code
Finally, just before the code goes into production, you remove the instrumented code.
Security testing tools are used to test a system's resistance to security threats such as computer viruses, worms, or denial of service attacks.

These tools support the execution of test procedures to confirm that there are no flaws in the security systems. They test to check if security systems are secure enough to resist external attacks on the network, support software, databases, and source code. They are particularly important if the application being tested is to be used in an unsecured environment, such as e-commerce or e-business and corporate applications on the Internet.

Because the skills required to use security tools are very specialized, you should ensure that only specialists operate them.

Security testing tools can be used to

identify virus attacks
identify weak passwords
check if the system has weak points such as open ports or points of attack
check if the operations are secure
check if the system is secured against denial of service attacks and the results of test attacks
simulate different types of external attacks


2. Using performance and monitoring tools
During testing you can also use additional tools such as

dynamic analysis tools
performance testing tools
monitoring tools
Dynamic analysis tools are used to analyze defects while the software code is executing. Developers use these tools during component testing and component integration testing to detect defects that are difficult to find during static testing.

For example, time dependencies lend themselves to dynamic testing, as does memory leakage handling, or boot-up processes.

You can also use dynamic analysis tools to check websites for dead links.

Dynamic analysis tools can

identify memory leaks
identify pointer arithmetic
detect time dependencies
report the status of the software during code execution
monitor the allocation, use, or deallocation of memory
Performance testing tools test the performance of a system under different load or usage patterns. Like test execution tools, performance testing tools use test scripts and data-driven testing, where variables that point to data in a data table are used to replace hard-coded inputs in test scripts.

You can also use performance testing tools to perform

load testing
In load testing, you check whether the system can cope with a certain number of transactions, loads, or usage patterns by pushing it beyond its normal and expected usage. You increase the load on the system gradually using a test driver so you can identify the usage pattern or the load that could make the system collapse. The output of the test is written to a log. After completion, you generate reports or graphs from the contents of the log to check the performance level.
volume testing
When you are checking volume, you test whether the system is able to handle a large amount of data. Take the case of a file that has many records in it. During volume testing, you check if the system can open and display all the records in the file.
stress testing
In stress testing, you check whether the system can handle more transactions than it is known to handle. You perform both load and volume testing. When you are checking load, you push the system to go beyond its normal and expected usage. You increase the load on the system gradually so you can identify the usage pattern or the load that can make the system collapse. This will help anticipate load issues after the system is deployed and help prevent them. For instance, when an HR system supports details of only 1,000 employees, entering information about the next employee might make the system collapse.
In performance testing, you verify how a system or a software application behaves under simulated usage conditions. For instance, when testing the performance of a web site, you check if the web site can handle the amount of traffic it is supposed to handle. Performance testing is best done at the system level. Ensuring the system is active, you replicate an end-user environment or user profiles. You send a number of test inputs to the system. The parameters that you set for the inputs may vary depending on the performance testing tool you use.

The tool then measures system characteristics such as response time and mean time between failure and sends you a report. If you find that the performance does not meet standards, you analyze where the problem is and how it can be improved.

Analyzing the output of performance testing tools takes time and skill. Therefore, it is a good idea to use specialists to carry out performance testing activities. This is especially so because the cost of some of these tools is quite high. Implementation and training costs increase the investment further. Using specialists will ensure that these tools are used optimally.

Performance testing tools are used to test an integrated group of systems, such as servers, databases, or an environment. Once functional, they need not be continually monitored.

Performance testing tools can typically

discover performance issues
measure loads, average response times, and timing of specific transactions
identify memory leaks
generate graphs or charts of responses
record problems such as locking, concurrency, excessive disk space, or system resources usage
Monitoring tools monitor the status and the performance of the systems in use. They are likely to be used after deployment. For instance, they may be used to recognize increasing demands on the system or abnormal behavior that requires alerting the administrator. If you use a tool to monitor your e-business websites, it will report any unusual behavior from visitors. If a monitoring tool detects a security attack then visitors can be automatically routed to another site, and site administrators receive an alert. You can prevent adverse consequences for the business such as negative publicity or financial loss.

Monitoring tools cover all aspects of IT infrastructure such as servers, databases, networks, performance, applications, and Internet usage. Because monitoring tools are used to monitor network traffic as well as the number of users on a network, they can help identify problems and alert the concerned people. They can also be used to log real-time information and historical data.

Apart from the testing tools covered, there are some other tools that are not specifically designed for testers or developers, but can still be used to support test activities. Examples of such tools are office productivity tools, database query tools, or communication tools, and backup or restore utilities.

For instance, in the absence of a test management tools, you can use a spreadsheet to store data temporarily.

Summary
Test tools provide tool support for testing activities such as test execution and logging, and performance and monitoring.

Test execution tools, test harnesses, test comparators, coverage measurement tools, and security tools help you in executing and logging test cases. Dynamic analysis tools, performance testing tools, and monitoring tools help you in activities such as analyzing code and monitoring performance.

Back to top

Introducing a Tool into an Organization
Appropriate introduction of a test tool
When an organization or project team decides to buy a tool to complete a task, they should study the advantages and disadvantages of buying the tool.

The factors you should keep in mind before buying the tool are

maturity of the internal test processes used
availability of alternate solutions
constraints faced and requirements specified
availability of tools that meet these requirements
detailed evaluation/proof of concept
cost of the selected tool
Before buying a test tool, you should first identify the strengths and weaknesses of the test organization. You should introduce the tool only if it can support the established test processes. To evaluate whether your organization is ready for tools, you can use the Test Process Improvement (TPI) model or the Capability Maturity Model Integration (CMMI) model. These models are software process improvement assessment models that can assess the maturity of an organization and provide guidelines for improving the test process.

It is a good practice to consider alternative solutions before investing in a tool. Trying out alternative solutions can sometimes be more beneficial for a test process than using a test tool or automating an old test pack.

For example, your organization may require version management of its products. Instead of purchasing a configuration management test tool , you could look at alternative options, such as purchasing a cheaper open source tool, or using internal employees to develop this tool.

After you've ensured that there are no alternative solutions available, you should consider the constraints and requirements of the tool.

The constraints could be financial or technical. The requirements could be about factors such as training needs or tool vendors' specifications. The requirements should be drawn up formally, prioritized and approved by the key stakeholders.

If you don't perform a detailed analysis of the need for the tool and specify the requirements and constraints, you could encounter problems such as delays, unnecessary expenditure, or inadequately equipped tools.

For example, you may require a tool that can be remotely accessed. The tool you adjudged to be the best after a series of demonstrations might not be deployable over the Internet. Purchasing the tool without understanding its constraints and your own requirements would result in owning an inadequately equipped tool.

After specifying the requirements, next you should check if the available tools are capable of overcoming the constraints and satisfying the requirements. If they are, shortlist them for purchase.

After you decide to purchase a tool, you can approach tool vendors, attend tool exhibitions, and search for tools on the Internet.

When discussing with vendors, you should ensure that you provide them with a list of the constraints and the requirements so they are clear about your requirements.

After you decide to buy a tool, it is important that you evaluate both the tool vendor and the tool. This evaluation is called a proof of concept. When you've shortlisted only one tool, you can combine the pilot project, which is the first use trial project of the tool with the proof of concept, which is the detailed evaluation of the tool before purchase.

During evaluation, you should evaluate the tool in the same test environment where the tool will be used. This ensures that you don't need to reconfigure the tool. For example, some static analysis tools don't support all versions of all programming languages, so you should test a static analysis tool in the programming language version you intend to use.

After completing the evaluation, you should again assess the tools against the requirements. If there are any additional features, you can note them as potential future requirements.

After evaluating the tool, you should negotiate the cost of the tool with its vendor. During the negotiation, you should discuss the purchase price, consultancy costs, training and implementation costs, and annual license fees. You should also negotiate the costs regarding a pilot run of the tool and the costs associated with a large scale implementation of the tool.

After negotiating the costs, you should conduct a pilot project to test the tool. A pilot project helps you

gain knowledge
You can gain knowledge through a pilot project when you test the tool and identify what the benefits and shortcomings of the tool are.
assess compatibility
The pilot project helps you assess if the test tool is compatible with the existing processes.
decide on process modifications
During the pilot, you can determine what processes and practices you would need to modify to accommodate the tool.
decide on how to ensure people make optimum use of the tool
The pilot project helps you identify how people can make optimum use of the tool so existing processes can be streamlined.
evaluate the benefits of the tool
During the pilot, you can evaluate if the benefits the tool promises can be achieved at a reasonable price. This will help in deciding whether you want to purchase the tool.
determine other details for using the tool
The pilot project will also help you determine other details for using the tool. For example, you can identify the templates required and guidelines needed to use the tool.
If you're satisfied with the results of the pilot project, you should buy the tool. To implement the purchased tool, you should

adopt an incremental approach
You should adopt an incremental approach by releasing the tool to the rest of the organization in phases. After a pilot, you can release the tool into areas where it is likely to be more useful.
adapt the tool to the existing processes
You should find the right balance for using the existing tools and practices with the new tool. You can do this by adapting the tool to the existing processes, old tools, and testware.
use the test pilot data
You can use the test pilot data to create user guidelines that are to be used by the testing team.
train employees
You should train employees on using the tool. This ensures that they are prepared to use the tool in a live project.
create a database
You should compile a database of issues faced and learned based on the findings of the pilot. This ensures that the team will always have solutions to problems that have been encountered.
                    
                    
