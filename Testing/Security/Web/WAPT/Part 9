                    Web Application Penetration Testing
                    Course Notes Part 9


Module 3 : Information Gathering (Cont)
Infrastructure (Cont)
  - Cookies are also an interesting resource that may reveal useful information in this phase
    - Each technology has its default cookies names therefore, we can potentially guess it by inspecting the cookie header
      -  PHP -> PHPSESSID=XXXXX 
      - .NET -> ASPSSESSIONIDYYYY=XXXXX 
      - JAVA -> JSESSION=XXXXX 
  - There may be many different result outputs depending on the service running on the machine, the version, Operating System etc
  - Another tool that may prove to be useful is WhatWeb
    - This is a command line tool that can be used to recognize website technologies
    - Pentesting distributions such as Kali Linux already have it installed by default
    - To start using the too simply type the following in the terminal -> whatweb -h 
    - It can be added to an environment by cloning the Github repo using
      - git clone https://github.com/urbanadventurer/WhatWeb.git
    - The tool itself is very easy to use
      - You just need to type the name of the tool followed by the address (IP or URL) of our target and hit enter
      - You can specify multiple targets in the command or even IP ranges
      - There are  options that allow us to specify different user agents, HTTP basic authentication credentials, cookies, proxy and more
      - Also worth noting is that the tool automatically follows redirections
      - If you desire a more readable output, just use the -v option
  - Another very useful tool that can be used directly from our web browser is called wappalyzer 
    - It is a Web Browser plugin based tool that works both on Firefox and Chrome
    - Then you just have to navigate your target website: you will see some icons in your address bar
    - Each icon gives you information about the Web Server, such as the Operating System, The Web Server, JavaScript frameworks etc
    - In order to inspect the information found, just click on an icon and a pop up will appear on your right, listing all the information gathered
  - Similar to how we fingerprinted the web server version, we can find fingerprints of what modules are installed and in use on the server
    - Modules we are looking for are ISAPI modules (for IIS) or Apache modules that may interfere with or alter our test results
    - More and more websites use search engine and human-friendly URLs (SEF URL's)
    - Ugly URLs are the ones that carry query string parameters and values that are meaningful to the web server 
    - EG www.example.com/read_doc.php?id=100 tells the server to query the database to fetch the document with id=100.
    - A search engine-friendly version would be www.example.com/read/Buffer_Overflow.html
    - The two are related
      - When a user requests read_doc.php?id=100
      - The server side module in charge of translating the URL will use regular expressions to match a Rewrite rule 
      - This will translate the URL according to the rules specified by the administrator
    - URL rewriting is done on Apache with the mod_rewrite module or .htaccess
    - On IIS it is handled by Ionic Isapi Rewrite or Helicon Isapi Rewrite
  - The presence of URL-rewriting is easy to recognise and should be kept in mind during the testing phase when attempting input validation attack
    - This type of attacks involves the use of malformed input (among the other data input) using the URL parameter
    - Not having a real URL just the rewritten URL, will make it much more difficult for a penetration tester to try these attacks on URLs
    - It will however, still be possible to carry malformed payload using other input channels such as forms, cookies and headers
  - Search engine friendly URLs are not a security feature, input validation attacks are still possible if you can reverse-engineer the translation rules
    - However, there will be only rare cases in which the rewritten URL is easy to reverse engineer to its original form
    - Input from forms though is still intat and can be tampered with
  - Subdomain Enumeration starts by mapping all available subdomains within a domain name
    - This not only widens the attack surface available but can sometimes reveal potential avenues of attack
    - This includes hidden management backend panels or intranet web applications that admins hoped to protect by using security through obscurity
  - There are several different ways to enumerate domains
    - Netcraft, Google, Crawling, Brute Force, Tools and Zone transfers
    - Using Netcraft to enumerate Sub-domains
      - Simply open the Netcraft search page and select subdomain matches  from the dropdown menu and type in the string
      - If the target has any subdomain, we will see it listed in the results page
    - Although tools such as Netcraft are very useful in finding subdomains, search engines are sometimes an even better option
      - Google search operators can be very powerful and can be used modify the results and enumerate a list of subdomains
      - To obtain a list of a site's subdomains eg microsoft the query string will be site: .microsoft.com
      - There are some subdomains that appear more often than others which will mean deleting them from results
      - This means further modifying the query string in which we can use the minus operator (-) in conjunction with site or inurl
        - site:.microsoft.com -inurl:www or site:.microsoft.com -site:www.microsoft.com to remove the www bit
      - We can keep tweaking our search query by removing the new subdomains found
        - site:microsoft.com -site:subdomain1.microsoft.com  -site:subdomain2.microsoft.com -inurl:subdomain3.microsoft.com 
        - This process can be exhaustive
  - In addition to search engines, there are a plenty of tools that can be used to enumerate subdomains
    - Some of them parse search engine results, while some others use wordlists to verify if a specific set of domains exist
    - dnsrecon subbrute fierce Nmap and theHarvester to name a few
    - All of these are very similar and it is encouraged to experiment
    - Subbrute as the name suggests, uses a default wordlist to find the subdomains of a specific target
      - These types of tools are very useful if we cannot rely on search engines 
      - To install clone the git repo using git clone https://github.com/TheRook/subbrute.git 
      - Once cloned you can launch the tool and display its options with the following command python subbrute.py -h 
      - The tool uses by default a wordlist named names.txt
      - To test on a domain eg Microsoft use python subbrute.py microsoft.com
      - There is also the possibility to use a custom wordlist
    - Dnsrecon is already installed if using Kali Linux
      - To run simply run the following command -> dnsrecon -h
      - Similarly to subbrute, dnsrecon can leverage wordlists to enumerate subdomains and it also offers the possibility to use search engines 
      - The option we are interested into is -g: perform Google enumeration with standard enumeration
      - This is a multi-threaded process so is a lot quicker, this is set up by using the --threads option
      - Using microsoft.com once again -> dnsrecon -d microsoft.com -g
      - It first execute some general enumeration by checking the DNS configuration and then begins enumerating the domains via Google
    - TheHarvester is a tool for gathering subdomain names from different public sources such as search engines or PGP key servers
      - theHarvester is also able to retrieve data related to the target organization from many websites such as LinkedIn, People123, Twitter
      - Kali Linux already has theHarvester installed by default we can simply run it as follows -> theharvester [options]
      - Using microsoft.com to run the tools using google as the search and only 200 results in a html file
        - theharvester -d microsoft.com -b google -l 200  -f /root/Desktop/msresults.html
  - In addition to search engines and tools, there are other ways we can discover information about domains and subdomains. 
    - One of these is through Zone Transfer
    - Zone transfers are usually the result of misconfiguration of the remote DNS server
    - They should be enabled (if required) only for trusted IP addresses
    - When zone transfers are available, we can enumerate all the DNS records for that zone
    - On Windows systems, we can gather information from Zone Transfer by running the following commands
      - nslookup server [NAMESERVER FOR mydomain.com] ls â€“d mydomain.com
    - On Linux systems run dig @nameserver axfr mydomain.com 
  - A virtual host is simply a website that shares an IP address with one or more other virtual hosts
    - These hosts are domains and subdomains
    - This is very common in a shared hosting environment where a multitude of websites share the same server/IP address
