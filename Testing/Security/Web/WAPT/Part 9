                    Web Application Penetration Testing
                    Course Notes Part 9


Module 3 : Information Gathering (Cont)
Infrastructure (Cont)
  - Cookies are also an interesting resource that may reveal useful information in this phase
    - Each technology has its default cookies names therefore, we can potentially guess it by inspecting the cookie header
      -  PHP -> PHPSESSID=XXXXX 
      - .NET -> ASPSSESSIONIDYYYY=XXXXX 
      - JAVA -> JSESSION=XXXXX 
  - There may be many different result outputs depending on the service running on the machine, the version, Operating System etc
  - Another tool that may prove to be useful is WhatWeb
    - This is a command line tool that can be used to recognize website technologies
    - Pentesting distributions such as Kali Linux already have it installed by default
    - To start using the too simply type the following in the terminal -> whatweb -h 
    - It can be added to an environment by cloning the Github repo using
      - git clone https://github.com/urbanadventurer/WhatWeb.git
    - The tool itself is very easy to use
      - You just need to type the name of the tool followed by the address (IP or URL) of our target and hit enter
      - You can specify multiple targets in the command or even IP ranges
      - There are  options that allow us to specify different user agents, HTTP basic authentication credentials, cookies, proxy and more
      - Also worth noting is that the tool automatically follows redirections
      - If you desire a more readable output, just use the -v option
  - Another very useful tool that can be used directly from our web browser is called wappalyzer 
    - It is a Web Browser plugin based tool that works both on Firefox and Chrome
    - Then you just have to navigate your target website: you will see some icons in your address bar
    - Each icon gives you information about the Web Server, such as the Operating System, The Web Server, JavaScript frameworks etc
    - In order to inspect the information found, just click on an icon and a pop up will appear on your right, listing all the information gathered
  - Similar to how we fingerprinted the web server version, we can find fingerprints of what modules are installed and in use on the server
    - Modules we are looking for are ISAPI modules (for IIS) or Apache modules that may interfere with or alter our test results
    - More and more websites use search engine and human-friendly URLs (SEF URL's)
    - Ugly URLs are the ones that carry query string parameters and values that are meaningful to the web server 
    - EG www.example.com/read_doc.php?id=100 tells the server to query the database to fetch the document with id=100.
    - A search engine-friendly version would be www.example.com/read/Buffer_Overflow.html
    - The two are related
      - When a user requests read_doc.php?id=100
      - The server side module in charge of translating the URL will use regular expressions to match a Rewrite rule 
      - This will translate the URL according to the rules specified by the administrator
    - URL rewriting is done on Apache with the mod_rewrite module or .htaccess
    - On IIS it is handled by Ionic Isapi Rewrite or Helicon Isapi Rewrite
  - The presence of URL-rewriting is easy to recognise and should be kept in mind during the testing phase when attempting input validation attack
    - This type of attacks involves the use of malformed input (among the other data input) using the URL parameter
    - Not having a real URL just the rewritten URL, will make it much more difficult for a penetration tester to try these attacks on URLs
    - It will however, still be possible to carry malformed payload using other input channels such as forms, cookies and headers
  - Search engine friendly URLs are not a security feature, input validation attacks are still possible if you can reverse-engineer the translation rules
    - However, there will be only rare cases in which the rewritten URL is easy to reverse engineer to its original form
    - Input from forms though is still intat and can be tampered with
  - Subdomain Enumeration starts by mapping all available subdomains within a domain name
    - This not only widens the attack surface available but can sometimes reveal potential avenues of attack
    - This includes hidden management backend panels or intranet web applications that admins hoped to protect by using security through obscurity
  - There are several different ways to enumerate domains
    - Netcraft, Google, Crawling, Brute Force, Tools and Zone transfers
    - Using Netcraft to enumerate Sub-domains
      - Simply open the Netcraft search page and select subdomain matches  from the dropdown menu and type in the string
      - If the target has any subdomain, we will see it listed in the results page
    - Although tools such as Netcraft are very useful in finding subdomains, search engines are sometimes an even better option
      - Google search operators can be very powerful and can be used modify the results and enumerate a list of subdomains
      - To obtain a list of a site's subdomains eg microsoft the query string will be site: .microsoft.com
      - There are some subdomains that appear more often than others which will mean deleting them from results
      - This means further modifying the query string in which we can use the minus operator (-) in conjunction with site or inurl
        - site:.microsoft.com -inurl:www or site:.microsoft.com -site:www.microsoft.com to remove the www bit
      -
  - In addition to search engines, there are a plenty of tools that can be used to enumerate subdomains
    -
  -

Fingerprinting Frameworks and Applications 
  - Once a list of subdomains has been obtained then the appropriate techniques can be applied
    - Essentially we start looking at the webpages running on each of the subdomains that were found
  - Common applications are those pieces of software that are available online for anyone to use 
    - There is an acronym to describe these COTS - Common off the shelf
  - They can be either open source or commercial
    - Access to the source code is what makes them interesting for analysis
  - There may be an opportunity to read both the application logic and the security controls implemented
    - This gives a big advantage over custom in-house applications where the logic needs to be guessed to certain degree
  -

