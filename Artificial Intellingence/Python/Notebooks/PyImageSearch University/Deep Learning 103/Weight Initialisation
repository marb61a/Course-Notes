Understanding Weight Initialization for Neural Networks - Notes

The text version of the tutorial is available at the following url
  - https://www.pyimagesearch.com/2021/05/06/understanding-weight-initialization-for-neural-networks/

The tutorial covers the concept of weight initialisation or more simply, how we initialize our weight matrices and bias vectors
  - This will not be an exhaustive guide but will show some popular nethods and some good rules of thumb  

How to initialise
  - Research has shown that weight initialisation is far more important than previously thought
  - Neural Networks need to be given a good starting point
  - How weights are initialised is critical in training deep learning networks

Constant Initialization 
  - This is not a recommended approach
  - All weights in the neural network are initialized with a constant value
  - Although simple to implement the problem with using this method is that it’s near impossible for us to break the symmetry of activations 
  - This is why this approach is not used in the real world

Uniform and Normal Distributions
  - A uniform distribution draws a random value from the range [lower, upper]
  - Every value in this range has an equal probability of being drawn
  - A normal distibution is defined by the Gaussian function where mean and standard deviation are defined
  - Both distributions can be used to initialise weights in Neural Networks
  - There are various heuristics that can be imposed to create what are better initialistions

LeCun Uniform and Normal
  - This is the default initialisation method for Pytorch and Torch
  - There are 2 parameters defined, F_in or the number of inputs and F_out or the number of outputs
  - This ccan be used with both normal and uniform distributions
  - This approach is better than just using normal and uniform ditributions but there are better approaches
  
Glorot/Xavier Uniform and Normal 
  - This is the default weight initialisations for Keras and TF
  - For the normal distribution the limit value is constructed by taking F_in and F_out and adding, then getting the square root, a zero-center is then used
  - This initialisation can also be used with the uniform distribution where limit has stronger restrictions
  - Learning tends to be fairly efficient and is recommended for most non-residual NN's
  
He et al./Kaiming/MSRA Uniform and Normal 
  - This is another initialisation scheme
  - Often referred to as “He et al. initialization,” “Kaiming initialization,” or simply “MSRA initialization"
  - This technique is named after Kaiming He who is the creator of ResNet
  - This method is typically used when training very deep NN's that either have residual components or use a ReLU like activation function
  - To initialise weights in a layer using this method with a uniform distribution set the limit to be = 6/F_in
  - Normal ditributions can also be used by setting sigma to 2/F_in and mu to 0

Some recommendations
  - Never use constant initialisation
  - Use Xavier/Glorot for non-residual networks
  - Use He et al. initialization for residual networks
  - Weight initialisation should be treated as a tuneable hyperparameter
  
