There is a text version of the tutorial available at the following address
  - https://www.pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/
  - It is recommended to read and study this prior to the video tutorial
  - Again the pyimagesearch module will evolve and expand
  - The xor file is similar to the perceptron example but uses the neuralnetwork.py file instead of perceptron.py
    - from pyimagesearch.nn import NeuralNetwork
    - Using the xor example file with the neuralnetwork file should correctly classify non-linear values
    - This will give us the correct answer from the xor dataset

1988 paper, Learning representations by back-propagating errors by Rumelhart, Hinton, and Williams
https://dl.acm.org/doi/10.5555/65669.104451

Backpropogation
  - This can be almost thought of as a right of passage for AI\NN devs
  - In almost all AI classes backpropagation will be thought and students will have to implement using Python
    - This will be without libraries such as Keras, Tensorflow etc

Backpropagation is arguably the most important algorithm in neural network history
  - This is because without (efficient) backpropagation, it would be impossible to train deep learning networks to the depths that we see today
  - This is why it is considered a foundation stone of NN learning
  - There are thousands of tutorials available on backpropogation so the tutorial will be only a high view
  - This is because the focus in on the practical implementation using Python

This chapter will build an actual neural network and train it using the back propagation algorithm
  - This is so students have a stronger understanding of how this algorithm is used to train neural networks from scratch.
  
The algorithm involved in Backpropagation has 2 phases
  - The first phase is also known as the propagation phase or forward pass and is where inputs are passed through the network and output predictions obtained
  - The backward pass where we compute the gradient of the loss function at the final layer of the network
    - And use this gradient to recursively apply the chain rule to update the weights in our network
    - This is also known as the weight update phase

Forward Pass
  - The purpose of the forward pass is to propagate our inputs through the network by applying a series of dot products and activations
    - This is done until the output layer of the network is reached
    - The layers between the input layer and the output layer are called the hidden layers
      - The hidden layer is where a lot of the works is done
      - There can be more than 1 hidden layer which is seen in the architecture
  - Using a XOR dataset as an example
    - As with a lot of examples X is a data point in the set and y is the output values
    - Target output values are the class labels
    - Taking an input from a design matrix holding 2 dimensional X datapoints, our goal is to correctly predict the target output value
    - To obtain perfect classification accuracy on this problem we’ll need a feedforward neural network with at least a single hidden layer
    - The easiest way to start testing is to use a 2-2-1 architecture but we need to ensure that the bias is included
    - This can be done in 2 ways, either as a separate variable
      - Or treat the bias as a trainable parameter within the weight matrix by inserting a column of 1’s into the feature vectors
      - This inseration operation would be done programmatically
    - This column can be inserted anywhere but is typically inserted as the first or last column
      - This will change the size of our input feature vector which is normally performed inside neural network implementation to avoid modifying the design matrix
      - This has the effect of modifying the architecture for a 2-2-1 to a 3-3-1
      - It will still be called a 2-2-1  but is different due to the addition of the bias term embedded in the weight matrix
      - Fully connected layers are layers where every node is connected to every node in the next layer
    - The input layer and all hidden layers require a bias term; however, the final output layer does not require a bias
      - The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer 
      - This means that training is more efficient and substantially easier to implement

The Backward Pass 
  - To apply the backpropagation algorithm, our activation function must be differentiable
    - This is so the partial derivative of the error can be computed
      - with respect to a given weight wi, j, loss (E), node output oj, and network output netj. 
  - There will not be too much discussion of the calculus involved as there is plenty of material available

Implementing Backpropagation with Python 
  - This will be done with the neuralnetwork.py in the nn folder which is within the pyimagesearch folder
  - As usual start with imports, then the constructor which needs a single argument although a second one can be used optionally
    - layers -->  A list of integers which represents the actual architecture of the feedforward network eg 2-2-1
    - Alpha --> This is where the learning rate of our neural network is specified and applied during the weight update phase
      - As a learning exercise this should be experimented with to see what changes happen with different learning rates
    - The initialise the list of weights w and store alpha and layers
    - Then loop over the layers before stopping before the final 2
    - This is becaause there is a special case where the inputs need bias but the outputs do not
  - The next function defined is a Python magic method called __repr
    - https://www.tutorialsteacher.com/python/magic-methods-in-python
    - This is used as it aids debugging
  - Then both the sigmoid function and the sigmoid derivative function are added
    - A quick note, whenever you perform backpropagation, always try to choose an activation function that is differentiable
  - Then the fit function is defined which is where the network training is done
    - The fit method requires two parameters, followed by two optional ones
    - The first, X is our training data, the second, y is the corresponding class labels for each entry in X
    - Then specify epochs which is the number of epochs we’ll train our network for
    - The displayUpdate parameter simply controls how many N epochs we’ll print training progress to our terminal
    - This is where the bias trick is perfomed and the number of epochs
    - For each epoch, we’ll loop over each individual data point in our training set, make a prediction on the data point
    - Then compute the backpropagation phase, and update then weight matrix
  - Then there is the fit_partial method
    - This is where the actual heart of the backpropagation algorithm is found
    - This function requires two parameters, x which is an individual data point from our design matrix, y is the class label 
    - Initialise a list which is responsible for storing the output activations for each layer as our data point
    - This is the start the forward propagation phase (FEEDFORWARD) and is started by looping over every layer in the network
      - The net input to the current layer is computed by taking the dot product between the activation and the weight matrix
      - The net output of the current layer is then computed by passing the net input through the nonlinear sigmoid activation function
      - Once there is net ouput it is added to the list of activations
      - This is the entirety of the Forward Pass
    - The next stage is to begin the backward pass
      - The first phase of the backward pass is to compute our error
      - This is simply the difference between our predicted label and the ground-truth label
      - A quick note is that using -1 in python indexing indicates the last value
      - The chain rule is then applied to build a list of deltas
      - The deltas will be used to update our weight matrices, scaled by the learning rate
      - The first entry in the deltas list is the error of our output layer multiplied by the derivative of the sigmoid for the output value 
      - If we take the delta for the the final layer in the network, we can now work backward using a for loop
      - Neural Networks like this are a series of matrices
      - We can loop backwards over the needed layers to to compute the delta updates for each layer
      - This process is repeated until we reach the first layer in the network
  - Then there is the predict function
    - As the network has been trained by this point we will need to be able to make predictions
    - This is where the predict method comes in and is pretty much a glorified Forward Pass
    - This function accepts one required parameter followed by a second optional one
      - The first is X which is the data points we’ll be predicting class labels for
      - Then there is y which is a boolean indicating whether we need to add a column of 1’s to X to perform the bias trick
    - p is then initialised which is the output predictions as the input data points X
      - This will be propagated though every network layer until the final output prediction is reached
      - There is also a  check to see if the bias term should be embedded into the data points
  - The final function is the calculate_loss function
    - This function will be used to calculate the loss across our entire training set
    - The loss returned in this case is Mean Squared Error
    - There are many different loss functions and each has their place
