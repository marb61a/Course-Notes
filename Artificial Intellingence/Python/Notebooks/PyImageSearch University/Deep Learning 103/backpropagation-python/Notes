There is a text version of the tutorial available at the following address
  - https://www.pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/
  - It is recommended to read and study this prior to the video tutorial

1988 paper, Learning representations by back-propagating errors by Rumelhart, Hinton, and Williams
https://dl.acm.org/doi/10.5555/65669.104451

Backpropogation
  - This can be almost thought of as a right of passage for AI\NN devs
  - In almost all AI classes backpropagation will be thought and students will have to implement using Python
    - This will be without libraries such as Keras, Tensorflow etc

Backpropagation is arguably the most important algorithm in neural network history
  - This is because without (efficient) backpropagation, it would be impossible to train deep learning networks to the depths that we see today
  - This is why it is considered a foundation stone of NN learning
  - There are thousands of tutorials available on backpropogation so the tutorial will be only a high view
  - This is because the focus in on the practical implementation using Python

This chapter will build an actual neural network and train it using the back propagation algorithm
  - This is so students have a stronger understanding of how this algorithm is used to train neural networks from scratch.
  
The algorithm involved in Backpropagation has 2 phases
  - The first phase is also known as the propagation phase or forward pass and is where inputs are passed through the network and output predictions obtained
  - The backward pass where we compute the gradient of the loss function at the final layer of the network
    - And use this gradient to recursively apply the chain rule to update the weights in our network
    - This is also known as the weight update phase

Forward Pass
  - The purpose of the forward pass is to propagate our inputs through the network by applying a series of dot products and activations
    - This is done until the output layer of the network is reached
  - Using a XOR dataset as an example
    - As with a lot of examples X is a data point in the set and y is the output values
    - Target output values are the class labels
    - Taking an input from a design matrix holding 2 dimensional X datapoints, our goal is to correctly predict the target output value
    - To obtain perfect classification accuracy on this problem we’ll need a feedforward neural network with at least a single hidden layer
    - The easiest way to start testing is to use a 2-2-1 architecture but we need to ensure that the bias is included
    - This can be done in 2 ways, either as a separate variable
      - Or treat the bias as a trainable parameter within the weight matrix by inserting a column of 1’s into the feature vectors
      - This inseration operation would be done programmatically
    - This column can be inserted anywhere but is typically inserted as the first or last column
      - This will change the size of our input feature vector which is normally performed inside neural network implementation to avoid modifying the design matrix
      - This has the effect of modifying the architecture for a 2-2-1 to a 3-3-1
      - It will still be called a 2-2-1  but is different due to the addition of the bias term embedded in the weight matrix
    - The input layer and all hidden layers require a bias term; however, the final output layer does not require a bias
      - The benefit of applying the bias trick is that we do not need to explicitly keep track of the bias parameter any longer 
      - This means that training is more efficient and substantially easier to implement

The Backward Pass 
  - To apply the backpropagation algorithm, our activation function must be differentiable
    - This is so the partial derivative of the error can be computed
      - with respect to a given weight wi, j, loss (E), node output oj, and network output netj. 
  - There will not be too much discussion of the calculus involved as there is plenty of material available

Implementing Backpropagation with Python 
  - This will be done with the neuralnetwork.py in the nn folder which is within the pyimagesearch folder
  - 
  
