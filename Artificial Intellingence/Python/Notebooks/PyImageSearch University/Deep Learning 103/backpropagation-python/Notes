There is a text version of the tutorial available at the following address
  - https://www.pyimagesearch.com/2021/05/06/backpropagation-from-scratch-with-python/
  - It is recommended to read and study this prior to the video tutorial

1988 paper, Learning representations by back-propagating errors by Rumelhart, Hinton, and Williams
https://dl.acm.org/doi/10.5555/65669.104451

Backpropogation
  - This can be almost thought of as a right of passage for AI\NN devs
  - In almost all AI classes backpropagation will be thought and students will have to implement using Python
    - This will be without libraries such as Keras, Tensorflow etc

Backpropagation is arguably the most important algorithm in neural network history
  - This is because without (efficient) backpropagation, it would be impossible to train deep learning networks to the depths that we see today
  - This is why it is considered a foundation stone of NN learning
  - There are thousands of tutorials available on backpropogation so the tutorial will be only a high view
  - This is because the focus in on the practical implementation using Python

This chapter will build an actual neural network and train it using the back propagation algorithm
  - This is so students have a stronger understanding of how this algorithm is used to train neural networks from scratch.
  
The algorithm involved in Backpropagation has 2 phases
  - The first phase is also known as the propagation phase or forward pass and is where inputs are passed through the network and output predictions obtained
  - The backward pass where we compute the gradient of the loss function at the final layer of the network
    - And use this gradient to recursively apply the chain rule to update the weights in our network
    - This is also known as the weight update phase

Forward Pass
  - The purpose of the forward pass is to propagate our inputs through the network by applying a series of dot products and activations
  - This is done until the output layer is reached
