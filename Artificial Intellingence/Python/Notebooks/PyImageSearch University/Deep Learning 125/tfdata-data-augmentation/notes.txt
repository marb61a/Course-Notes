                    Notes.txt

This is a separate notes file rather than adding to already large code files
  - This is the 3rd and final part in a series on using the TensorFlow data API

The text version of the tutorial is available at
  - https://www.pyimagesearch.com/2021/06/28/data-augmentation-with-tf-data-and-tensorflow/
  
TensorFlow has modules that can be imported from experimental
  - This means that they are still undergoing testing and have not been added into the main application

TF.data has been shown to be much quicker than the ImageDataGenerator class
  - This runs into a problem which is that data augmentation is typically done using the ImageDataGenerator class
  - It is not possible to use ImageDataGenerator in TF.data
  - The best way is to remove ImageDataGenerator from consideration as tf.data will improve speed of training etc
  - The methods involved are going to be larger but have benefits

Data augmentation is a critical aspect of training neural networks that are to be deployed in real-world scenarios. 
  - By applying data augmentation we can increase the ability of our model to generalize and make better, more accurate predictions on data it was not trained on.

There are 2 methods that are available in TF for data augmentation
  - The Sequential class and the preprocessing module to build a series of data augmentation operations, which is similar to ImageDataGenerator
  - We can also apply tf.image functions to manually create the data augmentation routine

The source code contains 2 main files
  - load_and_visualize.py which shows how to use the 2 main tf.data augmentation methods
  - training_with_sequential_aug.py which shows how to actually train a model

load_and_visualize.py
  - preprocessing is imported from experimental (Will soon be added in to main TF module)
    - experimental maybe a little more volatile than the main modules
  - The first method loads image
    - The image is loaded then converted to a floating point number, then resized to a project standard size (156*156)
    - Class labels are parsed from the image path
    - This is all done using TF inbuilt methods, autotune will be much quicker to optimise using these functions rather than OpenCV etc
  - There are then 2 functions which show how to apply data augmentation
    - The first is using layers
      - Images are passed in to an augmentation pipeline and returned 
    - The second function augments using ops
      - This uses TF built in functions for data augmentation
  - After these functions are the cli arguments
  - The image batch size is set to 8 so that they can be easily visualised to the screen
  - After that then the data processing pipeline is built
    - Autotune is used here to ensure efficiency
  - Then the check to see if data augmentation should be performed
    - If yes then augmentation will be done using either Tensorflow or Layers
    - If using layers then the args type will be be set to layers
      - This class is sequential which can be used to construct a pipeline
      - Sequential classes are used in Neural nets etc which build layer on layer
  - Prefetch should be set with the Autotune value
    - This is not as important in the load_and_visualize script but is in the training script
    - This also completes the pipeline
  - Using TF data augmentation gives a lot more control and more granularity on augmentation
    - This uses the using_ops methods
    - There maybe a need to use OpenCV when TF methods are not available

train_with_sequential.py
  - Will use the sequential method the same as discussed above
  - Activation, Dense and Flatten are imported to be able to build a simple CNN
  - The network being built will only have a single layer
  - The cifar10 dataset is used for training
  - Batch size is 64 and 50 epochs will be run
  - On testing pipelines do not apply random operations, these can be done in the training pipeline
  - Model architecture is arbitrary and can be changed
