The text version of the tutorial is available at the following address
  - https://www.pyimagesearch.com/2021/05/05/gradient-descent-algorithms-and-variations/

Gradient Descent Algorithms and Variations
  - This tutorial is about some of the varient implementations of Gradient Descent and how it all works (A high level overview)

The easiest way to think about Gradient Descent is to use the mountain example
  - Imagine standing on top of a mountain and you must reach the deepest valley
  - The bottom must be reach in as few steps as possible
  - The first thing to note is that we are blindfoled in the mountains
  - There are no GPS devices
  - There is however a little black box called a weight matrix
  - This box monitors our steps and gives suggestions on where to go
  - It will also update itself based on our movements
  - We must get to the bottom!!!

What is Gradient Descent
  - Gradient Descent is an optimisation algorithm
  - We start by taking the loss (or cost) function which is the function which computes the value to be minimised
    - There are multiple available so choose the appropriate one for the problem
  - After this we compute the gradient of the loss
  - The final step is to take a step opposite to the gradient as it takes us to the local minimum

How does Gradient Descent power Neural Networks and Deep Learning
  - The neural network is intialised with a random set of weights
  - 
  
