Convolutional Neural Networks (CNNs) and Layer Types  --> Notes

# There is a text version of the tutorial available at the following address
# https://www.pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/

The tutorial will cover CNN's and the layer types that they are comprised of. Up to this point 
filters have been hand defined but this is not a practical solution for Deep Learning, these
filters must be learned by the networks. Neural Networks take an input feature/image as an input
and transform it through a series of hidden layers which is usually done using nonlinear activation 
functions. Each hidden layer is also made up of a set of neurons, where each neuron is fully connected
to all neurons in the previous layer. The last layer of a neural network (i.e., the “output layer”) 
is also fully connected and represents the final output classifications of the network.

Neural networks operating directly on raw pixel intensities do not scale well and their accuracy is
not that great either. We can use Convolutional Neural Networks (CNNs) that take advantage of the 
input image structure and define a network architecture in a more sensible way. CNN's are not like
other neural networks as they are arranged in a 3d volume where depth can refer to things like the
number of channels in an images or the number of filters in a layer. Neurons in subsequent layers will
only be connected to a small region of the layer before it, this is referred to as local connectivity
and it saves a large number of parameters on the network.

There are many different types of layers that can be used to build CNN's, there are some that are more
likely to occur. CONV which is convolutional, ACT\RELU where we use the same or the actual activation 
function, POOL or pooling, FC or Fully connected, BN or Batch normalization, DO or DropOut. A CNN comes
when these layers are stacked in a certain manner. These layers can be shown using a simple notation
INPUT => CONV => RELU => FC => SOFTMAX. The SOFTMAX activation layer is often omitted from the network 
diagram as it is assumed it directly follows the final FC

CONV and FC (and to a lesser extent, BN ) are the only layers that contain parameters that are learned 
during the training process. Activation and Dropout layers are not considered as proper layers in and of
themselves but are often included in network diagrams to make the architecture explicitly clear. Pooling
layers which are of equal importance such as CONV and FC are also included in network diagrams as they 
have a substantial impact on the spatial dimensions of an image as it moves through a CNN. CONV, POOL, 
RELU and FC are the most important when defining the actual network architecture. Other layers are also
critical but they are slightly behind these 4 as they define the architecture. Activation functions 
themselves are practically assumed to be part of the architecture however they are also often omitted 
from the diagram but however, the activation layers are implicitly assumed to be part of the architecture.

The CONV layer is the core building block of a Convolutional Neural Network and this layer consists of a
set of K learnable filters, these filters have a height and width and are usually square. The filters
although small extend through the full volume depth. Inputs to CNN will have the number of channels in an
image as the depth when using RGB images. The concept of convolving a small filter with a large(r) input 
volume has special meaning in Convolutional Neural Networks and more specifically the local connectivity
and the receptive field of a neuron. When working with images, it’s often impractical to connect neurons 
in the current volume to all neurons in the previous volume as there are simply too many connections and
too many weights which makes it impossible to train deep networks on images with large spatial dimensions.
Instead, when utilizing CNNs, we choose to connect each neuron to only a local region of the input volume, 
the size of this field is called the receptive field of the neuron.  

The Depth of an output volume controls the number of neurons which are the filters in the CONV layer that 
connect to a local region of the input volume. Each filter then produces an activation map which will be
activated in the presence of oriented edges or blobs or color. For a given CONV layer, the depth of the 
activation map will be K, or simply the number of filters we are learning in the current layer. The set of
filters that are looking at the same input location is called the depth column.

If we take a convolution operation which consists of a small window sliding across a large window essentially
with the windows being matrices. If we take a slide or step of pixel each time then for each step a new depth
column would be created around the local region of the image. When creating CONV layers we normally use a 
stride step size S of either S = 1 or S = 2. Smaller strides will lead to overlapping receptive fields and 
larger output volumes. Conversely, larger strides will result in less overlapping receptive fields and smaller 
output volumes

We need to “pad” the borders of an image to retain the original image size when applying a convolution and the 
same holds true fro filters inside of a CNN. Using zero-padding, we can “pad” our input along the borders such
that our output volume size matches our input volume size. The amount of padding we apply is controlled by the
parameter P. This technique becomes highly critical when looking at deep CNN architectures that apply multiple 
CONV filters on top of each other. When performing a convolution operation where output volume is smaller than
input volume then there is a need to pad the input volume so that the output volume matches the input volumes
actual dimensions eg a 5x5 input which leads to a 3x3 output but with padding becomes a 7x7 input becomes a
5x5 output which matches the 5x5 input of actual values. 

After each CONV layer in a CNN, we apply a nonlinear activation function, such as ReLU, ELU, or any of the other
Leaky ReLU variants. We typically denote activation layers as RELU in network diagrams as since ReLU activations 
are most commonly used although ACT maybe used to denote Activation, either way the fact that the is an activation
function being applied is made clear. Again activation layers are not necessarily considered as layers because an
activation immediately follows a convolution is assumed and they are sometimes omitted from network diagrams
because of this.

There are two methods to reduce the size of an input volume — CONV layers with a stride > 1 and POOL layers. It is
common to insert POOL layers in-between consecutive CONV layers in a CNN architectures. The primary function of the 
POOL layer is to progressively reduce the spatial size (i.e., width and height) of the input volume. This is done to
reduce the amount of parameters and computation in the network, overfitting can also be controlled using pooling. 
Pool layers operate on each of the depth slices of an input independently using either the max or average function. 
Max pooling is typically done in the middle of the CNN architecture to reduce spatial size, whereas average pooling 
is normally used as the final layer of the network where there is a wish to avoid using FC layers altogether. The
most common type of POOL layer is max pooling, although this trend is changing with the introduction of different
micro-architectures. 2x2 is usually used by if a CNN uses images which are larger than 200 pixels for input then a 3x3
might be used. There have been some schools of thought that POOL layers should be discarded entirely and that instead
CONV layers with a larger stride would be used to handle downsampling the spatial dimensions of the volume. This 
approach has actually worked well in places. It’s becoming increasingly more common to not use POOL layers in the 
middle of the network architecture and only use average pooling at the end of the network if FC layers are to be avoided.

Neurons in FC layers are fully connected to all activations in the previous layer, and is the standard for feedforward 
neural networks. FC layers are always placed at the end of the network. It’s common to use one or two FC layers prior to 
applying the softmax classifier.

Batch Normalization layers as the name suggests, are used to normalize the activations of a given input volume before 
passing it into the next layer in the network. Batch normalization has been shown to be extremely effective at reducing 
the number of epochs it takes to train a neural network and it has also the added benefit of helping “stabilize” training
which allows for a larger variety of learning rates and regularization strengths. Using BN does not remove the need to 
tune parameters but should make things easier by making learning rate and regularization less volatile and more 
straightforward to tune, there should also be a lower final loss and a more stable loss curve when using BN in a network.
The biggest drawback of batch normalization is that it can actually slow down the wall time it takes to train your network
by anything up to a factor of 3 due to the computation of per-batch statistics and normalization. It is recommended that
BN is used however as it can make a large difference as it can help prevent overfitting, improve accuracy significantly
in fewer epochs than a network not using BN. The Batch Normalization should be placed after RELU so that positive valued 
features can be normalised without statistically biasing them with features that would have otherwise not made it to the 
next CONV layer. This works for the vast majority of situations however there are some occasions where placing the BN prior
to activation has been the better approach.

Dropout is actually a form of regularization that aims to help prevent overfitting by increasing testing accuracy, this 
might be at the expense of training accuracy however.  For each mini-batch in our training set, dropout layers, with 
probability p, randomly disconnect inputs from the preceding layer to the next layer in the network architecture. The
reason we apply dropout is to reduce overfitting by explicitly altering the network architecture at training time. 
Randomly dropping connections ensures that no single node in the network is responsible for “activating” when presented 
with a given pattern. Instead dropout ensures there are multiple, redundant nodes that will activate when presented with 
similar inputs which helps with the ability to generalise. The most common approach to placing dropout layers is to place
them with p = 0.5 in-between FC layers of an architecture where the final FC layer is assumed to be our softmax classifier.

There are some common patterns used to architect CNN's, the most common form of CNN architecture is to stack a few CONV and
RELU layers, following them with a POOL operation. This sequence continues until the volume width and height is small, at 
which point we apply one or more FC layers. Deeper architectures are applied usually when there is lots of labelled training
data and the problem is going to be a challenging one. Stacking multiple CONV layers before applying a POOL layer allows the 
CONV layers to develop more complex features before the destructive pooling operation is performed. There are of course other
architectures that are considered as advanced approaches.
