                    Simple Decision Trees
                    Notes

Decision Trees
  - A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences
    - This includes chance event outcomes, resource costs and utility
  - It is one way to display an algorithm that only contains conditional control statements
  - Decision Trees can lack accuracy at times however there are variants which will help
  - There are 2 types of Decision Tree
    - Regression Tree for continuous target variables
    - Classification for discrete categorical target variables
  - The first part of the tree is the root node
    - Where there is a decision to be made is called a root node
    - The small tree that follows from a decision is called a sub-tree

Concepts
  - The set of possible values is called the predictor space
  - This predictor space is divided into J distinct and non-overlapping regions
  - For every observation that falls into the region Rj, we make the same prediction
    - This is simply the mean of the response values for the training values in Rj
    - The goal is to minimise RSS
    - RSS is an acronym for Residual Sum of Squares
    - This is an error measure also used in linear regression settings
  - This uses a Top-Down Greedy Approach
    - This is known as recursive binary splitting
    - Top-down because it begins at the top of the tree and then successively splits the predictor space
    - Each split is indicated via two new branches further down on the tree
    - It is considered greedy because at each step of the tree building process the best split is made at that particular step
      - Instead of looking ahead and picking a split that will lead to a better tree in some future step

Stopping Condition
  - We need to understand how to control tree growth so that we can stop when there is a need to
  - Overfitting can be a problem with trees and can be controlled with limiting tree growth
  - Some stopping criteria will include
    - Minimum Observations at internal node
      - There are minimum numbers of observations required for further split
    - Minimum Observations at leaf node
      - There is a minimum number of observations needed at each node after splitting
    - Maximum Depth
      - Where the tree has reached the maximum number of layers possible
  
Data Used
  - There are 2 files in the folder which will be the data used in the notebook

Importing Data
  - It is easy to import data into Jupyter using Pandas
  - Be careful with filenames eg example.csv might cause Windows to recognise the file as example.csv.csv
  
Missing Values Treatment using Python
  - The time taken field in the regression data set has missing values
  - There are 494 values compared to the rest which have 506 values
  - These values will be replaced by the mean of the other values
    - This can be mode, median etc depending on needs
  - This will take the name of the column as a string in square brackets and the mean() method
  - To input the value of mean as values use the fillna() method
  - This takes several parameters, the first is the formula for mean which is assigned to value
    - The inplace parameter if set to false will not store information in the dataframe
    - To be able to make changes to the csv dataset inplace will have to be set to true

Dummy Variable Creation using Python
  - Categorical Variables will need to be converted to numerical variables
    - To do this Dummy variables can be used
  - There are 2 columns in the dataset that take non numerical values --> Genre and 3D_available
  - The numbers of dummy variables will always be -1 of the number of categories in each variable
    - 3D_available has 2 values so remove 1 to make 1 needed
  - Creating dummy variables uses the get_dummies() method
    - Again this takes multiple parameters, the first is data which is the dataframe so df can be used
    - The prefix options are for naming columns which is not needed
    - The drop_first option is set to true so that n-1 is satisfied
  - This will create new columns holding the new numerical variables

Dependent\Independent Data Split (Using Python)
  - All 18 variables (Column names) will be needed in X
    - These can be either written seperately or can just select anything that is not the Collection
    - This is done using the .loc[] method and square brackets
    - When using loc[] the rows needed will have to be added
    - The : (colon) symbolises that all rows can be used
    - After rows the columns needed are added in this case all except Collection
  - The Collection value will be used in Y
  
Test & Train Split
  - Training is not done on all available data
  - Some of the data is normally used for testing purposes
  - Test_train_split will need to be imported from the sklearn library
  - In general 20% of data is used for testing
  - Splitting the data into testing and training data is known as test_train_split
  - Using random_state has advantages
    - It can be set to any value but setting to 0 ensures the same test\train split will always have the same values
    - This helps when comparing model performance
    - Stick to a single value as changing will cause issues 
  - Using X_train.shape should show about 80% of the values with the same columns

