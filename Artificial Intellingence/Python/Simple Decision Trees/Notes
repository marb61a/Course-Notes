                    Simple Decision Trees
                    Notes

Decision Trees
  - A decision tree is a decision support tool that uses a tree-like model of decisions and their possible consequences
    - This includes chance event outcomes, resource costs and utility
  - It is one way to display an algorithm that only contains conditional control statements
  - Decision Trees can lack accuracy at times however there are variants which will help
  - There are 2 types of Decision Tree
    - Regression Tree for continuous target variables
    - Classification for discrete categorical target variables
  - The first part of the tree is the root node
    - Where there is a decision to be made is called a root node
    - The small tree that follows from a decision is called a sub-tree

Concepts
  - The set of possible values is called the predictor space
  - This predictor space is divided into J distinct and non-overlapping regions
  - For every observation that falls into the region Rj, we make the same prediction
    - This is simply the mean of the response values for the training values in Rj
    - The goal is to minimise RSS
    - RSS is an acronym for Residual Sum of Squares
    - This is an error measure also used in linear regression settings
  - This uses a Top-Down Greedy Approach
    - This is known as recursive binary splitting
    - Top-down because it begins at the top of the tree and then successively splits the predictor space
    - Each split is indicated via two new branches further down on the tree
    - It is considered greedy because at each step of the tree building process the best split is made at that particular step
      - Instead of looking ahead and picking a split that will lead to a better tree in some future step

Stopping Condition
  - We need to understand how to control tree growth so that we can stop when there is a need to
  - Overfitting can be a problem with trees and can be controlled with limiting tree growth
  - Some stopping criteria will include
    - Minimum Observations at internal node
      - There are minimum numbers of observations required for further split
    - Minimum Observations at leaf node
      - There is a minimum number of observations needed at each node after splitting
    - Maximum Depth
      - Where the tree has reached the maximum number of layers possible
  
Data Used
  - There are 2 files in the folder which will be the data used in the notebook

Importing Data
  - It is easy to import data into Jupyter using Pandas
  - Be careful with filenames eg example.csv might cause Windows to recognise the file as example.csv.csv
  
Missing Values Treatment using Python
  - The time taken field in the regression data set has missing values
  - There are 494 values compared to the rest which have 506 values
  - These values will be replaced by the mean of the other values
    - This can be mode, median etc depending on needs
  - This will take the name of the column as a string in square brackets and the mean() method
  - To input the value of mean as values use the fillna() method
  - This takes several parameters, the first is the formula for mean which is assigned to value
    - The inplace parameter if set to false will not store information in the dataframe
    - To be able to make changes to the csv dataset inplace will have to be set to true

Dummy Variable Creation using Python
  - Categorical Variables will need to be converted to numerical variables
    - To do this Dummy variables can be used
  - There are 2 columns in the dataset that take non numerical values --> Genre and 3D_available
  - The numbers of dummy variables will always be -1 of the number of categories in each variable
    - 3D_available has 2 values so remove 1 to make 1 needed
  - Creating dummy variables uses the get_dummies() method
    - Again this takes multiple parameters, the first is data which is the dataframe so df can be used
    - The prefix options are for naming columns which is not needed
    - The drop_first option is set to true so that n-1 is satisfied
  - This will create new columns holding the new numerical variables

Dependent\Independent Data Split (Using Python)
  - All 18 variables (Column names) will be needed in X
    - These can be either written seperately or can just select anything that is not the Collection
    - This is done using the .loc[] method and square brackets
    - When using loc[] the rows needed will have to be added
    - The : (colon) symbolises that all rows can be used
    - After rows the columns needed are added in this case all except Collection
  - The Collection value will be used in Y
  
Test & Train Split
  - Training is not done on all available data
  - Some of the data is normally used for testing purposes
  - Test_train_split will need to be imported from the sklearn library
  - In general 20% of data is used for testing
  - Splitting the data into testing and training data is known as test_train_split
  - Using random_state has advantages
    - It can be set to any value but setting to 0 ensures the same test\train split will always have the same values
    - This helps when comparing model performance
    - Stick to a single value as changing will cause issues 
  - Using X_train.shape should show about 80% of the values with the same columns

Creating a Decision Tree (In Python)
  - Creating a model using the X and y train data
  - Tree will need to be imported from Sklearn
  - The DecisionTreeRegressor method from tree is used
    - This takes several parameters which includes limiting the maximum tree depth using max_depth
  - To fit the created tree object with the X and y train simply use the fit() method and pass them in
  - After fitting the tree then values can be predicted using the trained model
    - This is done using the predict method which takes the X values (train and test)

Evaluating Model Performance (In Python)
  - mean_squared_error and r2_score will need to be imported from sklearn.metrics
    - mean_squared_error is the mean of the squares of the deviation of the predicted values from the actual values
    - r2_score (r squared) which is the goodness of fit
    - r squared lies between 0 and 1
      - 1 stands for perfect fit which means all deviations in y can be justified
      - 0 means that there is no fit at all as no deviations using the model can be seen
    - In general r2 tends to go from 0.4 to 0.8 where 0.8 is for excellent models
    - mean_squared_error is an absolute number with no range
      - It can be used to evaluate different model performance on the same data set 
  - mean_squared_error takes 2 parameters, the y_test and y_test_pred variables
  - r squared values will always be higher on data it has trained on
  
Plotting Decision Tree (In Python)
  - This is difficult and can require a lot of effort in Python
  - The first step is to create a dot file
    - This is then exported to an image file which can be used to create a graph
      - This is done using .export_graphviz
  - Image will then have to be imported from IPython.display
  - Pydotplus will then be imported
  - Graphviz and others may not be installed
    - Use the pip Python package manager to install packages eg pip install pandas
    - On Graphviz using Windows there maybe a need to download the exe and add it to PATH
    - Users may also need to use conda install as well as pip install from the anaconda shell
  - To get a graph the pydotplus graph_from_dot_data() method will be used
  - To create an image use the create_png() method
  
Pruning A Tree
  -
  
