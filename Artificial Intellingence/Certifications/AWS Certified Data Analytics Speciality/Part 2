                    AWS Certified Data Analytics Speciality
                    Course Notes Part 2
                    
                    
Collection Domain
Introduction to Data Collection Sytems (cont)
  - Select a collection system
    - How to order data and avoid duplication
    - How to tranform and filter data
  - Determine the operational characteristics of the collection system
    - Looking at streaming operational components
      - This will be Kinesis Data Streams and how it feeds into EC2, Kinesis Data Analytics and Lambda
      - Also how these tools feed visualisation tools like quicksight
    - Then will look at handdling fault tolerance and data persistence
      - This will look at using Kinesis Producer Library and how it interacts with Kinesis Data Streams to give fault tolerance
      - How the whole cycle including the Kinesis Consumer Library can be used to giver good data persistence
      - The cycle is Kinesis Producer Library -> Kinesis Data Streams -> Kinesis Consumer Library
    - Then will look at selecting a collection system that handles the frequency volume and source of data
      - This will include batch streaming and transactional data.
        - A batch example will be
          - This will cover how Kinesis Data Firehose and how it pushes data into a batch data cycle
          - This batch data cycle will use S3 and AWS Glue to catalogue
          - This will then be entered into a data lake where it can be queried via Athena
        - A streaming example will be 
          - Using Kinesis Data Firehose to send data to a streaming data cycle
          - This will contain Kinesis Data Analytics and Kinesis Data Firehose
          - It then sends the data on to redshift and on to Quicksight
        - Transactional data streams will also be examined
          - Using a data source to send data to AWS DMS (Database Migration Service)
          - How the DMS feeds into S3 which can be catalogued using Glue
          - After this then the data can be moved into the data lake
    - Then will look at comparing the different data collection systems
      - Comparing AWS DMS, Kinesis Data Streams, Kinesis Data Firehose and AWS Glue
  - Selecting a collection system that addresses the key properties of data such as the order, the format and compression 
    - Need to handle keeping data in the order that we want it to be in
    - Look at Order and Duplication
      - Using Kinesis Producer Library to produce a stream of data elements
      - They may be received by the consumer in a different order than intended and this needs to be handled
      - The same holds true for data being duplicated
    - Look at transforming and filtering data
      - These are important exam topics
      - Using Kinesis Data Firehose together with Lambda and AWS DMS


Operational characteristics collection systems
  - This is the first sub-domain of 3
  - To pass the exam the student will need to undertand the operational characteristics of a collection system
  - This can be streaming components such as Kinesis Data Streams, Kinesis Data Analytics and Producer Library, Consumer Library
  - When taking the exam understanding the various components, very important is understanding fault tolerance and data persistence
  - Fault Tolerance and Data Persistence
    - In the Kinesis Producer library (KPL) their is the concept of retries
    - The KPL can send a group of multiple records in each request
    - If a record fails then it is put back into the KPL buffer for a retry
    - One record failure does not fail a whole set of records
    - The KPL has rate limiting which limits per-shard throughput sent from a single producer which can prevent excessive retries
      - Excessive retries can overload a shard and can cause unecessary reshards
  - Availability and Durability of Ingestion Components
    - Kinesis Data Streams (KDS) replicates data synchronously across 3 availability zones in one region
      - Kinesis Data Firehose has the same availability
    - KDS is not used for protracted date persistence
      - Do not use KDS shards to hold for long periods of time
      - By defualt KDS holds data for 24 hours but this can be extended to 7 days
    - Kinesis Data Firehose streams data directly to a data destination
      - This means that there is no storing of data in KDF which differentiates it from KDS
      - There are multiple destinations available such as S3, Redshift, Splunk and Kinesis Data Analytics 
      - KDF can also transform data using Lambda function prior to data delivery
  - Fault Tolerance of Ingestion Components
    - The Kinesis Consumer Library (KCL) processes data from the KDS
    - The KPL sends data into shards in the KDS, then the KCL retrieves records
    - The KCL uses checkpointing through DynamoDB to check which records have been read from a shard
    - If a KCL read fails then the KCL uses a checkpoint cursor to resume at the failed record
    - Some notes from the exam
      - Uses unique names for applications in the KCL because DynamoDB tables use the same 
