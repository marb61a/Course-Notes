                    Classification and Regression 
                    Notes


Classification and Regression
  - In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities
    - An example is to classify flowers into different species or even sub-sprecies
  - Classification has 2 different types
    - Binary Classification which is distinguishing between 2 classes
      - This can be thought of as the answer to a yes\no question
      - Classifying emails as either spam or not spam is an example of a binary classification problem
    - Multiclass Classification which distinguishing between more than 2 classes
  - A clear way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output
    - If there is a continuation between possible outcomes then the problem is a regression one
    - One example is prediciting annual income, values may differ slightly but there is continuity
    - This is versus something like languages where something is in a language or a different language, there is no between in this case

Generalization, Overfitting, and Underfitting
  - In supervised learning, we want to build a model on the training data and then be able to make accurate predictions 
    - This is on new unseen data that has the same characteristics as the training dataset
  - If a model is able to make accurate predictions on unseen data, we say that it is able to generalize from the training set to the test set
  - In Machine Learning models the objective is to build a model that generalises as accurately as possible
  - If the training and test sets have enough in common, then we will expect the model to be accurate on the test set
    - There are some cases though where this can go wrong
    - One example is when overly complex models are built, this means then we can always be accurate on the training set
  - Building a model that is too complex based on the amount of information in the table is called overfitting
    - Models should always be as simple as they can be
    - Overfitting occurs when models are fitted too closely to a particular data set
    - This model works well on the training set but cannot generalise
  - On the other hand choosing too simple a model is called underfitting
    - The more realistic our model the better predictions on the training data can be
    - Overly complex models will focus too much on individual data points and generalisation suffers
  - There is a kind of sweet spot in between the too fitting errors that yields the best gerealisation performance
    - That model is the model to try and find
 
Relation of Model Complexity to Dataset Size
  - Model complexity is linked with the variation of inputs contained in a training dataset
  - The more complex models can be formed appropriately in the case of supervised learning tasks having more data
  - Datasets of fixed size are preferable but not realistic
    - In the real world there maybe an opportunity to limit dataset size which may be of more benefit that adjusting a model


k-Nearest Neighbor (kNN) Algorithm
  - The k-Nearest Neighbor algorithm is an example of instance-based learning where training set records are first stored
  - The classification of a new unclassified record is performed by comparing it to records in the training set it is most similar to 
  - k-Nearest Neighbor is used for classification and it is also applicable to estimation and prediction tasks (Regression)
  - The Euclidean Distance function is commonly-used to measure distance 
    - https://en.wikipedia.org/wiki/Euclidean_distance
    - Where x = x1,x2,x3 y = y1, y2, . . . , ym represent the m attribute values of two records
  - The k-NN algorithm is arguably the simplest machine learning algorithm
  - Building the Machine Learning model consists only of storing the training dataset
  - To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset, its nearest neighbors
  - k-Neighbours Classification
    - In the simplest version the k-NN algorithm considers exactly 1 nearest neighbour
    - This is the closest training data point to the point we want to make a prediction for
    - The prediction is then simply the known output for this training point

k-Nearest Neighbor (kNN) Classification
  - Instead of considering only the closest neighbor, we can also consider an arbitrary number, k, of neighbors
  - When more than 1 neighbour is being considered, voting is used to assign a label
  - This means that for each test point there is a count of how many neighbours belong to each class
  - The class that is more frequent is then assigned as it is the majority class among kNN
  - For 2 dimensional datasets illustrate all possible test point in the xy plane
  - The decision boundary is the divide where different classes are assigned
  - Considering more and more neighbors leads to a smoother decision boundary
    - A smoother boundary corresponds to a simpler model
      - Real-world plots are rarely very smooth
    - Few neighbours corresponds to high model complexity, many neighbours to low model complexity
  - There is an extreme case which could be taken
    - The number of neighbors is the number of all data points in the training set
    - Each test point would have exactly the same neighbors (all training points) and all predictions would be the same
    - This would be the class that is the most frequent in the training set

k-neighbors Regression
  -
  
