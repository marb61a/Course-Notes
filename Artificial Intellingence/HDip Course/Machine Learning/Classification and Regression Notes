                    Classification and Regression 
                    Notes

[![green-pi](https://img.shields.io/badge/Rendered%20with-Green%20Pi-00d571?style=flat-square)](https://github.com/nschloe/green-pi?activate&inlineMath=$)

Classification and Regression
  - In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities
    - An example is to classify flowers into different species or even sub-sprecies
  - Classification has 2 different types
    - Binary Classification which is distinguishing between 2 classes
      - This can be thought of as the answer to a yes\no question
      - Classifying emails as either spam or not spam is an example of a binary classification problem
    - Multiclass Classification which distinguishing between more than 2 classes
  - A clear way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output
    - If there is a continuation between possible outcomes then the problem is a regression one
    - One example is prediciting annual income, values may differ slightly but there is continuity
    - This is versus something like languages where something is in a language or a different language, there is no between in this case

Generalization, Overfitting, and Underfitting
  - In supervised learning, we want to build a model on the training data and then be able to make accurate predictions 
    - This is on new unseen data that has the same characteristics as the training dataset
  - If a model is able to make accurate predictions on unseen data, we say that it is able to generalize from the training set to the test set
  - In Machine Learning models the objective is to build a model that generalises as accurately as possible
  - If the training and test sets have enough in common, then we will expect the model to be accurate on the test set
    - There are some cases though where this can go wrong
    - One example is when overly complex models are built, this means then we can always be accurate on the training set
  - Building a model that is too complex based on the amount of information in the table is called overfitting
    - Models should always be as simple as they can be
    - Overfitting occurs when models are fitted too closely to a particular data set
    - This model works well on the training set but cannot generalise
  - On the other hand choosing too simple a model is called underfitting
    - The more realistic our model the better predictions on the training data can be
    - Overly complex models will focus too much on individual data points and generalisation suffers
  - There is a kind of sweet spot in between the too fitting errors that yields the best gerealisation performance
    - That model is the model to try and find
 
Relation of Model Complexity to Dataset Size
  - Model complexity is linked with the variation of inputs contained in a training dataset
  - The more complex models can be formed appropriately in the case of supervised learning tasks having more data
  - Datasets of fixed size are preferable but not realistic
    - In the real world there maybe an opportunity to limit dataset size which may be of more benefit that adjusting a model


k-Nearest Neighbor (kNN) Algorithm
  - The k-Nearest Neighbor algorithm is an example of instance-based learning where training set records are first stored
  - The classification of a new unclassified record is performed by comparing it to records in the training set it is most similar to 
  - k-Nearest Neighbor is used for classification and it is also applicable to estimation and prediction tasks (Regression)
  - The Euclidean Distance function is commonly-used to measure distance where x = x1,x2,x3
  $$
  𝑑_𝐸𝑢𝑐𝑙𝑖𝑑𝑒𝑎𝑛 (𝑥,𝑦)=√(∑_𝑖▒〖(𝑥_𝑖−𝑦_𝑖)〗^2 )
  $$
  - 
  
