                                          TECHNICAL SEO ARTICLE NOTES FROM MOZ.COM

February 20th, 2017 - Strategic SEO Decisions to Make Before Website Design and Build
Imagine a scenario: a client asks what they should do to improve their organic rankings. After a diligent tech audit, market analysis,
and a conversion funnel review, you have to deliver some tough recommendations:
“You have to redesign your site architecture,” or “You have to migrate your site altogether,” or even
“You have to rethink your business model, because currently you are not providing any significant value.”

This can happen when SEO is only seriously considered after the site and business are up and running.
This is a list of SEO-affecting areas that will be hard to change after the website is built.

Wider strategic questions that should be answered:
1. How do we communicate our mission statement online?
 - After you identify your classic marketing ‘value proposition,’ next comes working out how you communicate it online.
 - Are terms describing the customer problem/your solution being searched for? Your value proposition might not have many searches; 
   in this case, you need to create a brand association with the problem-solving for specific customer needs. 
 - How competitive are these terms? You may find that space is too competitive and you will need to look into alternative or long-tail 
   variations of your offering.

2. Do we understand our customer segments?
 - How large is our market? Is the potential audience growing or shrinking? (A tool to assist you: Google Trends.)
 - What are our key personas — their demographics, motivations, roles, and needs?
 - How do they behave online and offline? What are their touch points beyond the site? 

3. Who are our digital competitors?
First, you want to identify who fall under three main types of competitors:
 - You search competitors: those who rank for the product/service you offer. They will compete for the same keywords as those you are 
   targeting, but may cater to a completely different intent.
 - Your business competitors: those that are currently solving the customer problem you aim to solve.
 - Cross-industry competitors: those that solve your customer problem indirectly.
Develop a list of competitors, analyze where each stands and how much operational resource it will take to get where they are:
What are our competitors’ size and performance?
How do they differentiate themselves?
How strong is their brand?
What does their link profile look like?
Are they doing anything different/interesting with their site architecture?
Tools to assist you: Open Site Explorer, Majestic SEO, and Ahrefs for competitor link analysis, and SEM rush 

Technical areas to consider in order to avoid future migration/rebuild
1. HTTP or HTTPS
Decide on whether you want to use HTTPS or HTTP. In most instances, the answer will be the former, considering that this is also one of
the ranking factors by Google. The rule of thumb is that if you ever plan on accepting payments on your site, you need HTTPS on those
pages at a minimum.

2. Decide on a canonical version of your URLs
Duplicate content issues may arise when Google can access the same piece of content via multiple URLs. Without one clear version, 
pages will compete with one another unnecessarily. In developer’s eyes, a page is unique if it has a unique ID in the website’s 
database, while for search engines the URL is a unique identifier. A developer should be reminded that each piece of content should be 
accessed via only one URL.

3. Site speed
Developers are under pressure to deliver code on time and might neglect areas affecting page speed. Communicate the importance of page 
speed from the start and put in some time in the brief to optimize the site’s performance 

4. Languages and locations
If you are planning on targeting users from different countries, you need to decide whether your site would be multi-lingual, 
multi-regional, or both. Localized keyword research, hreflang considerations, and duplicate content are all issues better addressed 
before the site build.
Using separate country-level domains gives an advantage of being able to target a country or language more closely. This approach is, 
however, reliant upon you having the resources to build and maintain infrastructure, write unique content, and promote each domain.
If you plan to go down the route of multiple language/country combinations on a single site, typically the best approach is subfolders 
(e.g. example.com/uk, example.com/de). Subfolders can run from one platform/CMS, which means that development setup/maintenance is 
significantly lower.

5. Ease of editing and flexibility in a platform
Google tends to update their recommendations and requirements all the time. Your platform needs to be flexible enough to make quick 
changes at scale on your site.

Design areas to consider in order to avoid future redesign
1. Architecture and internal linking
An effective information architecture is critical if you want search engines to be able to find your content and serve it to users. 
If crawlers cannot access the content, they cannot rank it well. From a human point of view, information architecture is important so
that users can easily find what they are looking for.
Where possible, you should look to create a flat site structure that will keep pages no deeper than 4 clicks from the homepage. That 
allows search engines and users to find content in as few clicks as possible.
Use keyword and competitor research to guide which pages you should have. However, the way pages should be grouped and connected should
be user-focused. See how users map out relationships between your content using a card sorting technique — you don’t have to have 
website mockup or even products in order to do that. 

2. Content-first design
Consider what types of content you will host. Will it be large guides/whitepapers, or a video library? Your content strategy needs to 
be mapped out at this point to understand what formats you will use and hence what kind of functionality this will require. Knowing 
what content type you will producing will help with designing page types and create a more consistent user interface.

3. Machine readability (Flash, JS, iFrame) and structured data
Your web pages might use a variety of technologies such as Javascript, Flash, and Ajax that can be hard for crawlers to understand. 
Although they may be necessary to provide a better user experience, you need to be aware of the issues these technologies can cause. 
In order to improve your site’s machine readability, mark up your pages with structured data.

4. Responsive design
As we see more variation in devices and their requirements, along with shifting behavior patterns of mobile device use, ‘mobile’ is
becoming less of a separate channel and instead is becoming an underlying technology for accessing the web. Therefore, the long-term
goal should be to create a seamless and consistent user experience across all devices. In the interest of this goal, responsive design
and dynamic serving methods can assist with creating device-specific experiences.

Closing thoughts
As a business owner/someone responsible for launching a site, you have a lot on your plate. It is probably not the best use of your 
time to go down the rabbit hole, reading about how to implement structured data and whether JSON-LD is better than Microdata. This post
gives you important areas that you should keep in mind and address with those you are delegating them to.
	
	
					  
					  
February 9th, 2017 - A Guide to JSON-LD for Beginners
What is JSON-LD?
JSON-LD stands for JavaScript Object Notation for Linked Data, which consists of multi-dimensional arrays (think: list of 
attribute-value pairs). It is an implementation format for structuring data analogous to Microdata and RDFa. Typically, in terms of
SEO, JSON-LD is implemented leveraging the Schema.org vocabulary, a joint effort by Google, Bing, Yahoo!, and Yandex in 2011 to create
a unified structured data vocabulary for the web. (However, Bing and other search engines have not officially stated their support of 
JSON-LD implementations of Schema.org.) JSON-LD is considered to be simpler to implement, due to the ability to simply paste the markup
within the HTML document, versus having to wrap the markup around HTML elements (as one would do with Microdata).

What does JSON-LD do?
JSON-LD annotates elements on a page, structuring the data, which can then be used by search engines to disambiguate elements and 
establish facts surrounding entities, which is then associated with creating a more organized, better web overall.

Where in the HTML (for a webpage) does JSON-LD live?
Google recommends adding JSON-LD to the <head> section of the HTML document; however, it’s okay if the JSON-LD is within the <body> 
section. Google can also grasp dynamically generated tags in the DOM.

JSON-LD breakdown
The immutable tags (Think: You don’t need to memorize these, just copy/paste)
 - <script type="application/ld+json"> {
When you see JSON-LD, the first think you should always see is a <script> tag. The <script> tag with a type attribute says, 
“Hey browser, I’m calling the JavaScript that contains JSON-LD.”
Note: If your JSON-LD isn’t in the curly braces, it isn’t being parsed (i.e., curl it up).

"@context": "http://schema.org",
The second element that retains a permanent place in JSON-LD markup is the @context with the value of http://schema.org. The @context
says, “Hey browser, this is the vocabulary I’m referencing. You can find it at http://schema.org.” The benefit for an SEO is that we 
get to use any of the item types and item properties that Schema.org defines.

"@Type": "…",
The final element in the JSON-LD Schema copy/paste squad is the @type specification (after the colon, it becomes all data annotation).
@type specifies the item type being marked up. You can find a list of all item types at: https://schema.org/docs/full.html.

@type for nesting: When you use a nested item type, you’re going to need to nest another @type (this is particularly important to 
understanding product and breadcrumb markups).

Attribute-value pairs
The next step is to annotate information about the item type. You can find item properties within the item type’s Schema.org page.

In terms of the syntax of JSON-LD, there are two important elements for each item property:
Item Property – This comes from the Schema.org vocabulary and should always be in double straight quotation marks (the curly and single 
quotation marks are different and will interfere with validation), and must belong to the properties allowed within the item type 
(as specified within Schema.org).

Value – You insert your value here for the property. It’s vital the value aligns with the property and is singular (i.e., each value
must be annotated separately. In the situation of multiple values for an item property, use square brackets). Strings (characters) and 
URLs need the "double straight quotation marks." Numbers, integers, floats, or doubles (for the programming inclined) alone don’t need
quotation marks, but it’s also okay to put them into quotations (this just means they’ll be considered a string data type).

Square brackets
Square brackets exist for situations where there are multiple values for an item property. A common use is leveraging the sameAs item
property as using [square brackets] for listing multiple social media platforms.

Nesting
Nesting is defined as where information is organized in layers, or where objects contain other objects. The image of a nesting doll is 
a common analogy, where large dolls contain smaller dolls within them, as a relational data organization visual. Nest is a vital aspect
for accurately marking up Schema.org JSON-LD, because you’re going to have certain item properties that belong to item types that do 
not belong to others. For example, below we can see the item property "name" can refer to the event name, the name of the performer, 
and the name of the venue. The name of the performer and venue are both nested.

Match the correct name item properties to the appropriate item type:
Nesting in JSON-LD starts with the item property. Within the first item type (ex. Movie) you must first use the item property (ex. 
actor, director, image). That item property is identified and then we must open curly brackets with the new item type (as indicated by 
the "@type":) and attribute/value data.

JSON-LD nesting checklist:
 - Must use the item property (specific to the item type)
 - The value lives in curly braces
 - You MUST identify the item type of that property
 - Attribute/value properties must be included (typically there are requirements for what needs to be included)
 - No comma before the closing curly bracket
 - Comma after closing curly bracket if there are more item properties (if not, it’ll be followed by a curly brace)

Pitfalls
If your markup isn’t validating in Google’s Structured Data Testing Tool and you’re unsure of what’s going on, check this list. Below 
are some major pitfalls in creating JSON-LD structured data markup.

Syntax
“” are not the same as "" (curly versus straight; the struggle is real)

Mind your commas
Especially note the Structured Data Testing Tool’s little red “x” on the left-hand rail. Oftentimes the “x” will appear below a missing 
or extraneous comma

Vocabulary
Pay attention to required/allowed properties within each Schema.org’s item type specification page
Always check in Google’s Structured Data Testing Tool

Policy Violation
All annotated information must be on the page; adding information that is not on the page will likely not show in search results and is 
against Google guidelines
It is also against the rules to engage in manipulative practices (not a pitfall I’m worried about for you!)
Check/review Google’s Structured Data Policies
Microsoft (sorry Bill, I’m still a huge fan!)
Copy/paste from Word/Excel can create issues (added quotation marks, added style formatting)
Microsoft switches "" to “”
Solution: use an HTML editor
Process of adding JSON-LD to site

The process of creating JSON-LD structured data markup is dependent on one’s comfort with the Schema.org vocabulary and the JSON-LD 
syntax. Below outlines a process for a person newer to JSON-LD and Schema.org to create markups, while developing a deeper understanding
of the vocabulary.
Mentally answer:
What do you want to mark up?
Goal: Determine that you can mark up the content with the Schema.org vocabulary. Some things may make sense conceptually, but are not
available within the vocabulary.

Why do you want to mark it up?
Goal: Determine whether there is a business case, or perhaps you’re looking to experiment. You don’t want to mark content up just for
the sake of marking them up; you want to mark up content that will help search engines understand the most vital information on your 
page and maximize your ability to demonstrate that you are the best resource for users.
Look for resources on markups Google is supporting, how they are using them, and examples.
If you’re using a markup that Google is explicitly using (i.e., resources on Google), open the specific documentation page and any
relevant examples
Don’t feel like you have to create JSON-LD markup from scratch. Use Google’s examples to reverse-engineer your markups. (This isn’t to 
take away from your understanding of JSON-LD and the Schema.org vocabulary; however, no need to reinvent the wheel! #efficiency ☺).
Open up the Schema.org item type page
Especially when you’re starting off with Schema.org, skimming the Schema.org technical documentation page to get a gist of what the item
type entails, how many sites are using this markup, and its various properties can facilitate a better understanding as you continue 
along your structured data journey. After a while, this step might become necessary only when attempting a new markup or looking for a
corner case.
Copy/paste the immutable elements (i.e., from <script to "@type":)
Occasionally in Google’s examples they’ll leave out the <script> tags, but please note that they are vital for the content within the 
HTML document. JavaScript can’t be parsed without <script> tags.
Add desired item type you’re interested in marking up as the value of @type:
List item properties and values
This step doesn’t require syntax and is more of a mental organization exercise. Concentrate on what you want to markup — don’t worry 
about the nitty-gritty yet. Basically, you want to get out your thoughts before you start diving into the "how."
Add JSON-LD syntax, nesting where required/appropriate
The nitty-gritty step where you put everything into the syntax, nest it, and put markup together.
Test with the Structured Data Testing Tool
Confirm that the structured data is validating and that all item properties are listed and accurate.
Determine strategy for adding to the webpage
You can simply paste one markup into the <head> section or if a more dynamic/templated solution is applicable. 
Consider whether reference IDs can help supplement your markup 
					  
					  

January 10th 2017 - How to Find and Fix 14 Technical SEO Problems That Can Be Damaging Your Site Now
Definition of technical SEO is a bit fuzzy, defined here as aspects of a site comprising more technical problems that the average 
marketer wouldn’t identify and take a bit of experience to uncover. Technical SEO problems are generally, but not always, site-wide 
problems rather than specific page issues. Their fixes can help improve your site as a whole, rather than just pages.
This is not a complete technical SEO audit checklist, but a summary of some of the most common and damaging technical SEO problems 
that can be fixed now. 
1 - Check indexation immediately
Can you get organic traffic to your site if it doesn’t show up in Google search? No.
What to do: Type site:{yoursitename.com} into Google search and you’ll immediately see how many pages on your site are ranking.
What to ask 
  - Is that approximately the amount of pages that we’d expect to be indexing?
  - Are we seeing pages in the index that we don’t want?
  - Are we missing pages in the index that we want to rank?
What to do next:
  - Go deeper and check different buckets of pages on your site, such as product pages and blog posts
  - Check subdomains to make sure they’re indexing (or not)
  - Check old versions of your site to see if they're mistakenly being indexed instead of redirected
  - Look out for spam in case your site was hacked, going deep into the search result to look for anything uncommon 
Figure out exactly what’s causing indexing problems.

2 - Robots.txt
Perhaps the single most damaging character in all of SEO is a simple “/” improperly placed in the robots.txt file.
Entire sites can be blocked because of this one problem
What to do: Go to yoursitename.com/robots.txt and make sure it doesn’t show “User-agent: * Disallow: /”.
What to do next:
  - If you see “Disallow: /”, immediately talk to the developer. There may be a good reason it’s set up that way or an oversight.
  - If you have a complex robots.txt file, review it line-by-line with your developer to make sure it’s correct.

3 - Meta robots NOINDEX
NOINDEX can be even more damaging than a misconfigured robots.txt at times. A mistakenly configured robots.txt won’t pull your pages 
out of Google’s index if they’re already there, but a NOINDEX directive will remove all pages with this configuration.
Most commonly, the NOINDEX is set up when a website is in its development phase. A good developer will make sure this is removed 
from your live site, but you must verify that’s the case.
What to do: Manually do a spot-check by viewing the source code of your page, and looking for one of these:
  - 90% of the time you’ll want it to be either “INDEX, FOLLOW” or nothing at all. If you see one of the above, you need to take action.
  - It’s best to use a tool like Screaming Frog to scan all the pages on your site at once
What to do next:
  - If your site is regularly updated and improved by your development team, set a reminder to check this 
  - Schedule site audits with an SEO auditor software tool, like the Moz Pro Site Crawl

4 - One version per URL: URL Canonicalization
The average user doesn't really care if your home page shows up as all of these separately:
  - www.example.com
  - example.com
  - www.example.com/home.html
  - example.com/home.html
But the search engines do, and this configuration can dilute link equity and make your work harder.
Google will usually decide which version to index, but could index a mixed assortment of the versions, causing confusion and complexity.
Moz’s canonicalization guide sums it up perfectly: “For SEOs, canonicalization refers to individual web pages that can be loaded from 
multiple URLs. This is a problem because when multiple pages have the same content but different URLs, links that are intended to go to 
the same page get split up among multiple URLs. This means that the popularity of the pages gets split up.”
It’s likely that no one but an SEO would flag this as something to fix, but it can be an easy fix that has a huge impact on your site.
What to do:
 - Manually enter in multiple versions of your home page in the browser to see if they all resolve to the same URL look also for 
 - HTTP vs HTTPS versions of your URLs — only one should exist, if they don’t, you’ll want to work with your developer to set up 301 
   redirects to fix this. Use the “site:” operator in Google search to find out which versions of your pages are actually indexing
What to do next:
 - Scan your whole site at once with a scalable tool like Screaming Frog to find all pages faster
 - Set up a schedule to monitor your URL canonicalization on a weekly or monthly basis

5 - Rel=canonical
Although the rel=canonical tag is closely related with the canonicalization mentioned above, it should be noted differently because 
it’s used for more than resolving the same version of a slightly different URL. It’s also useful for preventing page duplication when
you have similar content across different pages — often an issue with ecommerce sites and managing categories and filters.
The best example of using this properly is how Shopify’s platform uses rel=canonical URLs to manage their product URLs as they 
relate to categories. When a product is a part of multiple categories, there are as many URLs as there are categories that product is a
part of. For example, Boll & Branch is on the Shopify platform, and on their Cable Knit Blanket product page we see that from the 
navigation menu, the user is taken to https://www.bollandbranch.com/collections/baby-blankets/products/cable-knit-baby-blanket.
But looking at the rel=canonical, we see it’s configured to point to the main URL:
	<link  href="https://www.bollandbranch.com/products/cable-knit-baby-blanket" />
Every ecommerce and CMS platform comes with a different default setting on implementing the rel=canonical tag, so check it out.
What to do:
 - Spot-check important pages to see if they're using the rel=canonical tag
 - Use a site scanning software to list out all the URLs on your site and determine if there are duplicate page problems that can be 
   solved with a rel=canonical tag
Read more on the different use cases for canonical tags and when best to use them

6 - Text in images
Text in images — Many sites are hiding important content behind images.Yes, Google can somewhat understand text on images, but the best 
practice for SEO is to keep important text not embedded in an image. CognitiveSEO ran a test on Google’s ability to extract text from
images, and there's evidence of some stunning accuracy from Google’s technology. The conclusion from CognitiveSEO is that “this search
was proof that the search engine does not, in fact, extract text from images to use it in its search queries. At least not as a general
rule.” Although H1 tags are not as important as they once were, it’s still an on-site SEO best practice to prominently display. This is 
actually most important for large sites with many, many pages such as massive ecommerce sites. It’s most important for these sites 
because they can rank their product or category pages with just a simple keyword-targeted main headline and a string of text.
What to do:
 - Manually inspect the most important pages on your site, checking if you’re hiding important text in your images
 - Use an SEO site crawler to scan all the site pages. Look for whether H1 and H2 tags are being found on pages across your site. Also 
   look for the word count as an indication.
What to do next:
 - Create a guide so that people know the best practice in your organization is to not hide text behind images
 - Collaborate with your design and development team to get the same design look that you had with text embedded in images, but using 
   CSS instead for image overlays

7 - Broken backlinks
If not properly overseen by a professional SEO, a website migration or relaunch project can spew out countless broken backlinks from 
other websites. This is a golden opportunity for recovering link equity. Some of the top pages on your site may have become 404 pages 
after a migration, so the backlinks pointing back to these 404 pages are effectively broken.
Two types of tools are great for finding broken backlinks — Google Search Console, and a backlink checker such as Moz or Majestic
In Search Console, you’ll want to review your top 404 errors and it will prioritize the top errors by broken backlinks:
What to do:
 - After identifying your top pages with backlinks that are dead, 301 redirect these to the best pages
 - Look for broken links because the linking site typed in your URL wrong or messed up the link code on their end.
What to do next:
 - Use other tools such as Mention or Google Alerts to keep an eye on unlinked mentions that you can reach out to for an extra link
 - Set up a recurring site crawl or manual check to look out for new broken links

8 - HTTPS is less optional
What was once only necessary for ecommerce sites is now becoming more of a necessity for all sites. Google just recently announced that 
they would start marking any non-HTTPS site as non-secure if the site accepts passwords or credit cards, Google plans to label all HTTP 
URLs as non-secure. Going further, it's possible to imagine that Google will start giving HTTPS site more of a ranking benefit over HTTP.
Plausable that not secure site warnings will start showing up for sites directly in the search results, before a user clicks through to
the site. Google currently displays this for hacked sites, This goes beyond just SEO, as this overlaps heavily with web development, IT,
and conversion rate optimization.
What to do:
 - If your site currently has HTTPS deployed, run your site through Screaming Frog to see how the pages are resolving
 - Ensure that all pages are resolving to the HTTPS version of the site (same as URL canonicalization mentioned earlier)
What to do next:
 - If your site is not on HTTPS, start mapping out the transition, as Google has made it clear how important it is to them
 - Properly manage a transition to HTTPS by enlisting an SEO migration strategy so as not to lose rankings
 
9. 301 & 302 redirects
Redirects are an amazing tool in an SEO’s arsenal for managing and controlling dead pages, for consolidating multiple pages, and for 
making website migrations work without a hitch. 301 redirects are permanent and 302 redirects are temporary. The best practice is to
always use 301 redirects when permanently redirecting a page. 301 redirects can be confusing for those new to SEO trying to properly 
use them:
 - Should you use them for all 404 errors? (Not always.)
 - Should you use them instead of the rel=canonical tag? (Sometimes, not always.)
 - Should you redirect all the old URLs from your previous site to the home page? (Almost never, it’s a terrible idea.)
It’s vitally important to have someone on your team who really understands how to properly strategize the usage and implementation of
301 redirects across your whole site. Despite some statements released recently about 302 redirects being as efficient at passing
authority as 301s, it’s not advised to do so. Recent studies have tested this and shown that 301s are the gold standard.
What to do:
 - Do a full review of all the URLs on your site and look at a high level
 - If using 302 redirects incorrectly for permanent redirects, change these to 301 redirects
 - Don’t go redirect-crazy on all 404 errors — use them for pages receiving links or traffic only to minimize your redirects list
What to do next:
 - If using 302 redirects, discuss with your development team why your site is using them
 - Build out a guide for your organization on the importance of using 301s over 302s
 - Review the redirects implementation from your last major site redesign or migration; there are often tons of errors
 - Never redirect all the pages from an old site to the home page unless there’s a really good reason
 - Include redirect checking in your monthly or weekly site scan process

10 - Meta refresh
The meta refresh is a client-side (as opposed to server-side) redirect and is not recommended by Google or professional SEOs, fairly 
simple one to check — either you have it or you don’t, and by and large there’s no debate that you shouldn’t be using these.
Google advises that instead of using those kinds of redirects, try to have your server do a normal 301 redirect. Search engines might 
recognize the JavaScript or meta refresh-type redirects, but  a clear 301 redirect is always much better.
What to do:
 - Manually spot-check individual pages using the Redirect Path Checker Chrome Extension
 - Check at scale with Screaming Frog or another site crawler
What to do next:
 - Communicate to your developers the importance of using 301 redirects as a standard and never using meta refreshes unless there’s a 
   really good reason
 - Schedule a monthly check to monitor redirect type usage
 
11. XML sitemaps
XML sitemaps help Google and other search engine spiders crawl and understand your site. Most often they have the biggest impact for 
large and complex sites that need to give extra direction to the crawlers. A sitemap can improve the crawling of your site, particularly
if your site meets one of the following criteria: 
- Your site is really large. 
- Your site has a large archive of content pages that are isolated or well not linked to each other. 
- Your site is new and has few external links to it.”
Problems with XML sitemaps include - 
 - Not creating it in the first place
 - Not including the location of the sitemap in the robots.txt
 - Allowing multiple versions of the sitemap to exist
 - Allowing old versions of the sitemap to exist
 - Not keeping Search Console updated with the freshest copy
 - Not using sitemap indexes for large sites
What to do:
 - Use the above list to review that you’re not violating any of these problems
 - Check the number of URLs submitted and indexed from your sitemap to get an idea of the quality of your sitemap and URLs
What to do next:
 - Monitor indexation of URLs submitted in XML sitemap frequently from within Search Console
 - If your site grows more complex, investigate ways to use XML sitemaps and sitemap indexes to your advantage, as Google limits each 
   sitemap to 10MB and 50,000 URLs
 
12 - Unnatural word count & page size
Be careful of high page word counts, items like including accidentally terms and conditions and then not displaying them using CSS.This 
can slow down the load speed of your page and could possibly trigger some penalty issues if seen as intentional cloaking.In addition to 
there can be other code bloat on the page, such as inline Javascript and CSS. Don’t rely on the developers to be proactive in 
identifying these types of issues.
What to do:
 - Scan your site and compare calculated word count and page size with what you expect
 - Review the source code of your pages and recommend areas to reduce bloat
 - Ensure that there’s no hidden text that can trip algorithmic penalties
What to do next:
 - There could be a good reason for hidden text in the source code from a developer’s perspective, but it can cause speed and other SEO
   issues if not fixed.
 - Review page size and word count across all URLs on your site periodically to keep tabs on any issues
 
13. Speed
Speed is key — and definitely falls under the purview of technical SEO. Google has clearly stated that speed is a small part of the 
algorithm: “Like us, our users place a lot of value in speed — that's why we've decided to take site speed into account in our search 
rankings. We use a variety of sources to determine the speed of a site relative to other sites.” Even with this clear SEO directive, 
and obvious UX and CRO benefits, speed is at the bottom of the priority list for many site managers. With mobile search clearly
cemented as just as important as desktop search, speed is even more important and can no longer be ignored.
What to do:
 - Audit your site speed and page speed using SEO auditing tools
 - Unless you’re operating a smaller site, work closely with your developer on this one and make your site as fast as possible.
 - Continuously push for resources to focus on site speed across your organization.

14 - Internal linking structure
Your internal linking structure can have a huge impact on your site’s crawlability from search spiders. Where does it fall on your list 
of priorities? It depends. If you’re optimizing a massive site with isolated pages that don’t fall within a clean site architecture a 
few clicks from the home page, you’ll need to put a lot of effort into it. If you’re managing a simple site on a standard platform like
WordPress, it’s not going to be at the top of your list.
You want to think about these things when building out your internal linking plan:
	- Scalable internal linking with plugins
	- Using optimized anchor text without over-optimizing
	- How internal linking relates to your main site navigation
Even with a rock-solid site architecture, putting a focus on internal links can push some sites higher up the search rankings.
What to do:
 - Test out manually how you can move around your site by clicking on in-content, editorial-type links on your blog posts, product pages, 
   and important site pages. Note where you see opportunity.
 - Use site auditor tools to find and organize the pages on your site by internal link count. Are your most important pages receiving 
   sufficient internal links?
What to do next:
 - Even if you build out the perfect site architecture, there’s more opportunity for internal link flow, so always keep internal linking 
   in mind when producing new pages Train content people on the importance of internal linking and how to implement links effectively. 



October 25th, 2016 The Technical SEO Renaissance: The Whys and Hows of SEO’s Forgotten Role in the Mechanics of the Web
Web technologies and their adoption are advancing at a frenetic pace. Meanwhile, technical SEO is more complicated and more important 
than ever before and SEO is going through a renaissance wherein the technical components are coming back and we need to be prepared. 

Changes in web technology are causing a technical renaissance
It's important that we understand the new shiny things so we can be more effective in optimizing them.
Search engines have had the technology to crawl the web the same way we see it in a browser for at least 10 years.

At some point, some SEO figured out that random() was always returning 0.5. I'm not sure if anyone figured out that JavaScript always
saw the date as sometime in the Summer of 2006, but I presume that has changed. I hope they now set the random seed and the date using
a keyed cryptographic hash of all of the loaded javascript and page text, so it's deterministic but very difficult to game. (You can 
make the date determistic for a month and dates of different pages jump forward at different times by adding an HMAC of page content 
(mod number of seconds in a month) to the current time, rounding down that time to a month boundary, and then subtracting back the 
value you added earlier. This prevents excessive index churn from switching all dates at once, and yet gives each page a unique date.)
JavaScript is obviously here to stay. Most of the web is using it to render content in some form or another. This means there’s 
potential for search quality to plummet over time if Google couldn't make sense of what content is on pages rendered with JavaScript.

Facebook’s contribution to the JavaScript MVW frameworks, React, is being adopted for the very similar speed and benefits 
of flexibility in the development process. Regarding SEO, the key difference between Angular and React is that, from the beginning, 
React had a renderToString function built in which allows the content to render properly from the server side. This makes the question 
of indexation of React pages rather trivial. AngularJS 1.x, on the other hand, has birthed an SEO best practice wherein you pre-render
pages using headless browser-driven snapshot appliance such as Prerender.io, Brombone, etc. This is somewhat ironic, as it's Google’s 
own product.

View Source is dead
As a result of the adoption of these JavaScript frameworks, using View Source to examine the code of a website is an obsolete practice.
What you’re seeing in View Source is not the computed Document Object Model (DOM). Rather, you’re seeing the code before it's processed
by the browser. The lack of understanding around why you might need to view a page’s code differently is another instance where having
a more detailed understanding of the technical components of how the web works is more effective. Depending on how the page is coded, 
you may see variables in the place of actual content, or you may not see the completed DOM tree that's there once the page has loaded 
completely. This is the fundamental reason why, as soon as an SEO hears that there’s JavaScript on the page, the recommendation is to
make sure all content is visible without JavaScript. If instead you look at the code in the Elements section of Chrome DevTools or 
Inspect Element in other browsers, you’ll find the fully executed DOM. You’ll see the variables are now filled in with copy. The URL 
for the rel-canonical is on the page, as is the meta description. Since search engines are crawling this way, you may be missing out on 
the complete story of what's going on if you default to just using View Source to examine the code of the site.

HTTP/2 is on the way
One of Google’s largest points of emphasis is page speed. An understanding of how networking impacts page speed is definitely a
must-have to be an effective SEO. Before HTTP/2 was announced, the HyperText Transfer Protocol specification had not been updated in a
very long time. In fact, we’ve been using HTTP/1.1 since 1999. HTTP/2 is a large departure from HTTP/1.1. Quickly though, one of the 
biggest differences is that HTTP/2 will make use of one TCP (Transmission Control Protocol) connection per origin and “multiplex” the
stream. If you’ve ever taken a look at the issues that Google PageSpeed Insights highlights, you’ll notice that one of the primary 
things that always comes up is limiting the number of HTTP requests/ This is what multiplexing helps eliminate; HTTP/2 opens up one 
connection to each server, pushing assets across it at the same time, often making determinations of required resources based on the 
initial resource. With browsers requiring Transport Layer Security (TLS) to leverage HTTP/2, it’s very likely that Google will make
some sort of push in the near future to get websites to adopt it. After all, speed and security have been common threads throughout 
everything in the past five years. As of late, more hosting providers have been highlighting the fact that they are making HTTP/2 
available, which is probably why there’s been a significant jump in its usage this year. The beauty of HTTP/2 is that most browsers 
already support it and you don’t have to do much to enable it unless your site is not secure.

SEO tools are lagging behind search engines
SEO tools have always lagged behind the capabilities of search engines. That’s to be expected, though, because SEO tools are built by 
smaller teams and the most important things must be prioritized. A lack of technical understanding may lead to you believe the 
information from the tools you use when they are inaccurate.It's possible that you've performed an audit of a site and found it
difficult to determine why a page has fallen out of the index. It could be because a developer was following Google’s documentation and 
specifying a directive in an HTTP header, but your SEO tool did not surface it. In fact, it’s generally better to set these at the HTTP
header level than to add bytes to your download time by filling up every page’s <head> with them. Google is crawling headless, despite
the computational expense, because they recognize that a lot of the web is being transformed by JavaScript. Recently, Screaming Frog
made the shift to render the entire page using JS. I do recognize the fact that it would be considerably more expensive for
all SEO tools to make this shift because cloud server usage is time-based and it takes significantly more time to render a page in a
browser than to just download the main HTML file. How much time?A ton more time, actually. I just wrote a simple script that just loads
the HTML using both cURL and HorsemanJS. cURL took an average of 5.25 milliseconds to download the HTML of the Yahoo homepage. 
HorsemanJS, on the other hand, took an average of 25,839.25 milliseconds or roughly 26 seconds to render the page. It’s the differenc
e between crawling 686,000 URLs an hour and 138. Ideally, SEO tools would extract the technologies in use on the site or perform some 
sort of DIFF operation on a few pages and then offer the option to crawl headless if it’s deemed worthwhile. Finally, Google’s specs on
mobile also say that you can use client-side redirects. I’m not aware of a tool that tracks this. Now, I’m not saying leveraging 
JavaScript redirects for mobile is the way you should do it. Rather that Google allows it, so we should be able to inspect it easily.
Luckily, until SEO tools catch up, Chrome DevTools does handle a lot of these things. For instance, the HTTP Request and Response 
headers section will show you x-robots, hreflang, and rel-canonical HTTP headers. You can also use DevTools’ GeoLocation Emulator to 
get view the web as though you are in a different location. 

What truly are rankings in 2016?
Average rankings actually make a lot more sense than what we look at in standard ranking tools. SEO tools pull rankings based on a 
situation that doesn't actually exist in the real world. The machines that scrape Google are meant to be clean and otherwise agnostic 
unless you explicitly specify a location.These tools look to understand how rankings would look to users searching for the first time
with no context or history with Google. Ranking software emulates a user who is logging onto the web for the first time ever and the 
first thing they think to do is search for “4ft fishing rod.” Then they continually search for a series of other related and/or
unrelated queries without ever actually clicking on a result ,they collect data that is not necessarily reflective of what real users 
see.  Rankings tools that allow you to track mobile rankings usually let you define one context or they will simply specify 
“mobile phone” as an option. Cindy Krum’s research indicates that SERP features and rankings will be different based on the combination
of user agent, phone make and model, browser, and even the content on their phone. Rankings tools also ignore the user’s reality of 
choice. There are simply so many elements that comprise the SERP, that #1 is simply NOT #1. In some cases, #1 is the 8th choice on the 
page and far below the fold. With AdWords having a 4th ad slot, organic being pushed far below the fold, and users not being sure of
the difference between organic and paid, being #1 in organic doesn’t mean what it used to.

What is cloaking in 2016?
Cloaking is officially defined as showing search engines something different from the user. What does that mean when Google allows
adaptive and responsive sites and crawls both headless and text-based? What does that mean when Googlebot respects 304 response codes?
Under adaptive and responsive models, it's often the case that more or less content is shown for different contexts. This is rare for 
responsive, as it's meant to reposition and size content by definition, but some implementations may instead reduce content components 
to make the viewing context work. In the case when a site responds to screen resolution by changing what content is shown and more 
content is shown beyond the resolution that Googlebot renders, how do they distinguish that from cloaking? Similarly, the 304 response 
code is way to indicate to the client that the content has not been modified since the last time it visited; therefore, there's no 
reason to download it again. Googlebot adheres to this response code to keep from being a bandwidth hog. So what’s to stop a webmaster 
from getting one version of the page indexed, changing it, and then returning a 304? I don’t know that there are definitive answers to 
those questions at this point. However, based on what I’m seeing in the wild, these have proven to be opportunities for technical SEOs
that are still dedicated to testing and learning.

Crawling
Accessibility of content as a fundamental component that SEOs must examine has not changed. What has changed is the type of analytical 
effort that needs to go into it. It’s been established that Google’s crawling capabilities have improved dramatically and people like 
Eric Wu have done a great job of surfacing the granular detail of those capabilities with experiments like JSCrawlability.com
Similarly, I wanted to try an experiment to see how Googlebot behaves once it loads a page. Using LuckyOrange, I attempted to capture a 
video of Googlebot once it gets to the page. I installed the LuckyOrange script on a page that hadn’t been indexed yet and set it up so
that it only only fires if the user agent contains “googlebot.” Once I was set up, I then invoked Fetch and Render from Search Console. 
I’d hoped to see mouse scrolling or an attempt at a form fill. Instead, the cursor never moved and Googlebot was only on the page for a 
few seconds. Later on, I saw another hit from Googlebot to that URL and then the page appeared in the index shortly thereafter. There 
was no record of the second visit in LuckyOrange. While I’d like to do more extensive testing on a bigger site to validate this finding,
my hypothesis from this anecdotal experience is that Googlebot will come to the site and make a determination of whether a page/site 
needs to be crawled using the headless crawler. Based on that, they’ll come back to the site using the right crawler for the job.
The moral of the story, however, is that what Google sees, how often they see it, and so on are still primary questions that we need to
answer as SEOs. While it’s not sexy, log file analysis is an absolutely necessary exercise, especially for large-site SEO projects — 
perhaps now more than ever, due to the complexities of sites. There are any number of log file analysis tools out there.
It follows conventionally held SEO wisdom that Googlebot crawls based on the pages that have the highest quality and/or quantity of
links pointing to them. In layering the the number of social shares, links, and Googlebot visits for our latest clients, we’re finding 
that there's more correlation between social shares and crawl activity than links. 

How log files help you understand AngularJS
Like any other web page or application, every request results in a record in the logs. But depending on how the server is setup, there 
are a ton of lessons that can come out of it with regard to AngularJS setups, especially if you’re pre-rendering using one of the 
snapshot technologies.For one of our clients, we found that oftentimes when the snapshot system needed to refresh its cache, it took too 
long and timed out. Googlebot understands these as 5XX errors. This behavior leads to those pages falling out of the index, and over
time we saw pages jump back and forth between ranking very highly and disappearing altogether, or another page on the site taking its 
place.
Additionally, we found that there were many instances wherein Googlebot was being misidentified as a human user. In turn, Googlebot was
served the AngularJS live page rather than the HTML snapshot. However, despite the fact that Googlebot was not seeing the HTML snapshots
for these pages, these pages were still making it into the index and ranking just fine. So we ended up working with the client on a test
to remove the snapshot system on sections of the site, and organic search traffic actually improved.
This is directly in line with what Google is saying in their deprecation announcement of the AJAX Crawling scheme. They are able to 
access content that is rendered using JavaScript and will index anything that is shown at load.
That's not to say that HTML snapshot systems are not worth using. The Googlebot behavior for pre-rendered pages is that they tend to be 
crawled more quickly and more frequently. My best guess is that this is due to the crawl being less computationally expensive for them
to execute. All in all, I’d say using HTML snapshots is still the best practice, but definitely not the only way for Google see these 
types of sites.
According to Google, you shouldn’t serve snapshots just for them, but for the speed enhancements that the user gets as well.

In general, websites shouldn't pre-render pages only for Google — we expect that you might pre-render pages for performance benefits for
users and that you would follow progressive enhancement guidelines. If you pre-render pages, make sure that the content served to
Googlebot matches the user's experience, both how it looks and how it interacts. Serving Googlebot different content than a normal user
would see is considered cloaking, and would be against our Webmaster Guidelines.
These are highly technical decisions that have a direct influence on organic search visibility. From my experience in interviewing SEOs 
to join our team at iPullRank over the last year, very few of them understand these concepts or are capable of diagnosing issues with
HTML snapshots. These issues are now commonplace and will only continue to grow as these technologies continue to be adopted.
However, if we’re to serve snapshots to the user too, it begs the question: Why would we use the framework in the first place? 
Naturally, tech stack decisions are ones that are beyond the scope of just SEO, but you might consider a framework that doesn’t require
such an appliance, like MeteorJS.

Alternatively, if you definitely want to stick with Angular, consider Angular 2, which supports the new Angular Universal. Angular 
Universal serves “isomorphic” JavaScript, which is another way to say that it pre-renders its content on the server side.
Before all of the crazy frameworks reared their confusing heads, Google has had one line of thought about emerging technologies — and 
that is “progressive enhancement.” With many new IoT devices on the horizon, we should be building websites to serve content for the 
lowest common denominator of functionality and save the bells and whistles for the devices that can render them.
If you're starting from scratch, a good approach is to build your site's structure and navigation using only HTML. Then, once you have
the site's pages, links, and content in place, you can spice up the appearance and interface with AJAX. Googlebot will be happy looking
at the HTML, while users with modern browsers can enjoy your AJAX bonuses.

Scraping is the fundamental flawed core of SEO analysis
Scraping is fundamental to everything that our SEO tools do. cURL is a library for making and handling HTTP requests. Most popular 
programming languages have bindings for the library and, as such, most SEO tools leverage the library or something similar to download 
web pages. Think of cURL as working similar to downloading a single file from an FTP; in terms of web pages, it doesn’t mean that the
page can be viewed in its entirety, because you’re not downloading all of the required files.This is a fundamental flaw of most SEO 
software for the very same reason View Source is not a valuable way to view a page’s code anymore. Because there are a number of 
JavaScript and/or CSS transformations that happen at load, and Google is crawling with headless browsers, you need to look at the 
Inspect (element) view of the code to get a sense of what Google can actually see. This is where headless browsing comes into play.
One of the more popular headless browsing libraries is PhantomJS. Many tools outside of the SEO world are written using this library 
for browser automation. Netflix even has one for scraping and taking screenshots called Sketchy. PhantomJS is built from a rendering 
engine called QtWebkit, which is to say it’s forked from the same code that Safari (and Chrome before Google forked it into Blink) is 
based on. While PhantomJS is missing the features of the latest browsers, it has enough features to support most things we need for SEO 
analysis. As you can see from the GitHub repository, HTML snapshot software such as Prerender.io is written using this library as well.
PhantomJS has a series of wrapper libraries that make it quite easy to use in a variety of different languages. For those of you 
interested in using it with NodeJS, check out HorsemanJS. For those of you that are more familiar with PHP, check out PHP PhantomJS.
A more recent and better qualified addition to the headless browser party is Headless Chromium. As you might have guessed, this is a 
headless version of the Chrome browser. If I were a betting man, I’d say what we’re looking at here is a some sort of toned-down fork of 
Googlebot. This is probably something that SEO companies should consider when rethinking their own crawling infrastructure.

Using in-browser scraping to do what your tools can’t
Although many SEO tools cannot examine the fully rendered DOM, that doesn’t mean that you, as an an individual SEO, have to miss out. 
Even without leveraging a headless browser, Chrome can be turned into a scraping machine with just a little bit of JavaScript. I’ve 
talked about this at length in my “How to Scrape Every Single Page on the Web” post. Using a little bit of jQuery, you can effectively 
select and print anything from a page to the JavaScript Console and then export it to a file in whatever structure you prefer. Scraping 
this way allows you to skip a lot of the coding that's required to make sites believe you’re a real user, like authentication and 
cookie management that has to happen on the server side. Of course, this way of scraping is good for one-offs rather than building 
software around. ArtooJS is a bookmarklet made to support in-browser scraping and automating scraping across a series of pages and 
saving the results to a file as JSON. A more fully featured solution for this is the Chrome Extension, WebScraper.io. It requires no
code and makes the whole process point-and-click.

How to approach content and linking from the technical context
Much of what SEO has been doing for the past few years has devolved into the creation of more content for more links. I don’t know that 
adding anything to the discussion around how to scale content or build more links is of value at this point, but I suspect there are 
some opportunities for existing links and content that are not top-of-mind for many people.

Google Looks at Entities First
Googlers announced recently that they look at entities first when reviewing a query. An entity is Google’s representation of proper 
nouns in their system to distinguish persons, places, and things, and inform their understanding of natural language. At this point in 
the talk, I ask people to put their hands up if they have an entity strategy. I’ve given the talk a dozen times at this point and there 
have only been two people to raise their hands. Bill Slawski is the foremost thought leader on this topic, you shoul read:
How Google May Perform Entity Recognition
SEO and the New Search Results
Entity Associations With Websites And Related Entities
I would also encourage you to use a natural language processing tool like AlchemyAPI or MonkeyLearn. Better still, use Google’s own
Natural Language Processing API to extract entities. The difference between your standard keyword research and entity strategies is 
that your entity strategy needs to be built from your existing content. So in identifying entities, you’ll want to do your keyword 
research first and then run those landing pages through an entity extraction tool to see how they line up. You’ll also want to run your
competitor landing pages through those same entity extraction APIs to identify what entities are being targeted for those keywords.

TF*IDF
Term Frequency/Inverse Document Frequency or TF*IDF is a natural language processing technique, topic modeling algorithms have been the
subject of much-heated debates in the SEO community in the past. The issue of concern is that topic modeling tools have the tendency to
push us back towards the Dark Ages of keyword density, rather than considering the idea of creating content that has utility for users.
However, in many European countries they swear by TF*IDF (or WDF*IDF — Within Document Frequency/Inverse Document Frequency) as a key 
technique that drives up organic visibility even without links. In Searchmetrics’ 2014 study of ranking factors they 
found that while TF*IDF specifically actually had a negative correlation with visibility, relevant and proof terms have strong positive 
correlations.Based on their examination of these factors, Searchmetrics made the call to drop TF*IDF from their analysis altogether in 
2015 in favor of the proof terms and relevant terms. Year over year the positive correlation holds for those types of terms, albeit not 
as high. In Moz’s own 2015 ranking factors, we find that LDA and TF*IDF related items remain in the highest on-page content factors.In 
effect, no matter what model you look at, the general idea is to use related keywords in your copy in order to rank better for your 
primary target keyword, because it works.Now, I can’t say we’ve examined the tactic in isolation, but I can say that the pages that 
we’ve optimized using TF*IDF have seen bigger jumps in rankings than those without it. While we leverage OnPage.org’s TF*IDF tool, 
we don’t follow it using hard and fast numerical rules. Instead, we allow the related keywords to influence ideation and then use them
as they make sense. At the very least, this order of technical optimization of content needs to revisited. 

302s vs 301s — seriously?
As of late, a reexamination of the 301 vs. 302 redirect has come back up in the SEO echo chamber. Once upon a time, we worked with a 
large media organization. As is par for the course, their tech team was resistant to implementing much of our recommendations. Yet 
they had millions of links both internally and externally pointing to URLs that returned 302 response codes. After many meetings, and a 
more compelling business case, the one substantial thing that we were able to convince them to do was switch those 302s into 301s. 
Nearly overnight there was an increase in rankings in the 1–3 rank zone. Despite seasonality, there was a jump in organic Search traffic
as well. The only substantial change at this point was the 302 to 301 switch. It resulted in a few million more organic search visits 
month over month. 

Internal linking, the technical approach
Under the PageRank model, it’s an axiom that the flow of link equity through the site is an incredibly important component to examine. 
Unfortunately, so much of the discussion with clients is only on the external links and not about how to better maximize the link 
equity that a site already has. There are a number of tools out there that bring this concept to the forefront. For instance, 
Searchmetrics calculates and visualizes the flow of link equity throughout the site. This gives you a sense of where you can build 
internal links to make other pages stronger. 

Structured data is the future of organic search
There has been much discussion about how Google is taking our content and attempting to cut our own websites out of the picture. With 
the traffic boon that the industry has seen from sites making it into the featured snippet, it’s pretty obvious that, in many cases,
there's more value for you in Google taking your content than in them not. With Vocal Search appliances on mobile devices and the 
forthcoming Google Home, there's only one answer that the user receives. These answers are fueled by rich cards and featured snippets, 
which are in turn fueled by structured data.Google has actually done us a huge favor regarding structured data in updating the 
specifications that allow JSON-LD. Before this, Schema.org was a matter of making tedious and specific changes to code with little ROI.
Now structured data powers a number of components of the SERP and can simply be placed at the <HEAD> of a document quite easily.

Google has very aggressive expectations around page speed, especially for the mobile context. They want the above-the-fold content to 
load within one second. However, 800 milliseconds of that time is pretty much out of your control. Based on what you can directly affect,
, you have 200 milliseconds to make content appear on the screen. Lots of what can be done on-page to influence the speed at which 
things load is optimizing the page for critical rendering path.To understand this, we need a sense of how browsers construct a web page.
The browser takes the uniform resource locator (URL) that you specify in your address bar and performs a DNS lookup on the domain name.
Once a socket is open and a connection is negotiated, it then asks the server for the HTML of the page you’ve requested.
The browser begins to parse the HTML into the Document Object Model until it encounters CSS, then it starts to parse the CSS into the 
CSS Object Model. If at any point it runs into JavaScript, it will pause the DOM and/or CSSOM construction until the JavaScript
completes execution, unless it is asynchronous. Once all of this is complete, the browser constructs the Render Tree, which then builds
the layout of the page and finally the elements of the page are painted.
In the Timeline section of Chrome DevTools, you can see the individual operations as they happen and how they contribute to load time. 
In the timeline at the top, you’ll always see the visualization as mostly yellow because JavaScript execution takes the most time out 
of any part of page construction. JavaScript causes page construction to halt until the the script execution is complete. This is
called “render-blocking” JavaScript.
That term may sound familiar to you because you’ve poked around in PageSpeed Insights looking for answers on how to make improvements 
and “Eliminate Render-blocking JavaScript” is a common one. The tool is primarily built to support optimization for the Critical 
Rendering Path. A lot of the recommendations involve issues like sizing resources statically, using asynchronous scripts, and specifying
image dimensions. Additionally, external resources contribute significantly to page load time. 

Using pre-browsing directives to speed things up
To support site speed improvements, most browsers have pre-browsing resource hints. These hints allow you to indicate to the browser
that a file will be needed later in the page, so while the components of the browser are idle, it can download or connect to those 
resources now. Chrome specifically looks to do these things automatically when it can, and may ignore your specification altogether. 
 However, these directives operate much like the rel-canonical tag — you're more likely to get value out of them than not.
Rel-preconnect – This directive allows you to resolve the DNS, initiate the TCP handshake, and negotiate the TLS tunnel between the 
client and server before you need to. When you don’t do this, these things happen one after another for each resource rather than 
simultaneously. As the diagram below indicates, in some cases you can shave nearly half a second off just by doing this. Alternatively,
if you just want to resolve the DNS in advance, you could use rel-dns-prefetch.
If you see a lot of idle time in your Timeline in Chrome DevTools, rel-preconnect can help you shave some of that off.
You can specify rel-preconnect with
<link rel=”preconnect” href=”https://domain.com”>
or rel-dns-prefetch with
<link rel=”dns-prefetch” href=”domain.com”>	

Rel-prefetch – This directive allows you to download a resource for a page that will be needed in the future. For instance, if you want
to pull the stylesheet of the next page or download the HTML for the next page, you can do so by specifying it as
<link rel=”prefetch” href=”nextpage.html”>	

Rel-prerender – A directive that allows you to load an entire page and all of its resources in an invisible tab. Once the user clicks a
link to go to that URL, the page appears instantly. If the user instead clicks on a link that you did not specify as the rel-prerender, 
the prerendered page is deleted from memory. 
	You specify the rel-prerender as follows:
	<link rel=”prerender” href=”nextpage.html”>
	
Rel-prerender improved a site’s speed 68.35% with one line of code.
There are a number of caveats that come with rel-prerender, but the most important one is that you can only specify one page at a time
and only one rel-prerender can be specified across all Chrome threads. If you’re using an analytics package that isn’t Google 
Analytics, or if you have ads on your pages, it will falsely count prerender hits as actual views to the page. What you’ll want to do 
is wrap any JavaScript that you don’t want to fire until the page is actually in view in the Page Visibility API. Effectively, you’ll
only fire analytics or show ads when the page is actually visible.

Rel-preload and rel-subresource – Following the same pattern as above, rel-preload and rel-subresource allow you to load things within 
the same page before they are needed. Rel-subresource is Chrome-specific, while rel-preload works for Chrome, Android, and Opera.
Finally, keep in mind that Chrome is sophisticated enough to make attempts at all of these things. Your resource hints help them 
develop the 100% confidence level to act on them. Chrome is making a series of predictions based on everything you type into the 
address bar and it keeps track of whether or not it’s making the right predictions to determine what to preconnect and prerender 
for you. Check out chrome://predictors to see what Chrome has been predicting based on your behavior.

Where does SEO go from here - 
Being a strong SEO requires a series of skills that's difficult for a single person to be great at. For instance, an SEO with strong 
technical skills may find it difficult to perform effective outreach or vice-versa. 

An SEO Engineer will need to have a grasp of all of the following to truly capitalize on these technical opportunities - 
Document Object Model – An understanding of the building blocks of web browsers is fundamental to the understanding how front-end
developers manipulate the web as they build it.
Critical Rendering Path – An understanding of how a browser constructs a page and what goes into the rendering of the page will help 
with the speed enhancements that Google is more aggressively requiring.
Structured Data and Markup – An understanding of how metadata can be specified to influence how Google understands the information 
being presented.
Page Speed – An understanding of the rest of the coding and networking components that impact page load times is the natural next step
to getting page speed up. Of course, this is a much bigger deal than SEO, as it impacts the general user experience.
Log File Analysis – An understanding of how search engines traverse websites and what they deem as important and accessible is a 
requirement, especially with the advent of new front-end technologies.
SEO for JavaScript Frameworks – An understanding of the implications of leveraging one of the popular frameworks for front-end 
development, as well as a detailed understanding of how, why, and when an HTML snapshot appliance may be required and what it takes to
implement them is critical. 
Chrome DevTools – An understanding of one of the most the powerful tools in the SEO toolkit, the Chrome web browser itself.
Chrome DevTools’ features coupled with a few third-party plugins close the gaps for many things that SEO tools cannot currently analyze.
The SEO Engineer needs to be able to build something quick to get the answers to questions that were previously unasked by our industry.
Acclerated Mobile Pages & Facebook Instant Pages – If the AMP Roadmap is any indication, Facebook Instant Pages is a similar 
specification and I suspect it will be difficult for them to continue to exist exclusively.
HTTP/2 – Understanding how this protocol will dramatically change the speed of the web and SEO implications of migrating from HTTP/1.1.



August 10th 2016 - An Essential Training Task List for Junior SEOs
SEO isn’t as black & white as most marketing channels, to become a true professional requires a broad skill set. It’s not that a 
professional SEO needs to know the answer for everything; rather, it’s more important to have the skills to be able to find the answer.
This is a Junior SEO task list designed to help new starters in the field get the right skills by doing hands-on jobs, and possibly to 
help find a specialism in SEO or digital marketing. How long should this take?  60–90 days.
Before anything, here’s some prerequisite reading:
 - Moz Beginner’s Guide to SEO
 - Google's SEO Starter Guide
 - Official Google Webmaster Guidelines
 - How does the Internet work?
 - How the Web works
 - What is a domain name?
 - What is the difference between webpage, website, web server, and search engine?
 - What is a web server?

Project 1 – Technical Fundamentals:
Master the lingo and have a decent idea of how the Internet works before they start having conversations with developers or contributing
online. 
 - Must be able to answer the following in detail:
 - What is HTTP / HTTPS / HTTP2? Explain connections and how they flow.
 - Do root domains have trailing slashes?
 - What are the fundamental parts of a URL?
 - What is "www," anyway?
 - What are generic ccTLDs?
 - Describe the transaction between client and server?
 - What do we mean when we say "client side" and "server side?"
 - Name 3 common servers. Explain each one.
 - How does DNS work?
 - What are ports?
 - How do I see/find my public IP address?
 - What is a proxy server?
 - What is a reverse proxy server?
 - How do CDNs work?
 - What is a VPN?
 - What are server response codes and how do they relate to SEO?
 - What is the difference between URL rewriting and redirecting?
 - What is MVC?
 - What is a development sprint / scrum?
 - Describe a development deployment workflow.
 - What are the core functions that power Google search?
 - What is PageRank?
 - What is toolbar PageRank?
 - What is the reasonable surfer model?
 - What is the random surfer model?
 - What is Mozrank, Domain Authority, and Page Authority — and how are they calculated?
 - Name 3 Google search parameters and explain what they do (hint: gl= country).
 - What advanced operator search query will return: all URLs with https, with “cat” in the title, not including www subdomains, and 
   only PDFs?
 - Describe filtering in search results, and which parameter can be appended to the search URL to omit filtering.
 - How can I Google search by a specific date?
 - If we say something is "indexed," what does that mean?
 - If we say something is "canonicalized," what does that mean?
 - If we say something is "indexable," what does that mean?
 - If we say something is "non indexable," what does that mean?
 - If we say something is "crawlable," what does that mean?
 - If we say something is "not crawlable," what does that mean?
 - If we say something is "blocked," what does that mean?
 - Give examples of "parameters" in the wild, and manipulate any parameter on any website to show different content.
 - How should you check rankings for a particular keyword in a particular country?
 - Where are some places online you can speak to Googlers for advice?
 - What are the following: rel canonical, noindex, nofollow, hreflang, mobile alternate?(Explain each directive and its behavior in 
   detail and state any variations in implementation)
   
Explaining metrics from popular search tools
 - Explain SearchMetrics search visibility — how is this calculated? Why would you see declines in SM graphs but not in actual organic 
   traffic?
 - Explain Google Trends Index — how is this calculated?
 - Explain Google Keyword Planner search volume estimates & competition metric — is search volume accurate? Is the competition metric 
   useful for organic?
 - Explain SEMrush.com’s organic traffic graphs — Why might you see declines in SEMrush graphs, but not in actual organic traffic?
 
Link architecture
 - By hand, map out the world’s first website — http://info.cern.ch/hypertext/WWW/TheProject.html (we want to see the full link 
   architecture here in a way that’s digestable)
 - Explain its efficiency from an SEO perspective — are this website’s pages linked efficiently? Why or why not?

Project 2 – Creating a (minimum) 10-page website
If the trainee doesn’t understand what something is, make sure that they try and figure it out themselves before coming for help. 
Building a website by hand is absolutely painful, and they might want to throw their computer out the window or just install
Wordpress — no, no, no. There are so many things to learn by doing it the hard way, which is the only way.
 - Grab a domain name and go setup shared hosting. A LAMP stack with Cpanel and log file access (example: hostgator) is probably the 
   easiest.
 - Set up Filezilla with your host’s FTP details
 - Set up a text editor (example: Notepad++, Sublime) and connect via FTP for quick deploy
 - Create a 10-page flat site (NO CMS. That means no Wordpress!)
 - Within the site, it must contain at least one instance of each the following:
   <div>,<table>,<a>,<strong>, <em>, <iframe>, <button>, <noscript>, <form>, <option>, <button>, <img>, <h1>, <h2>, <h3>, <p>, <span>
 - Inline CSS that shows/hides a div on hover
 - Unique titles, meta descriptions, and H1s on every page
 - Must contain at least 3 folders
 - Must have at least 5 pages that are targeted to a different country
 - Recreate the navigation menu from the bbc.co.uk homepage (or your choice) using an external CSS stylesheet
 - Do the exact same as the previous, but make the Javascript external, and the function must execute with a button click.
 - Must receive 1,000 organic sessions in one month
 - Must contain Google Analytics tracking, Google search console setup, Bing webmaster tools, and Yandex webmaster tools setup
 - Create a custom 404 page
 - Create a 301, 302, and 307 redirect
 - Create a canonical to an exact duplicate, and another to a unique page — watch behavior
The site must contain at least one instance of each of the following, and every page which contains a directive (accompanying pages 
affected by directives as well) must be tracked through a rank tracker:
 - Rel canonical
 - Noindex
 - Noindex, follow
 - Mobile alternate (one page must be mobile-friendly)
 - Noarchive
 - Noimageindex
 - Meta refresh
 -Set up rank tracking
The trainee can use whatever tracking tool they like; https://www.wincher.com/ is $6/month for 100 keywords. The purpose of the rank
tracking is to measure the effects of directives implemented, redirects, and general fluctuation.
Create the following XML sitemaps:
 - Write the following XML sitemaps by hand for at least 5 URLs: mobile, desktop, Android App, and create one desktop XML sitemap with 
   hreflang annotations
 - Figure out how to ping Google & Bing with your sitemap URL
Writing robots.txt
 - Design a robots.txt that has specific blocking conditions for regular Googlebot, Bingbot, all user agents. They must be independent 
   and not interfere with each other.
 - Write a rule that disallows everything, but allows at least 1 folder.
 - Test the robots.txt file through the Search Console robots.txt tester.
 - Crawl the site and fix errors (Use Screaming Frog)
 - Have the trainee read: https://www.screamingfrog.co.uk/seo-spider/user-guide/
 - Ensure the trainee has a full, registered version of the software
 - Crawl the site and have them correct any errors on the site

Project 3 – PR, Sales, Promotion and Community Involvement
These tasks can be done on an independent website or directly for a client; it depends on your organizational requirements. This is the
part of the training where the trainee learns how to negotiate, sell, listen, promote, and create exposure for themselves.
Sales & negotiation
 - Close one guest post deal (i.e. have your content placed on an external website). Bonus if this is done via a phone call.
 - Create & close one syndication deal (i.e. have your content placed and rel canonical’d back to your content). Bonus if this is done 
   via a phone call.
 - Close one advertising deal (this could be as simple as negotiating a banner placement, and as hard as completely managing the 
   development of the ad plus tracking)
 - Sit in on 5 sales calls (depending on your business, this may need to be adjusted — it could be customer service calls)
 - Sit in on 5 sales meetings (again, adjust this for your business)
PR
 - Create a story, write a press release, get the story covered by any publication (bonus if there’s a link back to your original release, 
   or a rel canonical)
 - Use a PR wire to syndicate, or find your own syndication partner
Community involvement
 - Sign up for a Moz account and answer at least 15 questions in the forum
 - Sign up for a Quora account and answer at least 5 questions
 - Write 3 blog posts and get them featured on an industry website
 - Speak at an event, no matter how small; must be at least 10 minutes long
   YouTube
 - Create a screencast tutorial, upload it to YouTube, get 1,000 views (they will also need to optimize description, tags, etc.)
 - Here’s an example: https://www.youtube.com/watch?v=EXhmF9rjqP4 (that was my first try at this, years ago which you can use as 
   inspiration)
Facebook & Twitter Paid Ads
 - On both networks, pay to get 100 visits from an ad. These campaigns must be tracked properly in an analytics platform, not only in 
   FB and Twitter analytics!
Adwords
 - Create 1 campaign (custom ad) with the goal of finding real number of impressions versus estimated search volume from Keyword Planner.
 - Bonus: Drive 100 visits with an ad. Remember to keep the costs low — this is just training!

Project 4 – Data Manipulation & Analytics
Spreadsheets are to SEOs as fire trucks are to firefighters. Trainees need to be proficient in Excel or Google Docs right from the start.
These tasks are useful for grasping data manipulation techniques in spreadsheets, Google Analytics, and some more advanced subjects,
like scraping and machine learning classification.
Excel skills - Must be able to fill in required arguments for the following formulas in under 6 seconds:
 - Index + match
 - VLOOKUP (teaching people to index-match, because it’s more versatile and is quicker when dealing with larger datasets)
 - COUNTIF, COUNTIFS (2 conditions)
 - SUMIF, SUMIFS (2 conditions)
 - IF & AND statement in the same formula
 - Max, Min, Sum, Avg, Correl, Percentile, Len, Mid, Left, Right, Search, & Offset are also required formulas.
Also:
 - Conditional formatting based on a formula
 - Create a meaningful pivot table + chart
 - Record a macro that will actually be used
 - Ability to copy, paste, move, transpose, and copy an entire row and paste in new sheet — all while never touching the mouse.
Google Analytics
 - Install Google Analytics (Universal Analytics), and Google Tag Manager at least once — ensure that the bare minimum tracking works 
   properly.
 - Pass the GAIQ Exam with at least 90%
 - Create a non-interaction event
 - Create a destination goal
 - Create a macro that finds a value in the DOM and only fires on a specific page
 - Create a custom segment, segmenting session by Google organic, mobile device only, Android operating system, US traffic only — 
   then share the segment with another account.
 - Create an alert for increasing 404 page errors (comparison by day, threshold is 10% change)
 - Install the Google Tag Assistant for Chrome and learn to record and decipher requests for debugging
 - Use the Google Analytics Query explorer to pull from any profile — you must pull at least 3 metrics, 1 dimension, sort by 1 metric, 
   and have 1 filter.
 - Create one Google Content Experiment — this involves creating two pages and A/B testing to find the winner. Traffic doesn’t determine
   the winner here; it’s conversion rate.
Google Search Console
 - Trainee must go through every report (I really mean every report), and double-check the accuracy of each using external SEO tools 
   (except crawl activity reports). 
 - Fetch and render 5 different pages from 5 domains, include at least 2 mobile pages
 - Fetch (only fetch) 3 more pages; 1 must be mobile
Submit an XML sitemap
 - Create https, http, www, and non-www versions of their site they built in the previous project and identify discrepancies.
 - Answer: Why don’t clicks from search analytics add up compared to Google Analytics?
 - Answer: How are impressions from search analytics measured?
Link auditing
 - Download link reports for 1 website. Use Google Search Console, Majestic, Ahrefs, and Moz, and combine them all in one Excel file 
   (or Google Doc sheet). If the total number of rows between all 4 exports are over Excel’s limit, the trainee will need to figure 
   out how to handle large files on their own (hint: SQL or other database).
 - Must combine all links, de-duplicate, have columns for all anchor texts, and check if links are still alive 
Explore machine learning
 - Use Bigml.com and create a decision tree for classification. Try and predict something. 
   Follow http://machinelearningmastery.com/bigml-tutorial-develop-your-first-decision-tree-and-make-predictions/.
Scrape something
 - Use at least 3 different methods to extract information from any webpage (hint: import.io, importxml)
Log file analysis
 - Let the trainee use whatever software they want to parse the log files; just remember to explain how different servers will have 
   different fields.
 - Grab a copy of any web server access log files that contain at least the following fields: user-agent, timestamp, URI, IP, Method, 
    Referrer (ensure that CDNs or other intermediary transactions are not rewriting the IP addresses).
Trainee must be able to do the following:
 - Find Googlebot requests; double-check by reverse DNS that it’s actually Googlebot
 - Find a 4xx error encountered by Googlebot, then find the referrer for that 4xx error by looking at other user agent requests to the 
   same 4xx error
 - Create a pivot table with all the URLs requested and the amount of times they were requested by Googlebot
Keyword Planner
 - The candidate must be able to do the following:
 - Find YoY search volume for any given term
 - Find keyword limits, both in the interface and by uploading a CSV
 - Find the mobile trends graph for a set of keywords
 - Use negative keywords
 - Find breakdown by device
 - Google Chrome Development tools
The candidate must be able to do the following:
 - Turn off Javascript
 - Manipulate elements of the page (As a fun exercise, get them to change a news article to a completely new story)
 - Find every request Chrome makes when visiting a webpage
 - Download the HAR file
 - Run a speed audit & security audit directly from the development tool interface
 - Change their user agent to Googlebot
 - Emulate an Apple iPhone 5
 - Add a CSS attribute (or change one)
 - Add a breakpoint
 - Use the shortcut key to bring up development tools

Project 5 – Miscellaneous / Fun Stuff
These projects are designed to broaden their skills, as well as as prepare the trainee for the future and introduce them to important
concepts.
Use a proxy and a VPN
As long as they are able to connect to a proxy and a VPN in any application, this is fine — ensure that they understand how to verify
their new IP.
 - Find a development team, and observe the development cycle
 - Have the trainees be present during a scrum/sprint kickoff, and a release.
 - Have the trainees help write development tickets and prioritize accordingly.
 - Have them spend a day helping other employees with different jobs
 - Have them spend a day with the PR, analytics folks, devs... everyone. The goal should be to understand what it’s like to live a day
   in their shoes, and assist them throughout the entire day.
 - Get a website THEY OWN penalized. Heck, make it two!
 - Now that the trainee has built a website by hand, feel free to get them to put up another couple of websites and get some traffic 
   pouring in.
 - Then, start searching for nasty links and other deceptive SEO tactics that are against the Webmaster Guidelines and get that website 
   penalized.
 - Bonus: Try to get the penalty reversed. Heh, good luck :)
API skills
 - Request data from 2 different APIs using at least 2 different technologies (either a programming language or software — I would 
   suggest the SEMrush APIand Alchemy Language API). 
Write 2 functions in 2 different programming languages — these need to be functions that do something useful 
Ideas:
 - A Javascript bookmark that extracts link metrics from Majestic or Moz for the given page
 - A simple application that extracts title, H1, and all links from a given URL
 - A simple application that emails you if a change has been detected on a webpage
 - Pull word count from 100 pages in less than 10 seconds
 - If I were to pick which technology, it would be Javascript and Python. Javascript (Node, Express, React, Angular, Ember, etc.) 
   because I believe things are moving this way, i.e. 1 language for both front and back end. Python because of its rich data science
   & machine learning libraries, which may become a core part of SEO tasks in the future.
 - I strongly recommend anyone in SEO to build their own search engine.
 - Complete intro to Computer Science (you build a search engine in Python). 
 - Sign up to https://opensolr.com/, crawl a small website, and build your own search engine. 
Super Evil Genius Bonus Training
 - Get them to pass http://oap.ninja/.
 - Strong in Google Analytics/Omniture
 - Assist in the development of presentations to clients
 - Advanced proficiency with MS Excel, SQL
 - Advanced writing, grammar, spelling, editing, and English skills with a creative flair
 - Creating press releases and distribution
 - Proficiency in design software, Photoshop and Illustrator preferred
 - Develop and implement architectural, technical, and content recommendations
 - Conduct keyword research including industry trends and competitive analysis
 - Experience with WordPress and/or Magento (preferred)
 - Experience creating content for links and outreach
 - Experience in building up social media profiles and executing a social media strategy
 - Ability to program in HTML/CSS, VB/VBA, C++, PHP, and/or Python are a plus
 - A/B and Multivariate testing
 - Knowledge of project management software such as Basecamp, MS Project, Visio, Salesforce, etc
 - Basic knowledge of PHP, HTML, XML, CSS, JavaScript
 - Develop + analyze weekly and monthly reports across multiple clients
