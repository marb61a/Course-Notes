February 6th, 2017 - It's Time to Stop Doing On-Page SEO Like It's 2012
If you're optimizing a page to rank well for a keyword or set of keywords, you probably use some sort of checklist to make sure you're 
doing the right things. That might be through an SEO plug-in like Yoast or through Moz Pro's On-Page Grader, or it might be just be a 
mental checklist. 
These five mistakes and biases are popping up too often in our field, so let's address each with simple, tactical fixes.
1 - Kill those keyword repetition rules
I know. Many tools, free and paid, check for how many times a keyword is used on a page and in certain elements (like alt attributes of 
images or meta description tags or in bold text). The SEO software world's on-page suggestions, Moz's included, are far behind Google 
sophistication in this sense, but you don't have to be. Use tools' simple rules and checks to make sure you're meeting the minimum bar, 
but don't fall for advice like "1 use of the keyword phrase every 100 words" or "at least 4 uses of the keyword in HTML text." The 
MozBar's on-page suggestions are pretty good for this (though even it has some flaws, e.g. 75-character URL limits strikes me as too
short), and don't get bogged down in much X number of repetitions malarky. Remember that Google cares a lot about how visitors interact
with your content. If searchers don't click on your listing, or do, but bounce back to the SERP because you're not delivering the 
content or experience they want, you'll soon be off page one (see Brafton's excellent, recent case study on this).

Bottom line: Yes, it's still wise to use the keyword that searchers type into Google in your title, your description, and on the page.
But repetition-based rules are not gonna boost your rankings, and may inhibit your usability and content quality, which have far 
greater impacts.

2 - Searcher intent > raw keyword use
Serve the goals of the searcher. Deliver the experience they need and the answers they want. This is vastly more important than any 
simplistic keyword use rule.
Want a quick and easy way to figure out what searchers are seeking around a broad keyword? Do some basic keyword research!
E.g. I popped "faberge eggs" into Keyword Explorer, looked at the suggestions list, chose the "are questions" filter, and BOOM. 
KWE is giving me insight into exactly what people want to know about the eggs: What are they? How do you make them? How much do they 
cost? How many were made? Who was Faberge? You don't have to use KWE for this; most keyword research tools — even free ones like 
Ubersuggest or AdWords — will get you there. The goal is to understand what searchers want, and deliver it to them. For example, 
there are a lot of image searches for Faberge Eggs, suggesting that photos are critical to delivering the right user experience. 
The many questions and searches related to price and construction suggest that some folks want their own and, thus, providing links or 
information about how to craft replicas or where to buy them probably makes great sense, too. In my experience, it's vastly easier to 
create content of any kind that serves your visitors first, then retrofit that content with keyword rules vs. the other way around.

Bottom line: Discover what searchers want and deliver it to them before you worry about keyword use or repetition in your content.

3 - Related topics and keywords are ESSENTIAL
Raw keyword repetitions and simplistic rules don't take you far in 2017, but... related topics absolutely do. Google wants to see 
documents that intelligently use words and phrases that connect — semantically, lexically, and logically — to the queries searchers are 
using. Those topics help tell Google's on-page quality analysis systems that your content A) is on-topic and relevant, B) includes 
critical answers to searchers' questions, and C) has credible, accurate information.

Seriously, that's the competition — 9 sites you've definitely heard of, whose media brands and domain authority would make you think a come-from-nowhere underdog wouldn't stand a chance in these SERPs. And yet, there it is, like a beautiful Cinderella story dominating page one.

Want to replicate this success? It's not that hard.

Step one: Use related topics and keywords. The MozBar makes this easy:


I believe there are a few other tools that provide this functionality, including the Italian SEO Suite, SEOZoom. The MozBar gets its suggestions by crawling the pages that rank for the keyword, extracting out unique terms and phrases that appear on those pages more frequently than in other content across the web, and then listing them in order of relative importance/value.

It makes sense that words like "Peter Carl Faberge," "Tsar," "Imperial Easter Egg," and "Faberge Museum" would all belong on any content targeting this search query. If you're missing those terms and trying to rank, you're in for a much more difficult slog than if you employ them.

Step two: If there's any chance for a featured snippet in the SERP, aim for it by optimizing the format of your content. That could mean a list or a short explanatory paragraph. It might mean a single sentence atop the page that gives the quick-and-dirty answer while beckoning a searcher to click and learn more. Dr. Pete's guide to ranking #0 with featured snippets will give you more depth on how to get this right.

The best part about this is that few SEOs are doing this well right now. Many don't even know these processes or tools exist. And that means... it's still a competitive advantage if you do it :-)

Bottom line: There are keywords beyond synonyms or raw repetitions that can help you rank and claim the featured snippet position. You can find them manually or with tools, and employ them in your content to dramatically boost on-page SEO.

#4: Stop assuming links always beat on-page

This one's dead simple. We need to change our biased thinking about links and content from the days of 2012. Back then, it was still the case that a few more links with anchor text would move even an irrelevant, low-quality page of content above better and more valuable pages. Today, it's vastly more likely that very-well-linked-to pages (as in the example above) are getting their butts handed to them by marketers who go above and beyond with their on-page SEO efforts, winning despite a link deficit because they deliver the content and the experience Google (and searchers) want.

Bottom line: If you're ranking on page 2 or 3, blunt-force link building shouldn't be the only tool in your wheelhouse. Modern on-page SEO that better serves searchers and more intelligently considers content formatting and word usage and searcher satisfaction has got to be part of the equation.

5 - Pages matter, but so, too, do the sites hosting them
In 2012, Wikipedia and big sites like them dominated many results simply by virtue of their raw link authority and importance. Today, 
domain authority still plays a role, but it's not just link equity or the size and popularity of the site that matters. There's an 
element of topical authority and expertise in Google's algorithm that can deliver dramatic results to those willing to lean into it.

For example, in the SEO field, Moz has topical authority thanks to our years of writing about the subject, earning links from the field, becoming associated with the subject, and the close semantic connection that the words "Moz" and "SEO" have all over the web. The entity *Moz* surely lives in some Google database with a close word-association to SEO, just as SeriousEats lives alongside recipes, Dribbble lives alongside design, Zappos lives alongside shoes, and Zillow lives alongside real estate.

In many cases, it's not just about optimizing a page for a keyword, or earning links to that page, but about what your brand means to people and how the entity of your brand or organization might be associated with topics and topical authority in Google's eyes. This means that "on-page optimization" sometimes extends to "on-site optimization" and even "off-site brand building."

If Moz wanted to start ranking well for keywords far outside its current areas of thought leadership and topical relevance, we'd likely need to do far more than just go through the on-page SEO checklist and get some anchor text links. We'd need to create associations between our site and that content space, and indicate to Google and to searchers that they could trust us on those topics. If you're working on ranking for sets of keywords around a subject area and struggling to make progress despite nailing those two, topical authority may be to blame.

How do you build up authority around a topic? You associate your brand with it through online and offline campaigns. You publish content about it. You earn links from sites that talk about it. Your brand name gets searched for by people seeking it. You develop a following from the influencers around it. You become synonymous with it. There are thousands of tactics to pursue, and every organization is going to do best with the tactics that work for their audience, play to their strengths, and enable them to uniquely stand out. Just make sure you figure this into your calculus when considering why you may not be ranking, and what you may need to do differently.

Bottom line: Websites earn associations and connections with subject matter areas in Google. To earn rankings, you may need to address your entire site's brand focus, not just an individual page's keyword targeting.





February 24th, 2015 - 15 SEO Best Practices for Structuring URLs
An important caveat before we begin: the optimal structures and practices I'll be describing in the tips below are NOT absolutely 
critical on any/every page you create. 
1 - Whenever possible, use a single domain & subdomain
Whatever heuristics the engines use to judge whether content should inherit the ranking ability of its parent domain seem to have 
trouble consistently passing to subdomains.That's not to say it can't work, and if a subdomain is the only way you can set up a blog or
produce the content you need, then it's better than nothing. But your blog is far more likely to perform well in the rankings and to 
help the rest of your site's content perform well if it's all together on one sub and root domain.

2 - The more readable by human beings, the better
It should come as no surprise that the easier a URL is to read for humans, the better it is for search engines. Accessibility has always
been a part of SEO, but never more so than today. The requirement isn't that every aspect of the URL must be absolutely clean and 
perfect, but that at least it can be easily understood and, hopefully, compelling to those seeking its content.

3 - Keywords in URLs: still a good thing
It's still the case that using the keywords you're targeting for rankings in your URLs is a solid idea, it is true for several reasons.
  - First, keywords in the URL help indicate to those who see your URL on social media, in an email, or as they hover on a link to click
 that they're getting what they want and expect, as shown in the Metafilter example below (note how hovering on the link shows the URL 
 in the bottom-left-hand corner):
  - Second, URLs get copied and pasted regularly, and when there's no anchor text used in a link, the URL itself serves as that anchor 
  text (which is still a powerful input for rankings)
  - Third, and finally, keywords in the URL show up in search results, and research has shown that the URL is one of the most prominent
  elements searchers consider when selecting which site to click.

4 - Multiple URLs serving the same content? Canonicalize 'em!
If you have two URLs that serve very similar content, consider canonicalizing them, using either a 301 redirect (if there's no real 
reason to maintain the duplicate) or a rel=canonical (if you want to maintain slightly different versions for some visitors, e.g. a 
printer-friendly page).Duplicate content isn't really a search engine penalty (at least, not until/unless you start duplicating at very 
large scales), but it can cause a split of ranking signals that can harm your search traffic potential. 

5 - Exclude dynamic parameters when possible
If you can avoid using URL parameters, do so. If you have more than two URL parameters, it's probably worth making a serious investment
to rewrite them as static, readable, text. Most CMS platforms have become savvy to this over the years, check out tools like mod_rewrite 
and ISAPI rewrite to help with this process. Some dynamic parameters are used for tracking clicks (like those inserted by popular social 
sharing apps such as Buffer). In general, these don't cause a huge problem, but they may make for somewhat unsightly and awkwardly long 
URLs. Use your own judgement around whether the tracking parameter benefits outweigh the negatives.Research from a 2014 RadiumOne study
suggests that social sharing (which has positive, but usually indirect impacts on SEO) with shorter URLs that clearly communicate the 
site and content perform better than non-branded shorteners or long, unclear URL strings.

6 - Shorter > longer
Shorter URLs are, generally speaking, preferable. You don't need to take this to the extreme, and if your URL is already less than 5
0-60 characters, don't worry about it at all. But if you have URLs pushing 100+ characters, there's probably an opportunity to rewrite 
them and gain value. This isn't a direct problem with Google or Bing—the search engines can process long URLs without much trouble. The 
issue, instead, lies with usability and user experience. Shorter URLs are easier to parse, to copy and paste, to share on social media, 
and to embed, and while these might all add up to only a fractional improvement in sharing or amplification, every tweet, like, share, 
pin, email, and link matters (either directly or, often, indirectly).

7 - Match URLs to titles most of the time (when it makes sense)
This doesn't mean that if the title of your piece is that your URL has to be a perfect match. The matching accomplishes a mostly 
human-centric goal, i.e. to imbue an excellent sense of what the web user will find on the page through the URL and then to deliver on 
that expectation with the headline/title.It's strongly recommended keeping the page title and the visible headline on the page a close 
match as well—one creates an expectation, and the other delivers on it.

8 - Including stop words isn't necessary
If your title/headline includes stop words (and, or, but, of, the, a, etc.), it's not critical to put them in the URL. You don't have 
to leave them out, either, but it can sometimes help to make a URL shorter and more readable in some sharing contexts. Use your best
judgement on whether to include or not based on the readability vs. length.

9 - Remove/control for unwieldy punctuation characters
There are a number of text characters that become nasty bits of hard-to-read cruft when inserted in the URL string. In general, it's a
best practice to remove or control for these. There's a great list of safe vs. unsafe characters available on Perishable Press.It's not
merely the poor readability these characters might cause, but also the potential for breaking certain browsers, crawlers, or proper 
parsing.

10 - Limit redirection hops to two or fewer
If a user or crawler requests URL A, which redirects to URL B. That's cool. It's even OK if URL B then redirects to URL C (not great—it 
would be more ideal to point URL A directly to URL C, but not terrible). However, if the URL redirect string continues past two hops, 
you could get into trouble. Generally speaking, search engines will follow these longer redirect jumps, but they've recommended against 
the practice in the past, and for less "important" URLs (in their eyes), they may not follow or count the ranking signals of the 
redirecting URLs as completely. The bigger trouble is browsers and users, who are both slowed down and sometimes even stymied 
(mobile browsers in particular can occasionally struggle with this) by longer redirect strings. Keep redirects to a minimum and you'll 
set yourself up for less problems.

11 - Fewer folders is generally better
Take a URL like this:
 - randswhisky.com/scotch/lagavulin/15yr/distillers-edition/pedro-ximenez-cask/750ml
And consider, instead, structuring it like this:
 - randswhisky.com/scotch/lagavulin-distillers-edition-750ml
It's not that the slashes (aka folders) will necessarily harm performance, but it can create a perception of site depth for both engines
and users, as well as making edits to the URL string considerably more complex (at least, in most CMS' protocols).

12 - Avoid hashes in URLs that create separate/unique content
The hash (or URL fragment identifier) has historically been a way to send a visitor to a specific location on a given page 
(e.g. Moz's blog posts use the hash to navigate you to a particular comment, like this one from my wife). Hashes can also be used like 
tracking parameters (e.g. randswhisky.com/lagavulin#src=twitter). Using URL hashes for something other than these, such as showing
unique content than what's available on the page without the hash or wholly separate pages is generally a bad idea. There are exceptions,
like those Google enables for developers seeking to use the hashbang format for dynamic AJAX applications, but even these aren't nearly 
as clean, visitor-friendly, or simple from an SEO perspective as statically rewritten URLs. Sites from Amazon to Twitter have found 
tremendous benefit in simplifying their previously complex and hash/hashbang-employing URLs. If you can avoid it, do.

13 - Be wary of case sensitivity
A couple years back, John Sherrod of Search Discovery wrote an excellent piece noting the challenges and issues around case-sensitivit
in URLs. Long story short—if you're using Microsoft/IIS servers, you're generally in the clear. If you're hosting with Linux/UNIX, you
can get into trouble as they can interpret separate cases, and thus randswhisky.com/AbC could be a different piece of content from
randswhisky.com/aBc. In an ideal world, you want URLs that use the wrong case to automatically redirect/canonicalize to the right one. 
There are htaccess rewrite protocols to assist ( like this one)—highly recommended if you're facing this problem.

14 - Hyphens and underscores are preferred word separators
In the last few years, the search engines have successfully overcome their previous challenges with this issue and now treat
underscores and hyphens similarly. Spaces can work, but they render awkwardly in URLs as %20, which detracts from the readability of 
your pages. Try to avoid them if possible (it's usually pretty easy in a modern CMS).

15 - Keyword stuffing and repetition are pointless and make your site look spammy
Repetition like this doesn't help your search rankings—Google and Bing have moved far beyond algorithms that positively reward a 
keyword appearing multiple times in the URL string. Don't hurt your chances of earning a click (which CAN impact your rankings) by 
overdoing keyword matching/repetition in your URLs.
